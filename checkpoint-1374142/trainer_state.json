{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 22.0,
  "eval_steps": 500,
  "global_step": 1374142,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00800832866180828,
      "grad_norm": 4.341034412384033,
      "learning_rate": 4.986652785563653e-05,
      "loss": 0.7995,
      "step": 500
    },
    {
      "epoch": 0.01601665732361656,
      "grad_norm": 4.903304576873779,
      "learning_rate": 4.973305571127306e-05,
      "loss": 0.6779,
      "step": 1000
    },
    {
      "epoch": 0.02402498598542484,
      "grad_norm": 7.063520908355713,
      "learning_rate": 4.959958356690959e-05,
      "loss": 0.6516,
      "step": 1500
    },
    {
      "epoch": 0.03203331464723312,
      "grad_norm": 3.4154152870178223,
      "learning_rate": 4.946611142254612e-05,
      "loss": 0.6306,
      "step": 2000
    },
    {
      "epoch": 0.040041643309041405,
      "grad_norm": 4.487967014312744,
      "learning_rate": 4.9332639278182645e-05,
      "loss": 0.6084,
      "step": 2500
    },
    {
      "epoch": 0.04804997197084968,
      "grad_norm": 5.1551971435546875,
      "learning_rate": 4.9199167133819176e-05,
      "loss": 0.6201,
      "step": 3000
    },
    {
      "epoch": 0.056058300632657966,
      "grad_norm": 6.992980003356934,
      "learning_rate": 4.90656949894557e-05,
      "loss": 0.6138,
      "step": 3500
    },
    {
      "epoch": 0.06406662929446624,
      "grad_norm": 14.44216537475586,
      "learning_rate": 4.893222284509223e-05,
      "loss": 0.5986,
      "step": 4000
    },
    {
      "epoch": 0.07207495795627453,
      "grad_norm": 5.647764205932617,
      "learning_rate": 4.879875070072876e-05,
      "loss": 0.6008,
      "step": 4500
    },
    {
      "epoch": 0.08008328661808281,
      "grad_norm": 6.318582534790039,
      "learning_rate": 4.8665278556365293e-05,
      "loss": 0.5978,
      "step": 5000
    },
    {
      "epoch": 0.08809161527989108,
      "grad_norm": 3.0823676586151123,
      "learning_rate": 4.853180641200182e-05,
      "loss": 0.6041,
      "step": 5500
    },
    {
      "epoch": 0.09609994394169936,
      "grad_norm": 3.658535957336426,
      "learning_rate": 4.839833426763835e-05,
      "loss": 0.5961,
      "step": 6000
    },
    {
      "epoch": 0.10410827260350765,
      "grad_norm": 9.705808639526367,
      "learning_rate": 4.826486212327487e-05,
      "loss": 0.575,
      "step": 6500
    },
    {
      "epoch": 0.11211660126531593,
      "grad_norm": 4.798050880432129,
      "learning_rate": 4.81313899789114e-05,
      "loss": 0.5939,
      "step": 7000
    },
    {
      "epoch": 0.12012492992712422,
      "grad_norm": 3.745378255844116,
      "learning_rate": 4.7997917834547936e-05,
      "loss": 0.5876,
      "step": 7500
    },
    {
      "epoch": 0.12813325858893249,
      "grad_norm": 6.620859146118164,
      "learning_rate": 4.786444569018446e-05,
      "loss": 0.5834,
      "step": 8000
    },
    {
      "epoch": 0.13614158725074077,
      "grad_norm": 6.508619785308838,
      "learning_rate": 4.773097354582099e-05,
      "loss": 0.5811,
      "step": 8500
    },
    {
      "epoch": 0.14414991591254905,
      "grad_norm": 3.5845539569854736,
      "learning_rate": 4.7597501401457515e-05,
      "loss": 0.586,
      "step": 9000
    },
    {
      "epoch": 0.15215824457435734,
      "grad_norm": 3.8543505668640137,
      "learning_rate": 4.7464029257094047e-05,
      "loss": 0.5764,
      "step": 9500
    },
    {
      "epoch": 0.16016657323616562,
      "grad_norm": 6.021883964538574,
      "learning_rate": 4.733055711273058e-05,
      "loss": 0.5818,
      "step": 10000
    },
    {
      "epoch": 0.1681749018979739,
      "grad_norm": 5.576465129852295,
      "learning_rate": 4.719708496836711e-05,
      "loss": 0.5723,
      "step": 10500
    },
    {
      "epoch": 0.17618323055978216,
      "grad_norm": 24.022804260253906,
      "learning_rate": 4.706361282400363e-05,
      "loss": 0.5747,
      "step": 11000
    },
    {
      "epoch": 0.18419155922159045,
      "grad_norm": 8.683987617492676,
      "learning_rate": 4.6930140679640164e-05,
      "loss": 0.5779,
      "step": 11500
    },
    {
      "epoch": 0.19219988788339873,
      "grad_norm": 9.578516006469727,
      "learning_rate": 4.679666853527669e-05,
      "loss": 0.5902,
      "step": 12000
    },
    {
      "epoch": 0.200208216545207,
      "grad_norm": 6.387156963348389,
      "learning_rate": 4.666319639091321e-05,
      "loss": 0.5706,
      "step": 12500
    },
    {
      "epoch": 0.2082165452070153,
      "grad_norm": 7.460468292236328,
      "learning_rate": 4.652972424654975e-05,
      "loss": 0.5753,
      "step": 13000
    },
    {
      "epoch": 0.21622487386882358,
      "grad_norm": 2.5201504230499268,
      "learning_rate": 4.6396252102186275e-05,
      "loss": 0.5617,
      "step": 13500
    },
    {
      "epoch": 0.22423320253063186,
      "grad_norm": 10.917168617248535,
      "learning_rate": 4.6262779957822806e-05,
      "loss": 0.5789,
      "step": 14000
    },
    {
      "epoch": 0.23224153119244015,
      "grad_norm": 4.143006801605225,
      "learning_rate": 4.612930781345933e-05,
      "loss": 0.5796,
      "step": 14500
    },
    {
      "epoch": 0.24024985985424843,
      "grad_norm": 5.7884907722473145,
      "learning_rate": 4.599583566909586e-05,
      "loss": 0.5753,
      "step": 15000
    },
    {
      "epoch": 0.2482581885160567,
      "grad_norm": 3.828791856765747,
      "learning_rate": 4.5862363524732386e-05,
      "loss": 0.5697,
      "step": 15500
    },
    {
      "epoch": 0.25626651717786497,
      "grad_norm": 4.151005268096924,
      "learning_rate": 4.5728891380368924e-05,
      "loss": 0.5676,
      "step": 16000
    },
    {
      "epoch": 0.26427484583967326,
      "grad_norm": 2.498488426208496,
      "learning_rate": 4.559541923600545e-05,
      "loss": 0.584,
      "step": 16500
    },
    {
      "epoch": 0.27228317450148154,
      "grad_norm": 7.905001163482666,
      "learning_rate": 4.546194709164198e-05,
      "loss": 0.5704,
      "step": 17000
    },
    {
      "epoch": 0.2802915031632898,
      "grad_norm": 5.520227909088135,
      "learning_rate": 4.5328474947278504e-05,
      "loss": 0.5705,
      "step": 17500
    },
    {
      "epoch": 0.2882998318250981,
      "grad_norm": 3.0246224403381348,
      "learning_rate": 4.5195002802915035e-05,
      "loss": 0.5685,
      "step": 18000
    },
    {
      "epoch": 0.2963081604869064,
      "grad_norm": 3.9577836990356445,
      "learning_rate": 4.5061530658551566e-05,
      "loss": 0.5672,
      "step": 18500
    },
    {
      "epoch": 0.3043164891487147,
      "grad_norm": 3.936753988265991,
      "learning_rate": 4.492805851418809e-05,
      "loss": 0.5723,
      "step": 19000
    },
    {
      "epoch": 0.31232481781052296,
      "grad_norm": 9.493486404418945,
      "learning_rate": 4.479458636982462e-05,
      "loss": 0.5821,
      "step": 19500
    },
    {
      "epoch": 0.32033314647233124,
      "grad_norm": 4.156578063964844,
      "learning_rate": 4.4661114225461146e-05,
      "loss": 0.5514,
      "step": 20000
    },
    {
      "epoch": 0.3283414751341395,
      "grad_norm": 3.5540716648101807,
      "learning_rate": 4.452764208109768e-05,
      "loss": 0.5495,
      "step": 20500
    },
    {
      "epoch": 0.3363498037959478,
      "grad_norm": 3.091482162475586,
      "learning_rate": 4.43941699367342e-05,
      "loss": 0.5689,
      "step": 21000
    },
    {
      "epoch": 0.34435813245775604,
      "grad_norm": 2.9373323917388916,
      "learning_rate": 4.426069779237074e-05,
      "loss": 0.5602,
      "step": 21500
    },
    {
      "epoch": 0.3523664611195643,
      "grad_norm": 2.800938129425049,
      "learning_rate": 4.4127225648007264e-05,
      "loss": 0.5615,
      "step": 22000
    },
    {
      "epoch": 0.3603747897813726,
      "grad_norm": 5.757203102111816,
      "learning_rate": 4.3993753503643795e-05,
      "loss": 0.5678,
      "step": 22500
    },
    {
      "epoch": 0.3683831184431809,
      "grad_norm": 3.7301228046417236,
      "learning_rate": 4.386028135928032e-05,
      "loss": 0.5777,
      "step": 23000
    },
    {
      "epoch": 0.3763914471049892,
      "grad_norm": 32.20902633666992,
      "learning_rate": 4.372680921491685e-05,
      "loss": 0.576,
      "step": 23500
    },
    {
      "epoch": 0.38439977576679746,
      "grad_norm": 4.901029586791992,
      "learning_rate": 4.3593337070553375e-05,
      "loss": 0.588,
      "step": 24000
    },
    {
      "epoch": 0.39240810442860574,
      "grad_norm": 4.396781921386719,
      "learning_rate": 4.3459864926189906e-05,
      "loss": 0.5851,
      "step": 24500
    },
    {
      "epoch": 0.400416433090414,
      "grad_norm": 4.765368461608887,
      "learning_rate": 4.332639278182644e-05,
      "loss": 0.5777,
      "step": 25000
    },
    {
      "epoch": 0.4084247617522223,
      "grad_norm": 10.55105972290039,
      "learning_rate": 4.319292063746296e-05,
      "loss": 0.5738,
      "step": 25500
    },
    {
      "epoch": 0.4164330904140306,
      "grad_norm": 5.845773220062256,
      "learning_rate": 4.305944849309949e-05,
      "loss": 0.5807,
      "step": 26000
    },
    {
      "epoch": 0.4244414190758389,
      "grad_norm": 7.819371700286865,
      "learning_rate": 4.292597634873602e-05,
      "loss": 0.5591,
      "step": 26500
    },
    {
      "epoch": 0.43244974773764716,
      "grad_norm": 2.8302836418151855,
      "learning_rate": 4.2792504204372555e-05,
      "loss": 0.5691,
      "step": 27000
    },
    {
      "epoch": 0.44045807639945544,
      "grad_norm": 10.305069923400879,
      "learning_rate": 4.265903206000908e-05,
      "loss": 0.552,
      "step": 27500
    },
    {
      "epoch": 0.44846640506126373,
      "grad_norm": 6.313445091247559,
      "learning_rate": 4.252555991564561e-05,
      "loss": 0.5567,
      "step": 28000
    },
    {
      "epoch": 0.456474733723072,
      "grad_norm": 4.808748245239258,
      "learning_rate": 4.2392087771282135e-05,
      "loss": 0.5686,
      "step": 28500
    },
    {
      "epoch": 0.4644830623848803,
      "grad_norm": 5.541644096374512,
      "learning_rate": 4.2258615626918666e-05,
      "loss": 0.562,
      "step": 29000
    },
    {
      "epoch": 0.4724913910466886,
      "grad_norm": 3.016921281814575,
      "learning_rate": 4.212514348255519e-05,
      "loss": 0.5752,
      "step": 29500
    },
    {
      "epoch": 0.48049971970849686,
      "grad_norm": 1.342978596687317,
      "learning_rate": 4.199167133819172e-05,
      "loss": 0.5676,
      "step": 30000
    },
    {
      "epoch": 0.4885080483703051,
      "grad_norm": 2.043856620788574,
      "learning_rate": 4.185819919382825e-05,
      "loss": 0.5774,
      "step": 30500
    },
    {
      "epoch": 0.4965163770321134,
      "grad_norm": 5.0765299797058105,
      "learning_rate": 4.172472704946478e-05,
      "loss": 0.5716,
      "step": 31000
    },
    {
      "epoch": 0.5045247056939217,
      "grad_norm": 5.079748630523682,
      "learning_rate": 4.159125490510131e-05,
      "loss": 0.5598,
      "step": 31500
    },
    {
      "epoch": 0.5125330343557299,
      "grad_norm": 3.0093252658843994,
      "learning_rate": 4.145778276073783e-05,
      "loss": 0.5828,
      "step": 32000
    },
    {
      "epoch": 0.5205413630175383,
      "grad_norm": 3.962170362472534,
      "learning_rate": 4.1324310616374364e-05,
      "loss": 0.5496,
      "step": 32500
    },
    {
      "epoch": 0.5285496916793465,
      "grad_norm": 3.651639223098755,
      "learning_rate": 4.1190838472010895e-05,
      "loss": 0.5586,
      "step": 33000
    },
    {
      "epoch": 0.5365580203411549,
      "grad_norm": 4.153874397277832,
      "learning_rate": 4.1057366327647426e-05,
      "loss": 0.5602,
      "step": 33500
    },
    {
      "epoch": 0.5445663490029631,
      "grad_norm": 4.265336990356445,
      "learning_rate": 4.092389418328395e-05,
      "loss": 0.5628,
      "step": 34000
    },
    {
      "epoch": 0.5525746776647713,
      "grad_norm": 7.398928642272949,
      "learning_rate": 4.079042203892048e-05,
      "loss": 0.5575,
      "step": 34500
    },
    {
      "epoch": 0.5605830063265796,
      "grad_norm": 2.9150454998016357,
      "learning_rate": 4.0656949894557006e-05,
      "loss": 0.5655,
      "step": 35000
    },
    {
      "epoch": 0.5685913349883879,
      "grad_norm": 10.627225875854492,
      "learning_rate": 4.052347775019354e-05,
      "loss": 0.5556,
      "step": 35500
    },
    {
      "epoch": 0.5765996636501962,
      "grad_norm": 5.627581596374512,
      "learning_rate": 4.039000560583007e-05,
      "loss": 0.5658,
      "step": 36000
    },
    {
      "epoch": 0.5846079923120044,
      "grad_norm": 4.279469013214111,
      "learning_rate": 4.025653346146659e-05,
      "loss": 0.5489,
      "step": 36500
    },
    {
      "epoch": 0.5926163209738128,
      "grad_norm": 2.1071250438690186,
      "learning_rate": 4.0123061317103124e-05,
      "loss": 0.5657,
      "step": 37000
    },
    {
      "epoch": 0.600624649635621,
      "grad_norm": 4.208123207092285,
      "learning_rate": 3.998958917273965e-05,
      "loss": 0.5634,
      "step": 37500
    },
    {
      "epoch": 0.6086329782974294,
      "grad_norm": 6.990226745605469,
      "learning_rate": 3.985611702837618e-05,
      "loss": 0.5506,
      "step": 38000
    },
    {
      "epoch": 0.6166413069592376,
      "grad_norm": 3.9392850399017334,
      "learning_rate": 3.972264488401271e-05,
      "loss": 0.5653,
      "step": 38500
    },
    {
      "epoch": 0.6246496356210459,
      "grad_norm": 4.577337265014648,
      "learning_rate": 3.958917273964924e-05,
      "loss": 0.5529,
      "step": 39000
    },
    {
      "epoch": 0.6326579642828541,
      "grad_norm": 4.566893577575684,
      "learning_rate": 3.9455700595285766e-05,
      "loss": 0.5548,
      "step": 39500
    },
    {
      "epoch": 0.6406662929446625,
      "grad_norm": 5.411877632141113,
      "learning_rate": 3.93222284509223e-05,
      "loss": 0.5537,
      "step": 40000
    },
    {
      "epoch": 0.6486746216064707,
      "grad_norm": 4.104384422302246,
      "learning_rate": 3.918875630655882e-05,
      "loss": 0.5651,
      "step": 40500
    },
    {
      "epoch": 0.656682950268279,
      "grad_norm": 2.938718557357788,
      "learning_rate": 3.905528416219535e-05,
      "loss": 0.566,
      "step": 41000
    },
    {
      "epoch": 0.6646912789300873,
      "grad_norm": 3.550554037094116,
      "learning_rate": 3.8921812017831883e-05,
      "loss": 0.5546,
      "step": 41500
    },
    {
      "epoch": 0.6726996075918956,
      "grad_norm": 1.508626103401184,
      "learning_rate": 3.878833987346841e-05,
      "loss": 0.5534,
      "step": 42000
    },
    {
      "epoch": 0.6807079362537038,
      "grad_norm": 2.913686752319336,
      "learning_rate": 3.865486772910494e-05,
      "loss": 0.5476,
      "step": 42500
    },
    {
      "epoch": 0.6887162649155121,
      "grad_norm": 3.9451050758361816,
      "learning_rate": 3.852139558474146e-05,
      "loss": 0.5494,
      "step": 43000
    },
    {
      "epoch": 0.6967245935773204,
      "grad_norm": 3.3587934970855713,
      "learning_rate": 3.8387923440377994e-05,
      "loss": 0.5487,
      "step": 43500
    },
    {
      "epoch": 0.7047329222391286,
      "grad_norm": 1.7476997375488281,
      "learning_rate": 3.8254451296014526e-05,
      "loss": 0.537,
      "step": 44000
    },
    {
      "epoch": 0.712741250900937,
      "grad_norm": 4.126948833465576,
      "learning_rate": 3.812097915165106e-05,
      "loss": 0.5448,
      "step": 44500
    },
    {
      "epoch": 0.7207495795627452,
      "grad_norm": 4.941864967346191,
      "learning_rate": 3.798750700728758e-05,
      "loss": 0.567,
      "step": 45000
    },
    {
      "epoch": 0.7287579082245536,
      "grad_norm": 3.1921803951263428,
      "learning_rate": 3.785403486292411e-05,
      "loss": 0.5621,
      "step": 45500
    },
    {
      "epoch": 0.7367662368863618,
      "grad_norm": 3.7173779010772705,
      "learning_rate": 3.7720562718560637e-05,
      "loss": 0.5479,
      "step": 46000
    },
    {
      "epoch": 0.7447745655481701,
      "grad_norm": 2.963423252105713,
      "learning_rate": 3.758709057419717e-05,
      "loss": 0.5611,
      "step": 46500
    },
    {
      "epoch": 0.7527828942099783,
      "grad_norm": 5.177140235900879,
      "learning_rate": 3.74536184298337e-05,
      "loss": 0.5445,
      "step": 47000
    },
    {
      "epoch": 0.7607912228717867,
      "grad_norm": 3.394289255142212,
      "learning_rate": 3.732014628547022e-05,
      "loss": 0.5473,
      "step": 47500
    },
    {
      "epoch": 0.7687995515335949,
      "grad_norm": 10.127290725708008,
      "learning_rate": 3.7186674141106754e-05,
      "loss": 0.5494,
      "step": 48000
    },
    {
      "epoch": 0.7768078801954033,
      "grad_norm": 5.279999256134033,
      "learning_rate": 3.705320199674328e-05,
      "loss": 0.5584,
      "step": 48500
    },
    {
      "epoch": 0.7848162088572115,
      "grad_norm": 13.918063163757324,
      "learning_rate": 3.691972985237981e-05,
      "loss": 0.5715,
      "step": 49000
    },
    {
      "epoch": 0.7928245375190198,
      "grad_norm": 5.973888397216797,
      "learning_rate": 3.6786257708016334e-05,
      "loss": 0.5735,
      "step": 49500
    },
    {
      "epoch": 0.800832866180828,
      "grad_norm": 17.23167610168457,
      "learning_rate": 3.665278556365287e-05,
      "loss": 0.5735,
      "step": 50000
    },
    {
      "epoch": 0.8088411948426364,
      "grad_norm": 3.0458498001098633,
      "learning_rate": 3.6519313419289396e-05,
      "loss": 0.6101,
      "step": 50500
    },
    {
      "epoch": 0.8168495235044446,
      "grad_norm": 5.47400426864624,
      "learning_rate": 3.638584127492593e-05,
      "loss": 0.569,
      "step": 51000
    },
    {
      "epoch": 0.824857852166253,
      "grad_norm": 3.2479183673858643,
      "learning_rate": 3.625236913056245e-05,
      "loss": 0.5747,
      "step": 51500
    },
    {
      "epoch": 0.8328661808280612,
      "grad_norm": 8.307097434997559,
      "learning_rate": 3.611889698619898e-05,
      "loss": 0.5658,
      "step": 52000
    },
    {
      "epoch": 0.8408745094898694,
      "grad_norm": 3.029916763305664,
      "learning_rate": 3.5985424841835514e-05,
      "loss": 0.5746,
      "step": 52500
    },
    {
      "epoch": 0.8488828381516778,
      "grad_norm": 3.647195339202881,
      "learning_rate": 3.585195269747204e-05,
      "loss": 0.6002,
      "step": 53000
    },
    {
      "epoch": 0.856891166813486,
      "grad_norm": 2.2078123092651367,
      "learning_rate": 3.571848055310857e-05,
      "loss": 0.5973,
      "step": 53500
    },
    {
      "epoch": 0.8648994954752943,
      "grad_norm": 4.585103511810303,
      "learning_rate": 3.5585008408745094e-05,
      "loss": 0.6071,
      "step": 54000
    },
    {
      "epoch": 0.8729078241371026,
      "grad_norm": 30.27272605895996,
      "learning_rate": 3.5451536264381625e-05,
      "loss": 0.5817,
      "step": 54500
    },
    {
      "epoch": 0.8809161527989109,
      "grad_norm": 3.619690418243408,
      "learning_rate": 3.531806412001815e-05,
      "loss": 0.6166,
      "step": 55000
    },
    {
      "epoch": 0.8889244814607191,
      "grad_norm": 4.975050449371338,
      "learning_rate": 3.518459197565469e-05,
      "loss": 0.618,
      "step": 55500
    },
    {
      "epoch": 0.8969328101225275,
      "grad_norm": 2.7916760444641113,
      "learning_rate": 3.505111983129121e-05,
      "loss": 0.5827,
      "step": 56000
    },
    {
      "epoch": 0.9049411387843357,
      "grad_norm": 2.5821750164031982,
      "learning_rate": 3.491764768692774e-05,
      "loss": 0.5558,
      "step": 56500
    },
    {
      "epoch": 0.912949467446144,
      "grad_norm": 11.432326316833496,
      "learning_rate": 3.478417554256427e-05,
      "loss": 0.5964,
      "step": 57000
    },
    {
      "epoch": 0.9209577961079523,
      "grad_norm": 7.981192111968994,
      "learning_rate": 3.46507033982008e-05,
      "loss": 0.5874,
      "step": 57500
    },
    {
      "epoch": 0.9289661247697606,
      "grad_norm": 8.353496551513672,
      "learning_rate": 3.451723125383732e-05,
      "loss": 0.6429,
      "step": 58000
    },
    {
      "epoch": 0.9369744534315688,
      "grad_norm": 3.98939847946167,
      "learning_rate": 3.4383759109473854e-05,
      "loss": 0.6455,
      "step": 58500
    },
    {
      "epoch": 0.9449827820933772,
      "grad_norm": 8.27649974822998,
      "learning_rate": 3.4250286965110385e-05,
      "loss": 0.5744,
      "step": 59000
    },
    {
      "epoch": 0.9529911107551854,
      "grad_norm": 3.5062062740325928,
      "learning_rate": 3.411681482074691e-05,
      "loss": 0.5702,
      "step": 59500
    },
    {
      "epoch": 0.9609994394169937,
      "grad_norm": 15.206467628479004,
      "learning_rate": 3.398334267638344e-05,
      "loss": 0.5758,
      "step": 60000
    },
    {
      "epoch": 0.969007768078802,
      "grad_norm": 17.04636573791504,
      "learning_rate": 3.3849870532019965e-05,
      "loss": 0.5856,
      "step": 60500
    },
    {
      "epoch": 0.9770160967406102,
      "grad_norm": 6.201862812042236,
      "learning_rate": 3.37163983876565e-05,
      "loss": 0.6112,
      "step": 61000
    },
    {
      "epoch": 0.9850244254024185,
      "grad_norm": 5.430823802947998,
      "learning_rate": 3.358292624329303e-05,
      "loss": 0.6022,
      "step": 61500
    },
    {
      "epoch": 0.9930327540642268,
      "grad_norm": 12.381128311157227,
      "learning_rate": 3.344945409892956e-05,
      "loss": 0.5864,
      "step": 62000
    },
    {
      "epoch": 1.001041082726035,
      "grad_norm": 4.036890029907227,
      "learning_rate": 3.331598195456608e-05,
      "loss": 0.5762,
      "step": 62500
    },
    {
      "epoch": 1.0090494113878434,
      "grad_norm": 3.122976541519165,
      "learning_rate": 3.3182509810202614e-05,
      "loss": 0.6011,
      "step": 63000
    },
    {
      "epoch": 1.0170577400496517,
      "grad_norm": 2.9697132110595703,
      "learning_rate": 3.304903766583914e-05,
      "loss": 0.5758,
      "step": 63500
    },
    {
      "epoch": 1.0250660687114599,
      "grad_norm": 7.880277156829834,
      "learning_rate": 3.291556552147567e-05,
      "loss": 0.5538,
      "step": 64000
    },
    {
      "epoch": 1.0330743973732681,
      "grad_norm": 10.287847518920898,
      "learning_rate": 3.27820933771122e-05,
      "loss": 0.5684,
      "step": 64500
    },
    {
      "epoch": 1.0410827260350766,
      "grad_norm": 29.196542739868164,
      "learning_rate": 3.2648621232748725e-05,
      "loss": 0.5852,
      "step": 65000
    },
    {
      "epoch": 1.0490910546968848,
      "grad_norm": 13.388043403625488,
      "learning_rate": 3.2515149088385256e-05,
      "loss": 0.5929,
      "step": 65500
    },
    {
      "epoch": 1.057099383358693,
      "grad_norm": 2.2638657093048096,
      "learning_rate": 3.238167694402178e-05,
      "loss": 0.5985,
      "step": 66000
    },
    {
      "epoch": 1.0651077120205013,
      "grad_norm": 46.76168441772461,
      "learning_rate": 3.224820479965831e-05,
      "loss": 0.5722,
      "step": 66500
    },
    {
      "epoch": 1.0731160406823097,
      "grad_norm": 10.283295631408691,
      "learning_rate": 3.211473265529484e-05,
      "loss": 0.5599,
      "step": 67000
    },
    {
      "epoch": 1.081124369344118,
      "grad_norm": 4.871533393859863,
      "learning_rate": 3.1981260510931374e-05,
      "loss": 0.5426,
      "step": 67500
    },
    {
      "epoch": 1.0891326980059262,
      "grad_norm": 4.29221248626709,
      "learning_rate": 3.18477883665679e-05,
      "loss": 0.5692,
      "step": 68000
    },
    {
      "epoch": 1.0971410266677344,
      "grad_norm": 21.451339721679688,
      "learning_rate": 3.171431622220443e-05,
      "loss": 0.5576,
      "step": 68500
    },
    {
      "epoch": 1.1051493553295426,
      "grad_norm": 5.6028666496276855,
      "learning_rate": 3.1580844077840954e-05,
      "loss": 0.5685,
      "step": 69000
    },
    {
      "epoch": 1.113157683991351,
      "grad_norm": 4.469378471374512,
      "learning_rate": 3.1447371933477485e-05,
      "loss": 0.5372,
      "step": 69500
    },
    {
      "epoch": 1.1211660126531593,
      "grad_norm": 6.250676155090332,
      "learning_rate": 3.1313899789114016e-05,
      "loss": 0.528,
      "step": 70000
    },
    {
      "epoch": 1.1291743413149675,
      "grad_norm": 2.7597177028656006,
      "learning_rate": 3.118042764475054e-05,
      "loss": 0.525,
      "step": 70500
    },
    {
      "epoch": 1.1371826699767758,
      "grad_norm": 3.4835798740386963,
      "learning_rate": 3.104695550038707e-05,
      "loss": 0.5368,
      "step": 71000
    },
    {
      "epoch": 1.1451909986385842,
      "grad_norm": 4.725292205810547,
      "learning_rate": 3.0913483356023596e-05,
      "loss": 0.53,
      "step": 71500
    },
    {
      "epoch": 1.1531993273003924,
      "grad_norm": 4.831820487976074,
      "learning_rate": 3.078001121166013e-05,
      "loss": 0.5339,
      "step": 72000
    },
    {
      "epoch": 1.1612076559622007,
      "grad_norm": 3.886702537536621,
      "learning_rate": 3.064653906729666e-05,
      "loss": 0.5383,
      "step": 72500
    },
    {
      "epoch": 1.1692159846240089,
      "grad_norm": 2.8685832023620605,
      "learning_rate": 3.0513066922933186e-05,
      "loss": 0.5331,
      "step": 73000
    },
    {
      "epoch": 1.1772243132858173,
      "grad_norm": 13.397988319396973,
      "learning_rate": 3.0379594778569713e-05,
      "loss": 0.5253,
      "step": 73500
    },
    {
      "epoch": 1.1852326419476256,
      "grad_norm": 6.273265361785889,
      "learning_rate": 3.024612263420624e-05,
      "loss": 0.5244,
      "step": 74000
    },
    {
      "epoch": 1.1932409706094338,
      "grad_norm": 3.430068016052246,
      "learning_rate": 3.011265048984277e-05,
      "loss": 0.5544,
      "step": 74500
    },
    {
      "epoch": 1.201249299271242,
      "grad_norm": 160.6565704345703,
      "learning_rate": 2.9979178345479297e-05,
      "loss": 0.5334,
      "step": 75000
    },
    {
      "epoch": 1.2092576279330505,
      "grad_norm": 4.157841205596924,
      "learning_rate": 2.984570620111583e-05,
      "loss": 0.5325,
      "step": 75500
    },
    {
      "epoch": 1.2172659565948587,
      "grad_norm": 23.81913185119629,
      "learning_rate": 2.971223405675236e-05,
      "loss": 0.5385,
      "step": 76000
    },
    {
      "epoch": 1.225274285256667,
      "grad_norm": 6.362563133239746,
      "learning_rate": 2.9578761912388887e-05,
      "loss": 0.5406,
      "step": 76500
    },
    {
      "epoch": 1.2332826139184752,
      "grad_norm": 2.9250121116638184,
      "learning_rate": 2.9445289768025414e-05,
      "loss": 0.5285,
      "step": 77000
    },
    {
      "epoch": 1.2412909425802834,
      "grad_norm": 12.129765510559082,
      "learning_rate": 2.9311817623661942e-05,
      "loss": 0.5336,
      "step": 77500
    },
    {
      "epoch": 1.2492992712420918,
      "grad_norm": 4.581326007843018,
      "learning_rate": 2.9178345479298473e-05,
      "loss": 0.5289,
      "step": 78000
    },
    {
      "epoch": 1.2573075999039,
      "grad_norm": 10.895811080932617,
      "learning_rate": 2.9044873334935e-05,
      "loss": 0.544,
      "step": 78500
    },
    {
      "epoch": 1.2653159285657083,
      "grad_norm": 4.070727825164795,
      "learning_rate": 2.891140119057153e-05,
      "loss": 0.5162,
      "step": 79000
    },
    {
      "epoch": 1.2733242572275167,
      "grad_norm": 4.06965970993042,
      "learning_rate": 2.8777929046208057e-05,
      "loss": 0.5294,
      "step": 79500
    },
    {
      "epoch": 1.281332585889325,
      "grad_norm": 4.579061985015869,
      "learning_rate": 2.8644456901844584e-05,
      "loss": 0.52,
      "step": 80000
    },
    {
      "epoch": 1.2893409145511332,
      "grad_norm": 6.460152626037598,
      "learning_rate": 2.8510984757481112e-05,
      "loss": 0.5258,
      "step": 80500
    },
    {
      "epoch": 1.2973492432129414,
      "grad_norm": 7.23961877822876,
      "learning_rate": 2.8377512613117647e-05,
      "loss": 0.5414,
      "step": 81000
    },
    {
      "epoch": 1.3053575718747497,
      "grad_norm": 2.1204023361206055,
      "learning_rate": 2.8244040468754174e-05,
      "loss": 0.5283,
      "step": 81500
    },
    {
      "epoch": 1.3133659005365579,
      "grad_norm": 5.8042073249816895,
      "learning_rate": 2.8110568324390702e-05,
      "loss": 0.5444,
      "step": 82000
    },
    {
      "epoch": 1.3213742291983663,
      "grad_norm": 3.8973348140716553,
      "learning_rate": 2.797709618002723e-05,
      "loss": 0.5247,
      "step": 82500
    },
    {
      "epoch": 1.3293825578601746,
      "grad_norm": 2.659574508666992,
      "learning_rate": 2.7843624035663758e-05,
      "loss": 0.5288,
      "step": 83000
    },
    {
      "epoch": 1.3373908865219828,
      "grad_norm": 3.706261157989502,
      "learning_rate": 2.7710151891300285e-05,
      "loss": 0.532,
      "step": 83500
    },
    {
      "epoch": 1.3453992151837912,
      "grad_norm": 20.325761795043945,
      "learning_rate": 2.7576679746936817e-05,
      "loss": 0.5168,
      "step": 84000
    },
    {
      "epoch": 1.3534075438455995,
      "grad_norm": 2.9860475063323975,
      "learning_rate": 2.7443207602573344e-05,
      "loss": 0.5249,
      "step": 84500
    },
    {
      "epoch": 1.3614158725074077,
      "grad_norm": 5.427518367767334,
      "learning_rate": 2.7309735458209872e-05,
      "loss": 0.5167,
      "step": 85000
    },
    {
      "epoch": 1.369424201169216,
      "grad_norm": 6.439121246337891,
      "learning_rate": 2.71762633138464e-05,
      "loss": 0.5392,
      "step": 85500
    },
    {
      "epoch": 1.3774325298310242,
      "grad_norm": 8.334936141967773,
      "learning_rate": 2.7042791169482927e-05,
      "loss": 0.5157,
      "step": 86000
    },
    {
      "epoch": 1.3854408584928326,
      "grad_norm": 4.830845355987549,
      "learning_rate": 2.6909319025119462e-05,
      "loss": 0.5368,
      "step": 86500
    },
    {
      "epoch": 1.3934491871546408,
      "grad_norm": 2.1246495246887207,
      "learning_rate": 2.677584688075599e-05,
      "loss": 0.5088,
      "step": 87000
    },
    {
      "epoch": 1.401457515816449,
      "grad_norm": 2.3075292110443115,
      "learning_rate": 2.6642374736392518e-05,
      "loss": 0.5368,
      "step": 87500
    },
    {
      "epoch": 1.4094658444782575,
      "grad_norm": 2.213160514831543,
      "learning_rate": 2.6508902592029045e-05,
      "loss": 0.5422,
      "step": 88000
    },
    {
      "epoch": 1.4174741731400657,
      "grad_norm": 12.27783489227295,
      "learning_rate": 2.6375430447665573e-05,
      "loss": 0.5403,
      "step": 88500
    },
    {
      "epoch": 1.425482501801874,
      "grad_norm": 4.510140419006348,
      "learning_rate": 2.62419583033021e-05,
      "loss": 0.5352,
      "step": 89000
    },
    {
      "epoch": 1.4334908304636822,
      "grad_norm": 4.781023025512695,
      "learning_rate": 2.6108486158938632e-05,
      "loss": 0.5149,
      "step": 89500
    },
    {
      "epoch": 1.4414991591254904,
      "grad_norm": 6.79545783996582,
      "learning_rate": 2.597501401457516e-05,
      "loss": 0.5373,
      "step": 90000
    },
    {
      "epoch": 1.4495074877872987,
      "grad_norm": 49.147579193115234,
      "learning_rate": 2.5841541870211687e-05,
      "loss": 0.5313,
      "step": 90500
    },
    {
      "epoch": 1.457515816449107,
      "grad_norm": 2.4180684089660645,
      "learning_rate": 2.5708069725848215e-05,
      "loss": 0.5341,
      "step": 91000
    },
    {
      "epoch": 1.4655241451109153,
      "grad_norm": 4.6710968017578125,
      "learning_rate": 2.5574597581484743e-05,
      "loss": 0.5254,
      "step": 91500
    },
    {
      "epoch": 1.4735324737727236,
      "grad_norm": 13.610833168029785,
      "learning_rate": 2.544112543712127e-05,
      "loss": 0.5334,
      "step": 92000
    },
    {
      "epoch": 1.481540802434532,
      "grad_norm": 3.6180546283721924,
      "learning_rate": 2.5307653292757805e-05,
      "loss": 0.5404,
      "step": 92500
    },
    {
      "epoch": 1.4895491310963402,
      "grad_norm": 9.872357368469238,
      "learning_rate": 2.5174181148394333e-05,
      "loss": 0.5243,
      "step": 93000
    },
    {
      "epoch": 1.4975574597581485,
      "grad_norm": 11.83433723449707,
      "learning_rate": 2.504070900403086e-05,
      "loss": 0.5212,
      "step": 93500
    },
    {
      "epoch": 1.5055657884199567,
      "grad_norm": 5.849399566650391,
      "learning_rate": 2.490723685966739e-05,
      "loss": 0.536,
      "step": 94000
    },
    {
      "epoch": 1.513574117081765,
      "grad_norm": 6.792448043823242,
      "learning_rate": 2.477376471530392e-05,
      "loss": 0.5139,
      "step": 94500
    },
    {
      "epoch": 1.5215824457435732,
      "grad_norm": 10.482234954833984,
      "learning_rate": 2.4640292570940447e-05,
      "loss": 0.514,
      "step": 95000
    },
    {
      "epoch": 1.5295907744053816,
      "grad_norm": 3.7406227588653564,
      "learning_rate": 2.4506820426576975e-05,
      "loss": 0.5278,
      "step": 95500
    },
    {
      "epoch": 1.5375991030671898,
      "grad_norm": 5.730695724487305,
      "learning_rate": 2.4373348282213503e-05,
      "loss": 0.5093,
      "step": 96000
    },
    {
      "epoch": 1.5456074317289983,
      "grad_norm": 3.3730945587158203,
      "learning_rate": 2.423987613785003e-05,
      "loss": 0.5235,
      "step": 96500
    },
    {
      "epoch": 1.5536157603908065,
      "grad_norm": 3.492950916290283,
      "learning_rate": 2.4106403993486558e-05,
      "loss": 0.5175,
      "step": 97000
    },
    {
      "epoch": 1.5616240890526147,
      "grad_norm": 6.554967880249023,
      "learning_rate": 2.397293184912309e-05,
      "loss": 0.5368,
      "step": 97500
    },
    {
      "epoch": 1.569632417714423,
      "grad_norm": 2.112107515335083,
      "learning_rate": 2.3839459704759617e-05,
      "loss": 0.5272,
      "step": 98000
    },
    {
      "epoch": 1.5776407463762312,
      "grad_norm": 6.589860439300537,
      "learning_rate": 2.3705987560396145e-05,
      "loss": 0.5219,
      "step": 98500
    },
    {
      "epoch": 1.5856490750380394,
      "grad_norm": 9.147485733032227,
      "learning_rate": 2.3572515416032676e-05,
      "loss": 0.5768,
      "step": 99000
    },
    {
      "epoch": 1.5936574036998479,
      "grad_norm": 2.691847801208496,
      "learning_rate": 2.3439043271669204e-05,
      "loss": 0.5257,
      "step": 99500
    },
    {
      "epoch": 1.601665732361656,
      "grad_norm": 2.4653239250183105,
      "learning_rate": 2.3305571127305735e-05,
      "loss": 0.522,
      "step": 100000
    },
    {
      "epoch": 1.6096740610234646,
      "grad_norm": 11.085479736328125,
      "learning_rate": 2.3172098982942263e-05,
      "loss": 0.5168,
      "step": 100500
    },
    {
      "epoch": 1.6176823896852728,
      "grad_norm": 10.273331642150879,
      "learning_rate": 2.303862683857879e-05,
      "loss": 0.5353,
      "step": 101000
    },
    {
      "epoch": 1.625690718347081,
      "grad_norm": 2.038649559020996,
      "learning_rate": 2.2905154694215318e-05,
      "loss": 0.5202,
      "step": 101500
    },
    {
      "epoch": 1.6336990470088892,
      "grad_norm": 4.0400004386901855,
      "learning_rate": 2.2771682549851846e-05,
      "loss": 0.5176,
      "step": 102000
    },
    {
      "epoch": 1.6417073756706975,
      "grad_norm": 2.4204163551330566,
      "learning_rate": 2.2638210405488374e-05,
      "loss": 0.5092,
      "step": 102500
    },
    {
      "epoch": 1.6497157043325057,
      "grad_norm": 3.1426825523376465,
      "learning_rate": 2.2504738261124905e-05,
      "loss": 0.5198,
      "step": 103000
    },
    {
      "epoch": 1.657724032994314,
      "grad_norm": 8.044781684875488,
      "learning_rate": 2.2371266116761433e-05,
      "loss": 0.5194,
      "step": 103500
    },
    {
      "epoch": 1.6657323616561224,
      "grad_norm": 3.9673469066619873,
      "learning_rate": 2.223779397239796e-05,
      "loss": 0.5217,
      "step": 104000
    },
    {
      "epoch": 1.6737406903179306,
      "grad_norm": 11.519086837768555,
      "learning_rate": 2.210432182803449e-05,
      "loss": 0.5095,
      "step": 104500
    },
    {
      "epoch": 1.681749018979739,
      "grad_norm": 5.464014530181885,
      "learning_rate": 2.197084968367102e-05,
      "loss": 0.5355,
      "step": 105000
    },
    {
      "epoch": 1.6897573476415473,
      "grad_norm": 4.318820476531982,
      "learning_rate": 2.1837377539307547e-05,
      "loss": 0.5205,
      "step": 105500
    },
    {
      "epoch": 1.6977656763033555,
      "grad_norm": 6.407961845397949,
      "learning_rate": 2.1703905394944078e-05,
      "loss": 0.5122,
      "step": 106000
    },
    {
      "epoch": 1.7057740049651637,
      "grad_norm": 8.025530815124512,
      "learning_rate": 2.1570433250580606e-05,
      "loss": 0.5217,
      "step": 106500
    },
    {
      "epoch": 1.713782333626972,
      "grad_norm": 4.854086399078369,
      "learning_rate": 2.1436961106217134e-05,
      "loss": 0.5269,
      "step": 107000
    },
    {
      "epoch": 1.7217906622887802,
      "grad_norm": 4.895215034484863,
      "learning_rate": 2.130348896185366e-05,
      "loss": 0.5468,
      "step": 107500
    },
    {
      "epoch": 1.7297989909505886,
      "grad_norm": 14.803995132446289,
      "learning_rate": 2.117001681749019e-05,
      "loss": 0.5172,
      "step": 108000
    },
    {
      "epoch": 1.7378073196123969,
      "grad_norm": 4.8039021492004395,
      "learning_rate": 2.103654467312672e-05,
      "loss": 0.5071,
      "step": 108500
    },
    {
      "epoch": 1.7458156482742053,
      "grad_norm": 6.83394718170166,
      "learning_rate": 2.0903072528763248e-05,
      "loss": 0.5062,
      "step": 109000
    },
    {
      "epoch": 1.7538239769360136,
      "grad_norm": 3.1579933166503906,
      "learning_rate": 2.0769600384399776e-05,
      "loss": 0.5295,
      "step": 109500
    },
    {
      "epoch": 1.7618323055978218,
      "grad_norm": 5.3173370361328125,
      "learning_rate": 2.0636128240036307e-05,
      "loss": 0.5125,
      "step": 110000
    },
    {
      "epoch": 1.76984063425963,
      "grad_norm": 3.3504600524902344,
      "learning_rate": 2.0502656095672835e-05,
      "loss": 0.5289,
      "step": 110500
    },
    {
      "epoch": 1.7778489629214382,
      "grad_norm": 1.5318541526794434,
      "learning_rate": 2.0369183951309362e-05,
      "loss": 0.519,
      "step": 111000
    },
    {
      "epoch": 1.7858572915832465,
      "grad_norm": 6.516898155212402,
      "learning_rate": 2.0235711806945893e-05,
      "loss": 0.5218,
      "step": 111500
    },
    {
      "epoch": 1.7938656202450547,
      "grad_norm": 5.115590572357178,
      "learning_rate": 2.010223966258242e-05,
      "loss": 0.5187,
      "step": 112000
    },
    {
      "epoch": 1.8018739489068631,
      "grad_norm": 1.8095836639404297,
      "learning_rate": 1.996876751821895e-05,
      "loss": 0.5086,
      "step": 112500
    },
    {
      "epoch": 1.8098822775686714,
      "grad_norm": 4.258097171783447,
      "learning_rate": 1.9835295373855477e-05,
      "loss": 0.5159,
      "step": 113000
    },
    {
      "epoch": 1.8178906062304798,
      "grad_norm": 3.70536732673645,
      "learning_rate": 1.9701823229492004e-05,
      "loss": 0.5177,
      "step": 113500
    },
    {
      "epoch": 1.825898934892288,
      "grad_norm": 4.52922248840332,
      "learning_rate": 1.9568351085128532e-05,
      "loss": 0.5163,
      "step": 114000
    },
    {
      "epoch": 1.8339072635540963,
      "grad_norm": 4.456188678741455,
      "learning_rate": 1.9434878940765063e-05,
      "loss": 0.5166,
      "step": 114500
    },
    {
      "epoch": 1.8419155922159045,
      "grad_norm": 3.5256004333496094,
      "learning_rate": 1.930140679640159e-05,
      "loss": 0.5269,
      "step": 115000
    },
    {
      "epoch": 1.8499239208777127,
      "grad_norm": 3.6694319248199463,
      "learning_rate": 1.916793465203812e-05,
      "loss": 0.5113,
      "step": 115500
    },
    {
      "epoch": 1.857932249539521,
      "grad_norm": 4.273138046264648,
      "learning_rate": 1.903446250767465e-05,
      "loss": 0.5139,
      "step": 116000
    },
    {
      "epoch": 1.8659405782013294,
      "grad_norm": 13.877301216125488,
      "learning_rate": 1.8900990363311178e-05,
      "loss": 0.5241,
      "step": 116500
    },
    {
      "epoch": 1.8739489068631376,
      "grad_norm": 3.783949851989746,
      "learning_rate": 1.876751821894771e-05,
      "loss": 0.5213,
      "step": 117000
    },
    {
      "epoch": 1.881957235524946,
      "grad_norm": 4.728069305419922,
      "learning_rate": 1.8634046074584237e-05,
      "loss": 0.527,
      "step": 117500
    },
    {
      "epoch": 1.8899655641867543,
      "grad_norm": 6.048025131225586,
      "learning_rate": 1.8500573930220764e-05,
      "loss": 0.5311,
      "step": 118000
    },
    {
      "epoch": 1.8979738928485625,
      "grad_norm": 6.48728609085083,
      "learning_rate": 1.8367101785857292e-05,
      "loss": 0.5388,
      "step": 118500
    },
    {
      "epoch": 1.9059822215103708,
      "grad_norm": 4.1666388511657715,
      "learning_rate": 1.823362964149382e-05,
      "loss": 0.5276,
      "step": 119000
    },
    {
      "epoch": 1.913990550172179,
      "grad_norm": 5.920313835144043,
      "learning_rate": 1.8100157497130348e-05,
      "loss": 0.5154,
      "step": 119500
    },
    {
      "epoch": 1.9219988788339872,
      "grad_norm": 5.515158176422119,
      "learning_rate": 1.796668535276688e-05,
      "loss": 0.5377,
      "step": 120000
    },
    {
      "epoch": 1.9300072074957955,
      "grad_norm": 4.867806911468506,
      "learning_rate": 1.7833213208403406e-05,
      "loss": 0.5142,
      "step": 120500
    },
    {
      "epoch": 1.938015536157604,
      "grad_norm": 5.740201473236084,
      "learning_rate": 1.7699741064039934e-05,
      "loss": 0.5121,
      "step": 121000
    },
    {
      "epoch": 1.9460238648194121,
      "grad_norm": 6.0920305252075195,
      "learning_rate": 1.7566268919676465e-05,
      "loss": 0.5073,
      "step": 121500
    },
    {
      "epoch": 1.9540321934812206,
      "grad_norm": 3.8682684898376465,
      "learning_rate": 1.7432796775312993e-05,
      "loss": 0.5235,
      "step": 122000
    },
    {
      "epoch": 1.9620405221430288,
      "grad_norm": 11.944962501525879,
      "learning_rate": 1.729932463094952e-05,
      "loss": 0.5218,
      "step": 122500
    },
    {
      "epoch": 1.970048850804837,
      "grad_norm": 4.42965841293335,
      "learning_rate": 1.7165852486586052e-05,
      "loss": 0.5051,
      "step": 123000
    },
    {
      "epoch": 1.9780571794666453,
      "grad_norm": 2.3707287311553955,
      "learning_rate": 1.703238034222258e-05,
      "loss": 0.51,
      "step": 123500
    },
    {
      "epoch": 1.9860655081284535,
      "grad_norm": 3.7566802501678467,
      "learning_rate": 1.6898908197859107e-05,
      "loss": 0.5182,
      "step": 124000
    },
    {
      "epoch": 1.9940738367902617,
      "grad_norm": 3.688204765319824,
      "learning_rate": 1.6765436053495635e-05,
      "loss": 0.5216,
      "step": 124500
    },
    {
      "epoch": 2.00208216545207,
      "grad_norm": 4.6394548416137695,
      "learning_rate": 1.6631963909132163e-05,
      "loss": 0.52,
      "step": 125000
    },
    {
      "epoch": 2.0100904941138786,
      "grad_norm": 3.345848560333252,
      "learning_rate": 1.6498491764768694e-05,
      "loss": 0.491,
      "step": 125500
    },
    {
      "epoch": 2.018098822775687,
      "grad_norm": 6.613098621368408,
      "learning_rate": 1.6365019620405222e-05,
      "loss": 0.4849,
      "step": 126000
    },
    {
      "epoch": 2.026107151437495,
      "grad_norm": 5.686723232269287,
      "learning_rate": 1.623154747604175e-05,
      "loss": 0.4725,
      "step": 126500
    },
    {
      "epoch": 2.0341154800993033,
      "grad_norm": 4.618155479431152,
      "learning_rate": 1.609807533167828e-05,
      "loss": 0.4939,
      "step": 127000
    },
    {
      "epoch": 2.0421238087611115,
      "grad_norm": 7.990642547607422,
      "learning_rate": 1.596460318731481e-05,
      "loss": 0.4953,
      "step": 127500
    },
    {
      "epoch": 2.0501321374229198,
      "grad_norm": 6.9659929275512695,
      "learning_rate": 1.5831131042951336e-05,
      "loss": 0.4811,
      "step": 128000
    },
    {
      "epoch": 2.058140466084728,
      "grad_norm": 4.644782543182373,
      "learning_rate": 1.5697658898587867e-05,
      "loss": 0.4862,
      "step": 128500
    },
    {
      "epoch": 2.0661487947465362,
      "grad_norm": 8.351694107055664,
      "learning_rate": 1.5564186754224395e-05,
      "loss": 0.4901,
      "step": 129000
    },
    {
      "epoch": 2.0741571234083445,
      "grad_norm": 4.961209297180176,
      "learning_rate": 1.5430714609860923e-05,
      "loss": 0.4705,
      "step": 129500
    },
    {
      "epoch": 2.082165452070153,
      "grad_norm": 11.790966033935547,
      "learning_rate": 1.529724246549745e-05,
      "loss": 0.4914,
      "step": 130000
    },
    {
      "epoch": 2.0901737807319614,
      "grad_norm": 15.83209228515625,
      "learning_rate": 1.516377032113398e-05,
      "loss": 0.4955,
      "step": 130500
    },
    {
      "epoch": 2.0981821093937696,
      "grad_norm": 6.013333320617676,
      "learning_rate": 1.5030298176770508e-05,
      "loss": 0.4834,
      "step": 131000
    },
    {
      "epoch": 2.106190438055578,
      "grad_norm": 8.343938827514648,
      "learning_rate": 1.4896826032407037e-05,
      "loss": 0.4839,
      "step": 131500
    },
    {
      "epoch": 2.114198766717386,
      "grad_norm": 8.838165283203125,
      "learning_rate": 1.4763353888043565e-05,
      "loss": 0.4923,
      "step": 132000
    },
    {
      "epoch": 2.1222070953791943,
      "grad_norm": 3.5784082412719727,
      "learning_rate": 1.4629881743680093e-05,
      "loss": 0.4999,
      "step": 132500
    },
    {
      "epoch": 2.1302154240410025,
      "grad_norm": 6.789596080780029,
      "learning_rate": 1.4496409599316624e-05,
      "loss": 0.4979,
      "step": 133000
    },
    {
      "epoch": 2.1382237527028107,
      "grad_norm": 3.2773313522338867,
      "learning_rate": 1.4362937454953152e-05,
      "loss": 0.4843,
      "step": 133500
    },
    {
      "epoch": 2.1462320813646194,
      "grad_norm": 7.179351806640625,
      "learning_rate": 1.4229465310589681e-05,
      "loss": 0.4835,
      "step": 134000
    },
    {
      "epoch": 2.1542404100264276,
      "grad_norm": 6.873589992523193,
      "learning_rate": 1.4095993166226209e-05,
      "loss": 0.5038,
      "step": 134500
    },
    {
      "epoch": 2.162248738688236,
      "grad_norm": 5.110401153564453,
      "learning_rate": 1.3962521021862737e-05,
      "loss": 0.4966,
      "step": 135000
    },
    {
      "epoch": 2.170257067350044,
      "grad_norm": 3.0599632263183594,
      "learning_rate": 1.3829048877499268e-05,
      "loss": 0.4974,
      "step": 135500
    },
    {
      "epoch": 2.1782653960118523,
      "grad_norm": 5.403199672698975,
      "learning_rate": 1.3695576733135795e-05,
      "loss": 0.4883,
      "step": 136000
    },
    {
      "epoch": 2.1862737246736605,
      "grad_norm": 10.166191101074219,
      "learning_rate": 1.3562104588772323e-05,
      "loss": 0.4801,
      "step": 136500
    },
    {
      "epoch": 2.1942820533354688,
      "grad_norm": 3.9219131469726562,
      "learning_rate": 1.3428632444408853e-05,
      "loss": 0.4995,
      "step": 137000
    },
    {
      "epoch": 2.202290381997277,
      "grad_norm": 9.928403854370117,
      "learning_rate": 1.329516030004538e-05,
      "loss": 0.4904,
      "step": 137500
    },
    {
      "epoch": 2.2102987106590852,
      "grad_norm": 7.370477199554443,
      "learning_rate": 1.3161688155681908e-05,
      "loss": 0.4715,
      "step": 138000
    },
    {
      "epoch": 2.218307039320894,
      "grad_norm": 4.17348575592041,
      "learning_rate": 1.302821601131844e-05,
      "loss": 0.4946,
      "step": 138500
    },
    {
      "epoch": 2.226315367982702,
      "grad_norm": 4.908074378967285,
      "learning_rate": 1.2894743866954967e-05,
      "loss": 0.4942,
      "step": 139000
    },
    {
      "epoch": 2.2343236966445104,
      "grad_norm": 5.767423152923584,
      "learning_rate": 1.2761271722591495e-05,
      "loss": 0.5023,
      "step": 139500
    },
    {
      "epoch": 2.2423320253063186,
      "grad_norm": 6.429279327392578,
      "learning_rate": 1.2627799578228024e-05,
      "loss": 0.4977,
      "step": 140000
    },
    {
      "epoch": 2.250340353968127,
      "grad_norm": 3.7908496856689453,
      "learning_rate": 1.2494327433864552e-05,
      "loss": 0.49,
      "step": 140500
    },
    {
      "epoch": 2.258348682629935,
      "grad_norm": 3.7526662349700928,
      "learning_rate": 1.2360855289501081e-05,
      "loss": 0.4814,
      "step": 141000
    },
    {
      "epoch": 2.2663570112917433,
      "grad_norm": 5.195950984954834,
      "learning_rate": 1.2227383145137611e-05,
      "loss": 0.4966,
      "step": 141500
    },
    {
      "epoch": 2.2743653399535515,
      "grad_norm": 2.5419557094573975,
      "learning_rate": 1.2093911000774139e-05,
      "loss": 0.4843,
      "step": 142000
    },
    {
      "epoch": 2.28237366861536,
      "grad_norm": 4.773759365081787,
      "learning_rate": 1.1960438856410668e-05,
      "loss": 0.4959,
      "step": 142500
    },
    {
      "epoch": 2.2903819972771684,
      "grad_norm": 1.8961355686187744,
      "learning_rate": 1.1826966712047197e-05,
      "loss": 0.4822,
      "step": 143000
    },
    {
      "epoch": 2.2983903259389766,
      "grad_norm": 5.982407093048096,
      "learning_rate": 1.1693494567683724e-05,
      "loss": 0.468,
      "step": 143500
    },
    {
      "epoch": 2.306398654600785,
      "grad_norm": 9.01443862915039,
      "learning_rate": 1.1560022423320253e-05,
      "loss": 0.4931,
      "step": 144000
    },
    {
      "epoch": 2.314406983262593,
      "grad_norm": 5.718390941619873,
      "learning_rate": 1.1426550278956782e-05,
      "loss": 0.5046,
      "step": 144500
    },
    {
      "epoch": 2.3224153119244013,
      "grad_norm": 3.213411808013916,
      "learning_rate": 1.1293078134593312e-05,
      "loss": 0.502,
      "step": 145000
    },
    {
      "epoch": 2.3304236405862095,
      "grad_norm": 5.6064839363098145,
      "learning_rate": 1.115960599022984e-05,
      "loss": 0.4843,
      "step": 145500
    },
    {
      "epoch": 2.3384319692480178,
      "grad_norm": 12.452762603759766,
      "learning_rate": 1.1026133845866369e-05,
      "loss": 0.4709,
      "step": 146000
    },
    {
      "epoch": 2.346440297909826,
      "grad_norm": 7.003706455230713,
      "learning_rate": 1.0892661701502897e-05,
      "loss": 0.4828,
      "step": 146500
    },
    {
      "epoch": 2.3544486265716347,
      "grad_norm": 3.095332145690918,
      "learning_rate": 1.0759189557139425e-05,
      "loss": 0.4759,
      "step": 147000
    },
    {
      "epoch": 2.362456955233443,
      "grad_norm": 4.040911674499512,
      "learning_rate": 1.0625717412775954e-05,
      "loss": 0.4848,
      "step": 147500
    },
    {
      "epoch": 2.370465283895251,
      "grad_norm": 3.7055633068084717,
      "learning_rate": 1.0492245268412483e-05,
      "loss": 0.4879,
      "step": 148000
    },
    {
      "epoch": 2.3784736125570594,
      "grad_norm": 3.9870054721832275,
      "learning_rate": 1.0358773124049013e-05,
      "loss": 0.4896,
      "step": 148500
    },
    {
      "epoch": 2.3864819412188676,
      "grad_norm": 2.65230393409729,
      "learning_rate": 1.022530097968554e-05,
      "loss": 0.4927,
      "step": 149000
    },
    {
      "epoch": 2.394490269880676,
      "grad_norm": 8.814361572265625,
      "learning_rate": 1.0091828835322068e-05,
      "loss": 0.4972,
      "step": 149500
    },
    {
      "epoch": 2.402498598542484,
      "grad_norm": 7.068905830383301,
      "learning_rate": 9.958356690958598e-06,
      "loss": 0.4935,
      "step": 150000
    },
    {
      "epoch": 2.4105069272042923,
      "grad_norm": 6.750758171081543,
      "learning_rate": 9.824884546595126e-06,
      "loss": 0.4888,
      "step": 150500
    },
    {
      "epoch": 2.418515255866101,
      "grad_norm": 1.9101755619049072,
      "learning_rate": 9.691412402231655e-06,
      "loss": 0.4748,
      "step": 151000
    },
    {
      "epoch": 2.426523584527909,
      "grad_norm": 4.402417182922363,
      "learning_rate": 9.557940257868184e-06,
      "loss": 0.4644,
      "step": 151500
    },
    {
      "epoch": 2.4345319131897174,
      "grad_norm": 5.037499904632568,
      "learning_rate": 9.424468113504712e-06,
      "loss": 0.4858,
      "step": 152000
    },
    {
      "epoch": 2.4425402418515256,
      "grad_norm": 7.066239356994629,
      "learning_rate": 9.29099596914124e-06,
      "loss": 0.4827,
      "step": 152500
    },
    {
      "epoch": 2.450548570513334,
      "grad_norm": 6.070245742797852,
      "learning_rate": 9.15752382477777e-06,
      "loss": 0.465,
      "step": 153000
    },
    {
      "epoch": 2.458556899175142,
      "grad_norm": 4.008568286895752,
      "learning_rate": 9.024051680414299e-06,
      "loss": 0.4721,
      "step": 153500
    },
    {
      "epoch": 2.4665652278369503,
      "grad_norm": 7.347427845001221,
      "learning_rate": 8.890579536050827e-06,
      "loss": 0.4727,
      "step": 154000
    },
    {
      "epoch": 2.4745735564987585,
      "grad_norm": 3.486144781112671,
      "learning_rate": 8.757107391687356e-06,
      "loss": 0.5034,
      "step": 154500
    },
    {
      "epoch": 2.4825818851605668,
      "grad_norm": 3.234773874282837,
      "learning_rate": 8.623635247323884e-06,
      "loss": 0.4851,
      "step": 155000
    },
    {
      "epoch": 2.4905902138223754,
      "grad_norm": 4.342700958251953,
      "learning_rate": 8.490163102960412e-06,
      "loss": 0.48,
      "step": 155500
    },
    {
      "epoch": 2.4985985424841837,
      "grad_norm": 7.917558193206787,
      "learning_rate": 8.356690958596941e-06,
      "loss": 0.4699,
      "step": 156000
    },
    {
      "epoch": 2.506606871145992,
      "grad_norm": 4.106176853179932,
      "learning_rate": 8.22321881423347e-06,
      "loss": 0.4864,
      "step": 156500
    },
    {
      "epoch": 2.5146151998078,
      "grad_norm": 4.815080165863037,
      "learning_rate": 8.08974666987e-06,
      "loss": 0.4763,
      "step": 157000
    },
    {
      "epoch": 2.5226235284696084,
      "grad_norm": 5.983972072601318,
      "learning_rate": 7.956274525506528e-06,
      "loss": 0.5105,
      "step": 157500
    },
    {
      "epoch": 2.5306318571314166,
      "grad_norm": 4.014471530914307,
      "learning_rate": 7.822802381143055e-06,
      "loss": 0.4797,
      "step": 158000
    },
    {
      "epoch": 2.538640185793225,
      "grad_norm": 5.227955341339111,
      "learning_rate": 7.689330236779585e-06,
      "loss": 0.4953,
      "step": 158500
    },
    {
      "epoch": 2.5466485144550335,
      "grad_norm": 3.9301795959472656,
      "learning_rate": 7.5558580924161125e-06,
      "loss": 0.4772,
      "step": 159000
    },
    {
      "epoch": 2.5546568431168417,
      "grad_norm": 9.068924903869629,
      "learning_rate": 7.422385948052642e-06,
      "loss": 0.4985,
      "step": 159500
    },
    {
      "epoch": 2.56266517177865,
      "grad_norm": 15.708979606628418,
      "learning_rate": 7.2889138036891706e-06,
      "loss": 0.4753,
      "step": 160000
    },
    {
      "epoch": 2.570673500440458,
      "grad_norm": 5.85085391998291,
      "learning_rate": 7.155441659325698e-06,
      "loss": 0.4959,
      "step": 160500
    },
    {
      "epoch": 2.5786818291022664,
      "grad_norm": 10.106379508972168,
      "learning_rate": 7.021969514962228e-06,
      "loss": 0.4785,
      "step": 161000
    },
    {
      "epoch": 2.5866901577640746,
      "grad_norm": 3.3619790077209473,
      "learning_rate": 6.888497370598756e-06,
      "loss": 0.4888,
      "step": 161500
    },
    {
      "epoch": 2.594698486425883,
      "grad_norm": 7.827800750732422,
      "learning_rate": 6.755025226235286e-06,
      "loss": 0.492,
      "step": 162000
    },
    {
      "epoch": 2.602706815087691,
      "grad_norm": 6.471998691558838,
      "learning_rate": 6.6215530818718135e-06,
      "loss": 0.4951,
      "step": 162500
    },
    {
      "epoch": 2.6107151437494993,
      "grad_norm": 2.487199544906616,
      "learning_rate": 6.488080937508342e-06,
      "loss": 0.4862,
      "step": 163000
    },
    {
      "epoch": 2.6187234724113075,
      "grad_norm": 7.580549240112305,
      "learning_rate": 6.3546087931448716e-06,
      "loss": 0.4932,
      "step": 163500
    },
    {
      "epoch": 2.6267318010731158,
      "grad_norm": 2.266861915588379,
      "learning_rate": 6.2211366487814e-06,
      "loss": 0.4804,
      "step": 164000
    },
    {
      "epoch": 2.6347401297349244,
      "grad_norm": 5.710116386413574,
      "learning_rate": 6.087664504417928e-06,
      "loss": 0.4715,
      "step": 164500
    },
    {
      "epoch": 2.6427484583967327,
      "grad_norm": 5.276626110076904,
      "learning_rate": 5.954192360054457e-06,
      "loss": 0.4886,
      "step": 165000
    },
    {
      "epoch": 2.650756787058541,
      "grad_norm": 7.006643295288086,
      "learning_rate": 5.820720215690986e-06,
      "loss": 0.479,
      "step": 165500
    },
    {
      "epoch": 2.658765115720349,
      "grad_norm": 4.763871669769287,
      "learning_rate": 5.687248071327514e-06,
      "loss": 0.4769,
      "step": 166000
    },
    {
      "epoch": 2.6667734443821574,
      "grad_norm": 2.7046520709991455,
      "learning_rate": 5.553775926964043e-06,
      "loss": 0.4932,
      "step": 166500
    },
    {
      "epoch": 2.6747817730439656,
      "grad_norm": 4.3285064697265625,
      "learning_rate": 5.420303782600572e-06,
      "loss": 0.4792,
      "step": 167000
    },
    {
      "epoch": 2.6827901017057743,
      "grad_norm": 6.022039413452148,
      "learning_rate": 5.2868316382371e-06,
      "loss": 0.4909,
      "step": 167500
    },
    {
      "epoch": 2.6907984303675825,
      "grad_norm": 1.3580108880996704,
      "learning_rate": 5.153359493873629e-06,
      "loss": 0.4817,
      "step": 168000
    },
    {
      "epoch": 2.6988067590293907,
      "grad_norm": 9.179396629333496,
      "learning_rate": 5.0198873495101575e-06,
      "loss": 0.4846,
      "step": 168500
    },
    {
      "epoch": 2.706815087691199,
      "grad_norm": 4.357848644256592,
      "learning_rate": 4.886415205146686e-06,
      "loss": 0.4766,
      "step": 169000
    },
    {
      "epoch": 2.714823416353007,
      "grad_norm": 9.21454906463623,
      "learning_rate": 4.752943060783215e-06,
      "loss": 0.481,
      "step": 169500
    },
    {
      "epoch": 2.7228317450148154,
      "grad_norm": 3.493400812149048,
      "learning_rate": 4.619470916419743e-06,
      "loss": 0.4807,
      "step": 170000
    },
    {
      "epoch": 2.7308400736766236,
      "grad_norm": 4.984789848327637,
      "learning_rate": 4.485998772056272e-06,
      "loss": 0.4876,
      "step": 170500
    },
    {
      "epoch": 2.738848402338432,
      "grad_norm": 6.943166732788086,
      "learning_rate": 4.352526627692801e-06,
      "loss": 0.4854,
      "step": 171000
    },
    {
      "epoch": 2.74685673100024,
      "grad_norm": 4.886231422424316,
      "learning_rate": 4.219054483329329e-06,
      "loss": 0.4858,
      "step": 171500
    },
    {
      "epoch": 2.7548650596620483,
      "grad_norm": 6.6674041748046875,
      "learning_rate": 4.085582338965858e-06,
      "loss": 0.4793,
      "step": 172000
    },
    {
      "epoch": 2.7628733883238565,
      "grad_norm": 7.113861083984375,
      "learning_rate": 3.952110194602387e-06,
      "loss": 0.4859,
      "step": 172500
    },
    {
      "epoch": 2.770881716985665,
      "grad_norm": 8.29572868347168,
      "learning_rate": 3.818638050238915e-06,
      "loss": 0.4846,
      "step": 173000
    },
    {
      "epoch": 2.7788900456474734,
      "grad_norm": 12.40919303894043,
      "learning_rate": 3.6851659058754443e-06,
      "loss": 0.4897,
      "step": 173500
    },
    {
      "epoch": 2.7868983743092817,
      "grad_norm": 6.548822402954102,
      "learning_rate": 3.5516937615119725e-06,
      "loss": 0.4744,
      "step": 174000
    },
    {
      "epoch": 2.79490670297109,
      "grad_norm": 5.141139984130859,
      "learning_rate": 3.418221617148501e-06,
      "loss": 0.4862,
      "step": 174500
    },
    {
      "epoch": 2.802915031632898,
      "grad_norm": 4.598352432250977,
      "learning_rate": 3.28474947278503e-06,
      "loss": 0.4849,
      "step": 175000
    },
    {
      "epoch": 2.8109233602947064,
      "grad_norm": 12.266292572021484,
      "learning_rate": 3.1512773284215583e-06,
      "loss": 0.4788,
      "step": 175500
    },
    {
      "epoch": 2.818931688956515,
      "grad_norm": 3.243722677230835,
      "learning_rate": 3.0178051840580873e-06,
      "loss": 0.4959,
      "step": 176000
    },
    {
      "epoch": 2.8269400176183233,
      "grad_norm": 9.964518547058105,
      "learning_rate": 2.884333039694616e-06,
      "loss": 0.4876,
      "step": 176500
    },
    {
      "epoch": 2.8349483462801315,
      "grad_norm": 4.309595584869385,
      "learning_rate": 2.7508608953311445e-06,
      "loss": 0.4772,
      "step": 177000
    },
    {
      "epoch": 2.8429566749419397,
      "grad_norm": 3.8585689067840576,
      "learning_rate": 2.617388750967673e-06,
      "loss": 0.485,
      "step": 177500
    },
    {
      "epoch": 2.850965003603748,
      "grad_norm": 4.30459451675415,
      "learning_rate": 2.483916606604202e-06,
      "loss": 0.4755,
      "step": 178000
    },
    {
      "epoch": 2.858973332265556,
      "grad_norm": 5.043994903564453,
      "learning_rate": 2.3504444622407303e-06,
      "loss": 0.4654,
      "step": 178500
    },
    {
      "epoch": 2.8669816609273644,
      "grad_norm": 9.565049171447754,
      "learning_rate": 2.216972317877259e-06,
      "loss": 0.4872,
      "step": 179000
    },
    {
      "epoch": 2.8749899895891726,
      "grad_norm": 10.213244438171387,
      "learning_rate": 2.083500173513788e-06,
      "loss": 0.4844,
      "step": 179500
    },
    {
      "epoch": 2.882998318250981,
      "grad_norm": 2.163796901702881,
      "learning_rate": 1.9500280291503165e-06,
      "loss": 0.4827,
      "step": 180000
    },
    {
      "epoch": 2.891006646912789,
      "grad_norm": 6.1276044845581055,
      "learning_rate": 1.816555884786845e-06,
      "loss": 0.4786,
      "step": 180500
    },
    {
      "epoch": 2.8990149755745973,
      "grad_norm": 4.01878547668457,
      "learning_rate": 1.6830837404233739e-06,
      "loss": 0.4927,
      "step": 181000
    },
    {
      "epoch": 2.907023304236406,
      "grad_norm": 9.985204696655273,
      "learning_rate": 1.5496115960599025e-06,
      "loss": 0.478,
      "step": 181500
    },
    {
      "epoch": 2.915031632898214,
      "grad_norm": 2.642123222351074,
      "learning_rate": 1.416139451696431e-06,
      "loss": 0.4762,
      "step": 182000
    },
    {
      "epoch": 2.9230399615600224,
      "grad_norm": 1.8521270751953125,
      "learning_rate": 1.2826673073329597e-06,
      "loss": 0.4527,
      "step": 182500
    },
    {
      "epoch": 2.9310482902218307,
      "grad_norm": 10.468952178955078,
      "learning_rate": 1.1491951629694885e-06,
      "loss": 0.5051,
      "step": 183000
    },
    {
      "epoch": 2.939056618883639,
      "grad_norm": 1.8325883150100708,
      "learning_rate": 1.0157230186060169e-06,
      "loss": 0.4791,
      "step": 183500
    },
    {
      "epoch": 2.947064947545447,
      "grad_norm": 6.917385578155518,
      "learning_rate": 8.822508742425457e-07,
      "loss": 0.4678,
      "step": 184000
    },
    {
      "epoch": 2.955073276207256,
      "grad_norm": 2.300797939300537,
      "learning_rate": 7.487787298790743e-07,
      "loss": 0.4817,
      "step": 184500
    },
    {
      "epoch": 2.963081604869064,
      "grad_norm": 7.867565631866455,
      "learning_rate": 6.153065855156028e-07,
      "loss": 0.4738,
      "step": 185000
    },
    {
      "epoch": 2.9710899335308723,
      "grad_norm": 9.112363815307617,
      "learning_rate": 4.818344411521315e-07,
      "loss": 0.483,
      "step": 185500
    },
    {
      "epoch": 2.9790982621926805,
      "grad_norm": 6.043625354766846,
      "learning_rate": 3.483622967886602e-07,
      "loss": 0.4814,
      "step": 186000
    },
    {
      "epoch": 2.9871065908544887,
      "grad_norm": 5.873631000518799,
      "learning_rate": 2.148901524251889e-07,
      "loss": 0.4721,
      "step": 186500
    },
    {
      "epoch": 2.995114919516297,
      "grad_norm": 11.005748748779297,
      "learning_rate": 8.141800806171753e-08,
      "loss": 0.4793,
      "step": 187000
    },
    {
      "epoch": 3.0028827674567586,
      "grad_norm": 4.015688419342041,
      "learning_rate": 2.4975976937860345e-05,
      "loss": 0.534,
      "step": 187500
    },
    {
      "epoch": 3.0108904548366433,
      "grad_norm": 5.157755374908447,
      "learning_rate": 2.4909246209694642e-05,
      "loss": 0.5275,
      "step": 188000
    },
    {
      "epoch": 3.018898142216528,
      "grad_norm": 5.690236568450928,
      "learning_rate": 2.4842515481528936e-05,
      "loss": 0.5143,
      "step": 188500
    },
    {
      "epoch": 3.0269058295964126,
      "grad_norm": 3.1381242275238037,
      "learning_rate": 2.477578475336323e-05,
      "loss": 0.5168,
      "step": 189000
    },
    {
      "epoch": 3.0349135169762973,
      "grad_norm": 5.241495609283447,
      "learning_rate": 2.4709054025197524e-05,
      "loss": 0.5325,
      "step": 189500
    },
    {
      "epoch": 3.042921204356182,
      "grad_norm": 25.723737716674805,
      "learning_rate": 2.4642323297031818e-05,
      "loss": 0.514,
      "step": 190000
    },
    {
      "epoch": 3.0509288917360666,
      "grad_norm": 4.157095909118652,
      "learning_rate": 2.457559256886611e-05,
      "loss": 0.5106,
      "step": 190500
    },
    {
      "epoch": 3.0589365791159513,
      "grad_norm": 2.689317226409912,
      "learning_rate": 2.4508861840700405e-05,
      "loss": 0.5235,
      "step": 191000
    },
    {
      "epoch": 3.066944266495836,
      "grad_norm": 3.8834471702575684,
      "learning_rate": 2.4442131112534703e-05,
      "loss": 0.5163,
      "step": 191500
    },
    {
      "epoch": 3.0749519538757206,
      "grad_norm": 4.402151107788086,
      "learning_rate": 2.4375400384368996e-05,
      "loss": 0.5401,
      "step": 192000
    },
    {
      "epoch": 3.0829596412556053,
      "grad_norm": 5.29595947265625,
      "learning_rate": 2.430866965620329e-05,
      "loss": 0.5272,
      "step": 192500
    },
    {
      "epoch": 3.09096732863549,
      "grad_norm": 4.661130428314209,
      "learning_rate": 2.4241938928037584e-05,
      "loss": 0.5213,
      "step": 193000
    },
    {
      "epoch": 3.0989750160153746,
      "grad_norm": 4.388880729675293,
      "learning_rate": 2.4175208199871878e-05,
      "loss": 0.5206,
      "step": 193500
    },
    {
      "epoch": 3.1069827033952593,
      "grad_norm": 3.181835412979126,
      "learning_rate": 2.4108477471706172e-05,
      "loss": 0.5336,
      "step": 194000
    },
    {
      "epoch": 3.114990390775144,
      "grad_norm": 3.5677480697631836,
      "learning_rate": 2.4041746743540466e-05,
      "loss": 0.537,
      "step": 194500
    },
    {
      "epoch": 3.122998078155029,
      "grad_norm": 8.93224048614502,
      "learning_rate": 2.397501601537476e-05,
      "loss": 0.5083,
      "step": 195000
    },
    {
      "epoch": 3.1310057655349137,
      "grad_norm": 2.1595449447631836,
      "learning_rate": 2.3908285287209057e-05,
      "loss": 0.5143,
      "step": 195500
    },
    {
      "epoch": 3.1390134529147984,
      "grad_norm": 2.918813943862915,
      "learning_rate": 2.384155455904335e-05,
      "loss": 0.5363,
      "step": 196000
    },
    {
      "epoch": 3.147021140294683,
      "grad_norm": 4.768782138824463,
      "learning_rate": 2.3774823830877645e-05,
      "loss": 0.5343,
      "step": 196500
    },
    {
      "epoch": 3.1550288276745677,
      "grad_norm": 3.8610031604766846,
      "learning_rate": 2.3708093102711935e-05,
      "loss": 0.5143,
      "step": 197000
    },
    {
      "epoch": 3.1630365150544524,
      "grad_norm": 4.565296173095703,
      "learning_rate": 2.3641362374546233e-05,
      "loss": 0.5154,
      "step": 197500
    },
    {
      "epoch": 3.171044202434337,
      "grad_norm": 3.290900468826294,
      "learning_rate": 2.3574631646380526e-05,
      "loss": 0.5269,
      "step": 198000
    },
    {
      "epoch": 3.1790518898142217,
      "grad_norm": 2.7042393684387207,
      "learning_rate": 2.350790091821482e-05,
      "loss": 0.525,
      "step": 198500
    },
    {
      "epoch": 3.1870595771941064,
      "grad_norm": 3.5647687911987305,
      "learning_rate": 2.3441170190049114e-05,
      "loss": 0.5401,
      "step": 199000
    },
    {
      "epoch": 3.195067264573991,
      "grad_norm": 9.507119178771973,
      "learning_rate": 2.337443946188341e-05,
      "loss": 0.5097,
      "step": 199500
    },
    {
      "epoch": 3.2030749519538757,
      "grad_norm": 4.8094940185546875,
      "learning_rate": 2.3307708733717702e-05,
      "loss": 0.5413,
      "step": 200000
    },
    {
      "epoch": 3.2110826393337604,
      "grad_norm": 2.002091646194458,
      "learning_rate": 2.3240978005551996e-05,
      "loss": 0.5189,
      "step": 200500
    },
    {
      "epoch": 3.219090326713645,
      "grad_norm": 3.932798147201538,
      "learning_rate": 2.317424727738629e-05,
      "loss": 0.5298,
      "step": 201000
    },
    {
      "epoch": 3.2270980140935297,
      "grad_norm": 3.5405900478363037,
      "learning_rate": 2.3107516549220587e-05,
      "loss": 0.5306,
      "step": 201500
    },
    {
      "epoch": 3.2351057014734144,
      "grad_norm": 2.8857152462005615,
      "learning_rate": 2.304078582105488e-05,
      "loss": 0.5109,
      "step": 202000
    },
    {
      "epoch": 3.243113388853299,
      "grad_norm": 5.128933429718018,
      "learning_rate": 2.2974055092889175e-05,
      "loss": 0.5245,
      "step": 202500
    },
    {
      "epoch": 3.2511210762331837,
      "grad_norm": 3.2885260581970215,
      "learning_rate": 2.2907324364723472e-05,
      "loss": 0.529,
      "step": 203000
    },
    {
      "epoch": 3.2591287636130684,
      "grad_norm": 3.4679770469665527,
      "learning_rate": 2.2840593636557762e-05,
      "loss": 0.5214,
      "step": 203500
    },
    {
      "epoch": 3.267136450992953,
      "grad_norm": 6.298839092254639,
      "learning_rate": 2.2773862908392056e-05,
      "loss": 0.5263,
      "step": 204000
    },
    {
      "epoch": 3.2751441383728377,
      "grad_norm": 7.017887115478516,
      "learning_rate": 2.270713218022635e-05,
      "loss": 0.5144,
      "step": 204500
    },
    {
      "epoch": 3.283151825752723,
      "grad_norm": 4.024120330810547,
      "learning_rate": 2.2640401452060647e-05,
      "loss": 0.5206,
      "step": 205000
    },
    {
      "epoch": 3.2911595131326075,
      "grad_norm": 5.880560874938965,
      "learning_rate": 2.257367072389494e-05,
      "loss": 0.5137,
      "step": 205500
    },
    {
      "epoch": 3.299167200512492,
      "grad_norm": 7.701251983642578,
      "learning_rate": 2.2506939995729235e-05,
      "loss": 0.5226,
      "step": 206000
    },
    {
      "epoch": 3.307174887892377,
      "grad_norm": 3.6405086517333984,
      "learning_rate": 2.244020926756353e-05,
      "loss": 0.5285,
      "step": 206500
    },
    {
      "epoch": 3.3151825752722615,
      "grad_norm": 5.838491439819336,
      "learning_rate": 2.2373478539397823e-05,
      "loss": 0.5199,
      "step": 207000
    },
    {
      "epoch": 3.323190262652146,
      "grad_norm": 3.035240888595581,
      "learning_rate": 2.2306747811232117e-05,
      "loss": 0.5235,
      "step": 207500
    },
    {
      "epoch": 3.331197950032031,
      "grad_norm": 25.145437240600586,
      "learning_rate": 2.224001708306641e-05,
      "loss": 0.511,
      "step": 208000
    },
    {
      "epoch": 3.3392056374119155,
      "grad_norm": 2.441861867904663,
      "learning_rate": 2.2173286354900705e-05,
      "loss": 0.5395,
      "step": 208500
    },
    {
      "epoch": 3.3472133247918,
      "grad_norm": 6.42642068862915,
      "learning_rate": 2.2106555626735002e-05,
      "loss": 0.5188,
      "step": 209000
    },
    {
      "epoch": 3.355221012171685,
      "grad_norm": 5.771033763885498,
      "learning_rate": 2.2039824898569296e-05,
      "loss": 0.5285,
      "step": 209500
    },
    {
      "epoch": 3.3632286995515694,
      "grad_norm": 5.8149309158325195,
      "learning_rate": 2.197309417040359e-05,
      "loss": 0.5206,
      "step": 210000
    },
    {
      "epoch": 3.371236386931454,
      "grad_norm": 7.800816059112549,
      "learning_rate": 2.190636344223788e-05,
      "loss": 0.5128,
      "step": 210500
    },
    {
      "epoch": 3.3792440743113388,
      "grad_norm": 6.287461280822754,
      "learning_rate": 2.1839632714072177e-05,
      "loss": 0.5238,
      "step": 211000
    },
    {
      "epoch": 3.3872517616912234,
      "grad_norm": 2.292232036590576,
      "learning_rate": 2.177290198590647e-05,
      "loss": 0.519,
      "step": 211500
    },
    {
      "epoch": 3.395259449071108,
      "grad_norm": 5.177983283996582,
      "learning_rate": 2.1706171257740765e-05,
      "loss": 0.5271,
      "step": 212000
    },
    {
      "epoch": 3.4032671364509928,
      "grad_norm": 5.217452049255371,
      "learning_rate": 2.163944052957506e-05,
      "loss": 0.5236,
      "step": 212500
    },
    {
      "epoch": 3.411274823830878,
      "grad_norm": 8.582279205322266,
      "learning_rate": 2.1572709801409356e-05,
      "loss": 0.5243,
      "step": 213000
    },
    {
      "epoch": 3.4192825112107625,
      "grad_norm": 4.369421005249023,
      "learning_rate": 2.1505979073243647e-05,
      "loss": 0.5211,
      "step": 213500
    },
    {
      "epoch": 3.427290198590647,
      "grad_norm": 4.6067962646484375,
      "learning_rate": 2.143924834507794e-05,
      "loss": 0.5287,
      "step": 214000
    },
    {
      "epoch": 3.435297885970532,
      "grad_norm": 9.103211402893066,
      "learning_rate": 2.1372517616912234e-05,
      "loss": 0.4971,
      "step": 214500
    },
    {
      "epoch": 3.4433055733504165,
      "grad_norm": 5.65288782119751,
      "learning_rate": 2.1305786888746532e-05,
      "loss": 0.5173,
      "step": 215000
    },
    {
      "epoch": 3.451313260730301,
      "grad_norm": 3.130932569503784,
      "learning_rate": 2.1239056160580826e-05,
      "loss": 0.5258,
      "step": 215500
    },
    {
      "epoch": 3.459320948110186,
      "grad_norm": 6.442335605621338,
      "learning_rate": 2.117232543241512e-05,
      "loss": 0.5342,
      "step": 216000
    },
    {
      "epoch": 3.4673286354900705,
      "grad_norm": 6.625487804412842,
      "learning_rate": 2.1105594704249417e-05,
      "loss": 0.5194,
      "step": 216500
    },
    {
      "epoch": 3.475336322869955,
      "grad_norm": 3.204094171524048,
      "learning_rate": 2.1038863976083707e-05,
      "loss": 0.542,
      "step": 217000
    },
    {
      "epoch": 3.48334401024984,
      "grad_norm": 4.633195400238037,
      "learning_rate": 2.0972133247918e-05,
      "loss": 0.5281,
      "step": 217500
    },
    {
      "epoch": 3.4913516976297245,
      "grad_norm": 12.156996726989746,
      "learning_rate": 2.0905402519752295e-05,
      "loss": 0.5345,
      "step": 218000
    },
    {
      "epoch": 3.499359385009609,
      "grad_norm": 13.965764999389648,
      "learning_rate": 2.0838671791586592e-05,
      "loss": 0.5416,
      "step": 218500
    },
    {
      "epoch": 3.507367072389494,
      "grad_norm": 4.767992973327637,
      "learning_rate": 2.0771941063420886e-05,
      "loss": 0.5072,
      "step": 219000
    },
    {
      "epoch": 3.5153747597693785,
      "grad_norm": 7.496109962463379,
      "learning_rate": 2.070521033525518e-05,
      "loss": 0.5229,
      "step": 219500
    },
    {
      "epoch": 3.523382447149263,
      "grad_norm": 3.921424388885498,
      "learning_rate": 2.0638479607089474e-05,
      "loss": 0.5199,
      "step": 220000
    },
    {
      "epoch": 3.531390134529148,
      "grad_norm": 3.760723829269409,
      "learning_rate": 2.0571748878923768e-05,
      "loss": 0.5144,
      "step": 220500
    },
    {
      "epoch": 3.5393978219090325,
      "grad_norm": 6.502561569213867,
      "learning_rate": 2.050501815075806e-05,
      "loss": 0.5143,
      "step": 221000
    },
    {
      "epoch": 3.547405509288917,
      "grad_norm": 3.0126633644104004,
      "learning_rate": 2.0438287422592356e-05,
      "loss": 0.5317,
      "step": 221500
    },
    {
      "epoch": 3.555413196668802,
      "grad_norm": 5.682426929473877,
      "learning_rate": 2.037155669442665e-05,
      "loss": 0.5149,
      "step": 222000
    },
    {
      "epoch": 3.5634208840486865,
      "grad_norm": 3.9135797023773193,
      "learning_rate": 2.0304825966260947e-05,
      "loss": 0.5168,
      "step": 222500
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 1.7475917339324951,
      "learning_rate": 2.023809523809524e-05,
      "loss": 0.5209,
      "step": 223000
    },
    {
      "epoch": 3.5794362588084563,
      "grad_norm": 2.3430371284484863,
      "learning_rate": 2.0171364509929534e-05,
      "loss": 0.5237,
      "step": 223500
    },
    {
      "epoch": 3.587443946188341,
      "grad_norm": 3.2557568550109863,
      "learning_rate": 2.0104633781763825e-05,
      "loss": 0.5139,
      "step": 224000
    },
    {
      "epoch": 3.5954516335682256,
      "grad_norm": 3.662773370742798,
      "learning_rate": 2.0037903053598122e-05,
      "loss": 0.5045,
      "step": 224500
    },
    {
      "epoch": 3.6034593209481103,
      "grad_norm": 1.573135256767273,
      "learning_rate": 1.9971172325432416e-05,
      "loss": 0.5054,
      "step": 225000
    },
    {
      "epoch": 3.611467008327995,
      "grad_norm": 6.390472888946533,
      "learning_rate": 1.990444159726671e-05,
      "loss": 0.5076,
      "step": 225500
    },
    {
      "epoch": 3.6194746957078796,
      "grad_norm": 4.968250274658203,
      "learning_rate": 1.9837710869101004e-05,
      "loss": 0.5206,
      "step": 226000
    },
    {
      "epoch": 3.6274823830877643,
      "grad_norm": 6.153519153594971,
      "learning_rate": 1.97709801409353e-05,
      "loss": 0.5154,
      "step": 226500
    },
    {
      "epoch": 3.635490070467649,
      "grad_norm": 2.077425003051758,
      "learning_rate": 1.970424941276959e-05,
      "loss": 0.5295,
      "step": 227000
    },
    {
      "epoch": 3.6434977578475336,
      "grad_norm": 15.79783821105957,
      "learning_rate": 1.9637518684603885e-05,
      "loss": 0.5065,
      "step": 227500
    },
    {
      "epoch": 3.6515054452274183,
      "grad_norm": 2.5502419471740723,
      "learning_rate": 1.9570787956438183e-05,
      "loss": 0.514,
      "step": 228000
    },
    {
      "epoch": 3.659513132607303,
      "grad_norm": 4.73549222946167,
      "learning_rate": 1.9504057228272477e-05,
      "loss": 0.5015,
      "step": 228500
    },
    {
      "epoch": 3.6675208199871876,
      "grad_norm": 4.1045122146606445,
      "learning_rate": 1.943732650010677e-05,
      "loss": 0.5226,
      "step": 229000
    },
    {
      "epoch": 3.6755285073670723,
      "grad_norm": 4.976570129394531,
      "learning_rate": 1.9370595771941064e-05,
      "loss": 0.507,
      "step": 229500
    },
    {
      "epoch": 3.683536194746957,
      "grad_norm": 2.324946403503418,
      "learning_rate": 1.930386504377536e-05,
      "loss": 0.5313,
      "step": 230000
    },
    {
      "epoch": 3.691543882126842,
      "grad_norm": 2.331120252609253,
      "learning_rate": 1.9237134315609652e-05,
      "loss": 0.505,
      "step": 230500
    },
    {
      "epoch": 3.6995515695067267,
      "grad_norm": 7.613391399383545,
      "learning_rate": 1.9170403587443946e-05,
      "loss": 0.5106,
      "step": 231000
    },
    {
      "epoch": 3.7075592568866114,
      "grad_norm": 7.033883094787598,
      "learning_rate": 1.910367285927824e-05,
      "loss": 0.5171,
      "step": 231500
    },
    {
      "epoch": 3.715566944266496,
      "grad_norm": 5.399119853973389,
      "learning_rate": 1.9036942131112537e-05,
      "loss": 0.561,
      "step": 232000
    },
    {
      "epoch": 3.7235746316463807,
      "grad_norm": 6.534111022949219,
      "learning_rate": 1.897021140294683e-05,
      "loss": 0.5443,
      "step": 232500
    },
    {
      "epoch": 3.7315823190262654,
      "grad_norm": 4.875268459320068,
      "learning_rate": 1.8903480674781125e-05,
      "loss": 0.5745,
      "step": 233000
    },
    {
      "epoch": 3.73959000640615,
      "grad_norm": 2.143779993057251,
      "learning_rate": 1.883674994661542e-05,
      "loss": 0.5668,
      "step": 233500
    },
    {
      "epoch": 3.7475976937860347,
      "grad_norm": 5.978457450866699,
      "learning_rate": 1.8770019218449713e-05,
      "loss": 0.5561,
      "step": 234000
    },
    {
      "epoch": 3.7556053811659194,
      "grad_norm": 34.79189682006836,
      "learning_rate": 1.8703288490284006e-05,
      "loss": 0.6064,
      "step": 234500
    },
    {
      "epoch": 3.763613068545804,
      "grad_norm": 1.356866478919983,
      "learning_rate": 1.86365577621183e-05,
      "loss": 0.5811,
      "step": 235000
    },
    {
      "epoch": 3.7716207559256887,
      "grad_norm": 144.53857421875,
      "learning_rate": 1.8569827033952594e-05,
      "loss": 0.5538,
      "step": 235500
    },
    {
      "epoch": 3.7796284433055733,
      "grad_norm": 7.587902545928955,
      "learning_rate": 1.850309630578689e-05,
      "loss": 0.5843,
      "step": 236000
    },
    {
      "epoch": 3.787636130685458,
      "grad_norm": 9.950390815734863,
      "learning_rate": 1.8436365577621185e-05,
      "loss": 0.5302,
      "step": 236500
    },
    {
      "epoch": 3.7956438180653427,
      "grad_norm": 3.351644515991211,
      "learning_rate": 1.836963484945548e-05,
      "loss": 0.5496,
      "step": 237000
    },
    {
      "epoch": 3.8036515054452273,
      "grad_norm": 6.246541500091553,
      "learning_rate": 1.830290412128977e-05,
      "loss": 0.5585,
      "step": 237500
    },
    {
      "epoch": 3.811659192825112,
      "grad_norm": 3.85917329788208,
      "learning_rate": 1.8236173393124067e-05,
      "loss": 0.5273,
      "step": 238000
    },
    {
      "epoch": 3.8196668802049967,
      "grad_norm": 8.33314323425293,
      "learning_rate": 1.816944266495836e-05,
      "loss": 0.5375,
      "step": 238500
    },
    {
      "epoch": 3.8276745675848813,
      "grad_norm": 5.771579742431641,
      "learning_rate": 1.8102711936792655e-05,
      "loss": 0.5332,
      "step": 239000
    },
    {
      "epoch": 3.835682254964766,
      "grad_norm": 3.8213882446289062,
      "learning_rate": 1.803598120862695e-05,
      "loss": 0.5309,
      "step": 239500
    },
    {
      "epoch": 3.8436899423446507,
      "grad_norm": 5.864470958709717,
      "learning_rate": 1.7969250480461246e-05,
      "loss": 0.5356,
      "step": 240000
    },
    {
      "epoch": 3.8516976297245353,
      "grad_norm": 4.482711315155029,
      "learning_rate": 1.7902519752295536e-05,
      "loss": 0.5407,
      "step": 240500
    },
    {
      "epoch": 3.85970531710442,
      "grad_norm": 6.829257488250732,
      "learning_rate": 1.783578902412983e-05,
      "loss": 0.52,
      "step": 241000
    },
    {
      "epoch": 3.8677130044843047,
      "grad_norm": 6.028481483459473,
      "learning_rate": 1.7769058295964127e-05,
      "loss": 0.5301,
      "step": 241500
    },
    {
      "epoch": 3.8757206918641898,
      "grad_norm": 5.929718494415283,
      "learning_rate": 1.770232756779842e-05,
      "loss": 0.5516,
      "step": 242000
    },
    {
      "epoch": 3.8837283792440744,
      "grad_norm": 7.70997428894043,
      "learning_rate": 1.7635596839632715e-05,
      "loss": 0.5652,
      "step": 242500
    },
    {
      "epoch": 3.891736066623959,
      "grad_norm": 5.585678577423096,
      "learning_rate": 1.756886611146701e-05,
      "loss": 0.5486,
      "step": 243000
    },
    {
      "epoch": 3.8997437540038438,
      "grad_norm": 3.879845142364502,
      "learning_rate": 1.7502135383301303e-05,
      "loss": 0.5159,
      "step": 243500
    },
    {
      "epoch": 3.9077514413837284,
      "grad_norm": 2.580503463745117,
      "learning_rate": 1.7435404655135597e-05,
      "loss": 0.5339,
      "step": 244000
    },
    {
      "epoch": 3.915759128763613,
      "grad_norm": 2.3360297679901123,
      "learning_rate": 1.736867392696989e-05,
      "loss": 0.5402,
      "step": 244500
    },
    {
      "epoch": 3.9237668161434978,
      "grad_norm": 24.927274703979492,
      "learning_rate": 1.7301943198804185e-05,
      "loss": 0.5554,
      "step": 245000
    },
    {
      "epoch": 3.9317745035233824,
      "grad_norm": 8.378798484802246,
      "learning_rate": 1.7235212470638482e-05,
      "loss": 0.5453,
      "step": 245500
    },
    {
      "epoch": 3.939782190903267,
      "grad_norm": 8.902276992797852,
      "learning_rate": 1.7168481742472776e-05,
      "loss": 0.5816,
      "step": 246000
    },
    {
      "epoch": 3.9477898782831518,
      "grad_norm": 3.1411001682281494,
      "learning_rate": 1.710175101430707e-05,
      "loss": 0.5308,
      "step": 246500
    },
    {
      "epoch": 3.9557975656630364,
      "grad_norm": 3.4038777351379395,
      "learning_rate": 1.7035020286141364e-05,
      "loss": 0.5373,
      "step": 247000
    },
    {
      "epoch": 3.963805253042921,
      "grad_norm": 2.4442849159240723,
      "learning_rate": 1.6968289557975657e-05,
      "loss": 0.564,
      "step": 247500
    },
    {
      "epoch": 3.9718129404228057,
      "grad_norm": 4.104819297790527,
      "learning_rate": 1.690155882980995e-05,
      "loss": 0.5598,
      "step": 248000
    },
    {
      "epoch": 3.979820627802691,
      "grad_norm": 2.071009874343872,
      "learning_rate": 1.6834828101644245e-05,
      "loss": 0.5699,
      "step": 248500
    },
    {
      "epoch": 3.9878283151825755,
      "grad_norm": 6.5647149085998535,
      "learning_rate": 1.676809737347854e-05,
      "loss": 0.5705,
      "step": 249000
    },
    {
      "epoch": 3.99583600256246,
      "grad_norm": 2.4963955879211426,
      "learning_rate": 1.6701366645312836e-05,
      "loss": 0.5634,
      "step": 249500
    },
    {
      "epoch": 4.003843689942345,
      "grad_norm": 150.49623107910156,
      "learning_rate": 1.663463591714713e-05,
      "loss": 0.5454,
      "step": 250000
    },
    {
      "epoch": 4.0118513773222295,
      "grad_norm": 7.540205955505371,
      "learning_rate": 1.6567905188981424e-05,
      "loss": 0.5346,
      "step": 250500
    },
    {
      "epoch": 4.019859064702114,
      "grad_norm": 10.86510181427002,
      "learning_rate": 1.6501174460815715e-05,
      "loss": 0.5441,
      "step": 251000
    },
    {
      "epoch": 4.027866752081999,
      "grad_norm": 182.1198272705078,
      "learning_rate": 1.6434443732650012e-05,
      "loss": 0.524,
      "step": 251500
    },
    {
      "epoch": 4.0358744394618835,
      "grad_norm": 2.2060883045196533,
      "learning_rate": 1.6367713004484306e-05,
      "loss": 0.5263,
      "step": 252000
    },
    {
      "epoch": 4.043882126841768,
      "grad_norm": 9.623943328857422,
      "learning_rate": 1.63009822763186e-05,
      "loss": 0.5536,
      "step": 252500
    },
    {
      "epoch": 4.051889814221653,
      "grad_norm": 4.34499979019165,
      "learning_rate": 1.6234251548152893e-05,
      "loss": 0.5182,
      "step": 253000
    },
    {
      "epoch": 4.0598975016015375,
      "grad_norm": 5.492642879486084,
      "learning_rate": 1.616752081998719e-05,
      "loss": 0.5241,
      "step": 253500
    },
    {
      "epoch": 4.067905188981422,
      "grad_norm": 3.2466022968292236,
      "learning_rate": 1.610079009182148e-05,
      "loss": 0.5434,
      "step": 254000
    },
    {
      "epoch": 4.075912876361307,
      "grad_norm": 3.040654420852661,
      "learning_rate": 1.6034059363655775e-05,
      "loss": 0.5893,
      "step": 254500
    },
    {
      "epoch": 4.0839205637411915,
      "grad_norm": 2.047579288482666,
      "learning_rate": 1.5967328635490072e-05,
      "loss": 0.5916,
      "step": 255000
    },
    {
      "epoch": 4.091928251121076,
      "grad_norm": 6.849547386169434,
      "learning_rate": 1.5900597907324366e-05,
      "loss": 0.5689,
      "step": 255500
    },
    {
      "epoch": 4.099935938500961,
      "grad_norm": 3.4308221340179443,
      "learning_rate": 1.583386717915866e-05,
      "loss": 0.5307,
      "step": 256000
    },
    {
      "epoch": 4.1079436258808455,
      "grad_norm": 2.9829118251800537,
      "learning_rate": 1.5767136450992954e-05,
      "loss": 0.5431,
      "step": 256500
    },
    {
      "epoch": 4.11595131326073,
      "grad_norm": 6.21203088760376,
      "learning_rate": 1.5700405722827248e-05,
      "loss": 0.5076,
      "step": 257000
    },
    {
      "epoch": 4.123959000640615,
      "grad_norm": 6.3341803550720215,
      "learning_rate": 1.563367499466154e-05,
      "loss": 0.5167,
      "step": 257500
    },
    {
      "epoch": 4.1319666880204995,
      "grad_norm": 6.211516857147217,
      "learning_rate": 1.5566944266495836e-05,
      "loss": 0.5062,
      "step": 258000
    },
    {
      "epoch": 4.139974375400384,
      "grad_norm": 3.269399881362915,
      "learning_rate": 1.550021353833013e-05,
      "loss": 0.5045,
      "step": 258500
    },
    {
      "epoch": 4.147982062780269,
      "grad_norm": 4.230234146118164,
      "learning_rate": 1.5433482810164427e-05,
      "loss": 0.5039,
      "step": 259000
    },
    {
      "epoch": 4.1559897501601535,
      "grad_norm": 51.506507873535156,
      "learning_rate": 1.536675208199872e-05,
      "loss": 0.5041,
      "step": 259500
    },
    {
      "epoch": 4.163997437540038,
      "grad_norm": 5.417057991027832,
      "learning_rate": 1.5300021353833014e-05,
      "loss": 0.5228,
      "step": 260000
    },
    {
      "epoch": 4.172005124919923,
      "grad_norm": 4.610279560089111,
      "learning_rate": 1.5233290625667307e-05,
      "loss": 0.5001,
      "step": 260500
    },
    {
      "epoch": 4.1800128122998075,
      "grad_norm": 4.606158256530762,
      "learning_rate": 1.5166559897501604e-05,
      "loss": 0.5168,
      "step": 261000
    },
    {
      "epoch": 4.188020499679692,
      "grad_norm": 4.496291160583496,
      "learning_rate": 1.5099829169335896e-05,
      "loss": 0.499,
      "step": 261500
    },
    {
      "epoch": 4.196028187059577,
      "grad_norm": 6.866757869720459,
      "learning_rate": 1.503309844117019e-05,
      "loss": 0.5048,
      "step": 262000
    },
    {
      "epoch": 4.2040358744394615,
      "grad_norm": 6.067983627319336,
      "learning_rate": 1.4966367713004484e-05,
      "loss": 0.5123,
      "step": 262500
    },
    {
      "epoch": 4.212043561819346,
      "grad_norm": 8.032356262207031,
      "learning_rate": 1.489963698483878e-05,
      "loss": 0.5052,
      "step": 263000
    },
    {
      "epoch": 4.220051249199232,
      "grad_norm": 6.138614177703857,
      "learning_rate": 1.4832906256673073e-05,
      "loss": 0.4918,
      "step": 263500
    },
    {
      "epoch": 4.228058936579116,
      "grad_norm": 4.1537957191467285,
      "learning_rate": 1.4766175528507367e-05,
      "loss": 0.509,
      "step": 264000
    },
    {
      "epoch": 4.236066623959001,
      "grad_norm": 4.48830509185791,
      "learning_rate": 1.4699444800341661e-05,
      "loss": 0.5048,
      "step": 264500
    },
    {
      "epoch": 4.244074311338886,
      "grad_norm": 2.930828809738159,
      "learning_rate": 1.4632714072175957e-05,
      "loss": 0.5077,
      "step": 265000
    },
    {
      "epoch": 4.25208199871877,
      "grad_norm": 1.3002238273620605,
      "learning_rate": 1.456598334401025e-05,
      "loss": 0.5012,
      "step": 265500
    },
    {
      "epoch": 4.260089686098655,
      "grad_norm": 15.42573070526123,
      "learning_rate": 1.4499252615844544e-05,
      "loss": 0.5163,
      "step": 266000
    },
    {
      "epoch": 4.26809737347854,
      "grad_norm": 4.384527206420898,
      "learning_rate": 1.443252188767884e-05,
      "loss": 0.5119,
      "step": 266500
    },
    {
      "epoch": 4.276105060858424,
      "grad_norm": 43.1288948059082,
      "learning_rate": 1.4365791159513134e-05,
      "loss": 0.5252,
      "step": 267000
    },
    {
      "epoch": 4.284112748238309,
      "grad_norm": 6.192732810974121,
      "learning_rate": 1.4299060431347428e-05,
      "loss": 0.5008,
      "step": 267500
    },
    {
      "epoch": 4.292120435618194,
      "grad_norm": 3.314314603805542,
      "learning_rate": 1.4232329703181722e-05,
      "loss": 0.5118,
      "step": 268000
    },
    {
      "epoch": 4.300128122998078,
      "grad_norm": 4.360948085784912,
      "learning_rate": 1.4165598975016017e-05,
      "loss": 0.5008,
      "step": 268500
    },
    {
      "epoch": 4.308135810377963,
      "grad_norm": 3.090627908706665,
      "learning_rate": 1.4098868246850311e-05,
      "loss": 0.5119,
      "step": 269000
    },
    {
      "epoch": 4.316143497757848,
      "grad_norm": 4.3881683349609375,
      "learning_rate": 1.4032137518684605e-05,
      "loss": 0.5238,
      "step": 269500
    },
    {
      "epoch": 4.324151185137732,
      "grad_norm": 6.906801700592041,
      "learning_rate": 1.3965406790518897e-05,
      "loss": 0.5201,
      "step": 270000
    },
    {
      "epoch": 4.332158872517617,
      "grad_norm": 19.744882583618164,
      "learning_rate": 1.3898676062353194e-05,
      "loss": 0.4947,
      "step": 270500
    },
    {
      "epoch": 4.340166559897502,
      "grad_norm": 10.030303001403809,
      "learning_rate": 1.3831945334187488e-05,
      "loss": 0.5004,
      "step": 271000
    },
    {
      "epoch": 4.348174247277386,
      "grad_norm": 4.8604254722595215,
      "learning_rate": 1.3765214606021782e-05,
      "loss": 0.5005,
      "step": 271500
    },
    {
      "epoch": 4.356181934657271,
      "grad_norm": 3.4427003860473633,
      "learning_rate": 1.3698483877856074e-05,
      "loss": 0.5288,
      "step": 272000
    },
    {
      "epoch": 4.364189622037156,
      "grad_norm": 3.0980567932128906,
      "learning_rate": 1.3631753149690372e-05,
      "loss": 0.5289,
      "step": 272500
    },
    {
      "epoch": 4.37219730941704,
      "grad_norm": 6.897751331329346,
      "learning_rate": 1.3565022421524665e-05,
      "loss": 0.5225,
      "step": 273000
    },
    {
      "epoch": 4.380204996796925,
      "grad_norm": 7.022542476654053,
      "learning_rate": 1.3498291693358958e-05,
      "loss": 0.5182,
      "step": 273500
    },
    {
      "epoch": 4.38821268417681,
      "grad_norm": 4.048801898956299,
      "learning_rate": 1.3431560965193251e-05,
      "loss": 0.501,
      "step": 274000
    },
    {
      "epoch": 4.396220371556694,
      "grad_norm": 7.635848522186279,
      "learning_rate": 1.3364830237027549e-05,
      "loss": 0.5309,
      "step": 274500
    },
    {
      "epoch": 4.404228058936579,
      "grad_norm": 5.4705424308776855,
      "learning_rate": 1.3298099508861841e-05,
      "loss": 0.5232,
      "step": 275000
    },
    {
      "epoch": 4.412235746316464,
      "grad_norm": 5.873051643371582,
      "learning_rate": 1.3231368780696135e-05,
      "loss": 0.5298,
      "step": 275500
    },
    {
      "epoch": 4.420243433696348,
      "grad_norm": 14.253281593322754,
      "learning_rate": 1.3164638052530429e-05,
      "loss": 0.5122,
      "step": 276000
    },
    {
      "epoch": 4.428251121076233,
      "grad_norm": 10.148263931274414,
      "learning_rate": 1.3097907324364724e-05,
      "loss": 0.5012,
      "step": 276500
    },
    {
      "epoch": 4.436258808456118,
      "grad_norm": 10.017847061157227,
      "learning_rate": 1.3031176596199018e-05,
      "loss": 0.4908,
      "step": 277000
    },
    {
      "epoch": 4.444266495836002,
      "grad_norm": 4.911150932312012,
      "learning_rate": 1.2964445868033312e-05,
      "loss": 0.5063,
      "step": 277500
    },
    {
      "epoch": 4.452274183215887,
      "grad_norm": 3.4809353351593018,
      "learning_rate": 1.2897715139867606e-05,
      "loss": 0.5049,
      "step": 278000
    },
    {
      "epoch": 4.460281870595772,
      "grad_norm": 1.6174324750900269,
      "learning_rate": 1.2830984411701901e-05,
      "loss": 0.5078,
      "step": 278500
    },
    {
      "epoch": 4.468289557975656,
      "grad_norm": 6.694969654083252,
      "learning_rate": 1.2764253683536195e-05,
      "loss": 0.5172,
      "step": 279000
    },
    {
      "epoch": 4.476297245355541,
      "grad_norm": 5.052237510681152,
      "learning_rate": 1.269752295537049e-05,
      "loss": 0.517,
      "step": 279500
    },
    {
      "epoch": 4.484304932735426,
      "grad_norm": 7.702732563018799,
      "learning_rate": 1.2630792227204785e-05,
      "loss": 0.5022,
      "step": 280000
    },
    {
      "epoch": 4.49231262011531,
      "grad_norm": 1.7527016401290894,
      "learning_rate": 1.2564061499039079e-05,
      "loss": 0.521,
      "step": 280500
    },
    {
      "epoch": 4.500320307495196,
      "grad_norm": 6.1656880378723145,
      "learning_rate": 1.2497330770873372e-05,
      "loss": 0.5103,
      "step": 281000
    },
    {
      "epoch": 4.50832799487508,
      "grad_norm": 7.274654865264893,
      "learning_rate": 1.2430600042707666e-05,
      "loss": 0.5069,
      "step": 281500
    },
    {
      "epoch": 4.516335682254965,
      "grad_norm": 6.096523761749268,
      "learning_rate": 1.236386931454196e-05,
      "loss": 0.5151,
      "step": 282000
    },
    {
      "epoch": 4.52434336963485,
      "grad_norm": 2.2012948989868164,
      "learning_rate": 1.2297138586376256e-05,
      "loss": 0.5008,
      "step": 282500
    },
    {
      "epoch": 4.5323510570147345,
      "grad_norm": 3.280164957046509,
      "learning_rate": 1.223040785821055e-05,
      "loss": 0.5022,
      "step": 283000
    },
    {
      "epoch": 4.540358744394619,
      "grad_norm": 20.409669876098633,
      "learning_rate": 1.2163677130044844e-05,
      "loss": 0.5087,
      "step": 283500
    },
    {
      "epoch": 4.548366431774504,
      "grad_norm": 3.3011295795440674,
      "learning_rate": 1.2096946401879137e-05,
      "loss": 0.5076,
      "step": 284000
    },
    {
      "epoch": 4.5563741191543885,
      "grad_norm": 4.40270471572876,
      "learning_rate": 1.2030215673713433e-05,
      "loss": 0.4972,
      "step": 284500
    },
    {
      "epoch": 4.564381806534273,
      "grad_norm": 2.9016330242156982,
      "learning_rate": 1.1963484945547725e-05,
      "loss": 0.5287,
      "step": 285000
    },
    {
      "epoch": 4.572389493914158,
      "grad_norm": 4.172075271606445,
      "learning_rate": 1.189675421738202e-05,
      "loss": 0.4951,
      "step": 285500
    },
    {
      "epoch": 4.5803971812940425,
      "grad_norm": 6.362120628356934,
      "learning_rate": 1.1830023489216315e-05,
      "loss": 0.491,
      "step": 286000
    },
    {
      "epoch": 4.588404868673927,
      "grad_norm": 7.832621097564697,
      "learning_rate": 1.176329276105061e-05,
      "loss": 0.5017,
      "step": 286500
    },
    {
      "epoch": 4.596412556053812,
      "grad_norm": 3.891714096069336,
      "learning_rate": 1.1696562032884902e-05,
      "loss": 0.5129,
      "step": 287000
    },
    {
      "epoch": 4.6044202434336965,
      "grad_norm": 3.234403610229492,
      "learning_rate": 1.1629831304719198e-05,
      "loss": 0.5164,
      "step": 287500
    },
    {
      "epoch": 4.612427930813581,
      "grad_norm": 7.876539707183838,
      "learning_rate": 1.1563100576553492e-05,
      "loss": 0.4971,
      "step": 288000
    },
    {
      "epoch": 4.620435618193466,
      "grad_norm": 11.54860782623291,
      "learning_rate": 1.1496369848387786e-05,
      "loss": 0.5194,
      "step": 288500
    },
    {
      "epoch": 4.6284433055733505,
      "grad_norm": 4.4252448081970215,
      "learning_rate": 1.142963912022208e-05,
      "loss": 0.4944,
      "step": 289000
    },
    {
      "epoch": 4.636450992953235,
      "grad_norm": 6.529786586761475,
      "learning_rate": 1.1362908392056375e-05,
      "loss": 0.5108,
      "step": 289500
    },
    {
      "epoch": 4.64445868033312,
      "grad_norm": 5.203530311584473,
      "learning_rate": 1.1296177663890669e-05,
      "loss": 0.5009,
      "step": 290000
    },
    {
      "epoch": 4.6524663677130045,
      "grad_norm": 9.2627534866333,
      "learning_rate": 1.1229446935724963e-05,
      "loss": 0.508,
      "step": 290500
    },
    {
      "epoch": 4.660474055092889,
      "grad_norm": 4.854918003082275,
      "learning_rate": 1.1162716207559257e-05,
      "loss": 0.4899,
      "step": 291000
    },
    {
      "epoch": 4.668481742472774,
      "grad_norm": 3.552065134048462,
      "learning_rate": 1.1095985479393552e-05,
      "loss": 0.5064,
      "step": 291500
    },
    {
      "epoch": 4.6764894298526585,
      "grad_norm": 2.438626527786255,
      "learning_rate": 1.1029254751227846e-05,
      "loss": 0.5072,
      "step": 292000
    },
    {
      "epoch": 4.684497117232543,
      "grad_norm": 7.479953765869141,
      "learning_rate": 1.096252402306214e-05,
      "loss": 0.52,
      "step": 292500
    },
    {
      "epoch": 4.692504804612428,
      "grad_norm": 7.670618534088135,
      "learning_rate": 1.0895793294896436e-05,
      "loss": 0.5084,
      "step": 293000
    },
    {
      "epoch": 4.7005124919923125,
      "grad_norm": 145.91036987304688,
      "learning_rate": 1.0829062566730728e-05,
      "loss": 0.5138,
      "step": 293500
    },
    {
      "epoch": 4.708520179372197,
      "grad_norm": 3.721892833709717,
      "learning_rate": 1.0762331838565023e-05,
      "loss": 0.506,
      "step": 294000
    },
    {
      "epoch": 4.716527866752082,
      "grad_norm": 12.177992820739746,
      "learning_rate": 1.0695601110399317e-05,
      "loss": 0.5187,
      "step": 294500
    },
    {
      "epoch": 4.7245355541319665,
      "grad_norm": 3.512115001678467,
      "learning_rate": 1.0628870382233611e-05,
      "loss": 0.4954,
      "step": 295000
    },
    {
      "epoch": 4.732543241511851,
      "grad_norm": 4.103353977203369,
      "learning_rate": 1.0562139654067905e-05,
      "loss": 0.5208,
      "step": 295500
    },
    {
      "epoch": 4.740550928891736,
      "grad_norm": 3.5942816734313965,
      "learning_rate": 1.04954089259022e-05,
      "loss": 0.5162,
      "step": 296000
    },
    {
      "epoch": 4.7485586162716205,
      "grad_norm": 11.2730131149292,
      "learning_rate": 1.0428678197736495e-05,
      "loss": 0.5347,
      "step": 296500
    },
    {
      "epoch": 4.756566303651505,
      "grad_norm": 3.0580708980560303,
      "learning_rate": 1.0361947469570788e-05,
      "loss": 0.5246,
      "step": 297000
    },
    {
      "epoch": 4.76457399103139,
      "grad_norm": 4.894040107727051,
      "learning_rate": 1.0295216741405082e-05,
      "loss": 0.5356,
      "step": 297500
    },
    {
      "epoch": 4.7725816784112745,
      "grad_norm": 4.470303058624268,
      "learning_rate": 1.0228486013239378e-05,
      "loss": 0.4998,
      "step": 298000
    },
    {
      "epoch": 4.78058936579116,
      "grad_norm": 4.158844470977783,
      "learning_rate": 1.016175528507367e-05,
      "loss": 0.4937,
      "step": 298500
    },
    {
      "epoch": 4.788597053171044,
      "grad_norm": 2.6250667572021484,
      "learning_rate": 1.0095024556907966e-05,
      "loss": 0.5246,
      "step": 299000
    },
    {
      "epoch": 4.796604740550929,
      "grad_norm": 14.928330421447754,
      "learning_rate": 1.002829382874226e-05,
      "loss": 0.5085,
      "step": 299500
    },
    {
      "epoch": 4.804612427930813,
      "grad_norm": 8.037238121032715,
      "learning_rate": 9.961563100576553e-06,
      "loss": 0.5,
      "step": 300000
    },
    {
      "epoch": 4.812620115310699,
      "grad_norm": 3.0412943363189697,
      "learning_rate": 9.894832372410847e-06,
      "loss": 0.5019,
      "step": 300500
    },
    {
      "epoch": 4.820627802690583,
      "grad_norm": 1.9552834033966064,
      "learning_rate": 9.828101644245143e-06,
      "loss": 0.5136,
      "step": 301000
    },
    {
      "epoch": 4.828635490070468,
      "grad_norm": 3.6778342723846436,
      "learning_rate": 9.761370916079437e-06,
      "loss": 0.4959,
      "step": 301500
    },
    {
      "epoch": 4.836643177450353,
      "grad_norm": 8.766436576843262,
      "learning_rate": 9.69464018791373e-06,
      "loss": 0.5069,
      "step": 302000
    },
    {
      "epoch": 4.844650864830237,
      "grad_norm": 5.2205891609191895,
      "learning_rate": 9.627909459748024e-06,
      "loss": 0.4909,
      "step": 302500
    },
    {
      "epoch": 4.852658552210122,
      "grad_norm": 2.4997761249542236,
      "learning_rate": 9.56117873158232e-06,
      "loss": 0.5134,
      "step": 303000
    },
    {
      "epoch": 4.860666239590007,
      "grad_norm": 4.627324104309082,
      "learning_rate": 9.494448003416614e-06,
      "loss": 0.4884,
      "step": 303500
    },
    {
      "epoch": 4.868673926969891,
      "grad_norm": 4.733732223510742,
      "learning_rate": 9.427717275250908e-06,
      "loss": 0.4853,
      "step": 304000
    },
    {
      "epoch": 4.876681614349776,
      "grad_norm": 6.7082390785217285,
      "learning_rate": 9.360986547085203e-06,
      "loss": 0.4945,
      "step": 304500
    },
    {
      "epoch": 4.884689301729661,
      "grad_norm": 4.812807559967041,
      "learning_rate": 9.294255818919497e-06,
      "loss": 0.4858,
      "step": 305000
    },
    {
      "epoch": 4.892696989109545,
      "grad_norm": 6.763813018798828,
      "learning_rate": 9.227525090753791e-06,
      "loss": 0.5036,
      "step": 305500
    },
    {
      "epoch": 4.90070467648943,
      "grad_norm": 3.2796010971069336,
      "learning_rate": 9.160794362588085e-06,
      "loss": 0.4857,
      "step": 306000
    },
    {
      "epoch": 4.908712363869315,
      "grad_norm": 6.4682135581970215,
      "learning_rate": 9.09406363442238e-06,
      "loss": 0.5022,
      "step": 306500
    },
    {
      "epoch": 4.916720051249199,
      "grad_norm": 4.228434085845947,
      "learning_rate": 9.027332906256673e-06,
      "loss": 0.4854,
      "step": 307000
    },
    {
      "epoch": 4.924727738629084,
      "grad_norm": 4.0644378662109375,
      "learning_rate": 8.960602178090968e-06,
      "loss": 0.5171,
      "step": 307500
    },
    {
      "epoch": 4.932735426008969,
      "grad_norm": 3.108691930770874,
      "learning_rate": 8.893871449925262e-06,
      "loss": 0.4814,
      "step": 308000
    },
    {
      "epoch": 4.940743113388853,
      "grad_norm": 3.828885555267334,
      "learning_rate": 8.827140721759556e-06,
      "loss": 0.4868,
      "step": 308500
    },
    {
      "epoch": 4.948750800768738,
      "grad_norm": 3.627183675765991,
      "learning_rate": 8.76040999359385e-06,
      "loss": 0.5101,
      "step": 309000
    },
    {
      "epoch": 4.956758488148623,
      "grad_norm": 7.307015419006348,
      "learning_rate": 8.693679265428145e-06,
      "loss": 0.4837,
      "step": 309500
    },
    {
      "epoch": 4.964766175528507,
      "grad_norm": 3.346383571624756,
      "learning_rate": 8.62694853726244e-06,
      "loss": 0.5127,
      "step": 310000
    },
    {
      "epoch": 4.972773862908392,
      "grad_norm": 2.1650428771972656,
      "learning_rate": 8.560217809096733e-06,
      "loss": 0.4977,
      "step": 310500
    },
    {
      "epoch": 4.980781550288277,
      "grad_norm": 2.1133127212524414,
      "learning_rate": 8.493487080931027e-06,
      "loss": 0.5016,
      "step": 311000
    },
    {
      "epoch": 4.988789237668161,
      "grad_norm": 2.6721668243408203,
      "learning_rate": 8.426756352765323e-06,
      "loss": 0.495,
      "step": 311500
    },
    {
      "epoch": 4.996796925048046,
      "grad_norm": 4.618086338043213,
      "learning_rate": 8.360025624599615e-06,
      "loss": 0.4964,
      "step": 312000
    },
    {
      "epoch": 5.004804612427931,
      "grad_norm": 8.765559196472168,
      "learning_rate": 8.29329489643391e-06,
      "loss": 0.4745,
      "step": 312500
    },
    {
      "epoch": 5.012812299807815,
      "grad_norm": 4.1259894371032715,
      "learning_rate": 8.226564168268204e-06,
      "loss": 0.4651,
      "step": 313000
    },
    {
      "epoch": 5.0208199871877,
      "grad_norm": 6.5845417976379395,
      "learning_rate": 8.159833440102498e-06,
      "loss": 0.4879,
      "step": 313500
    },
    {
      "epoch": 5.028827674567585,
      "grad_norm": 5.27044677734375,
      "learning_rate": 8.093102711936792e-06,
      "loss": 0.494,
      "step": 314000
    },
    {
      "epoch": 5.036835361947469,
      "grad_norm": 3.092660665512085,
      "learning_rate": 8.026371983771088e-06,
      "loss": 0.4856,
      "step": 314500
    },
    {
      "epoch": 5.044843049327354,
      "grad_norm": 3.2038395404815674,
      "learning_rate": 7.959641255605381e-06,
      "loss": 0.469,
      "step": 315000
    },
    {
      "epoch": 5.052850736707239,
      "grad_norm": 2.299567699432373,
      "learning_rate": 7.892910527439675e-06,
      "loss": 0.4842,
      "step": 315500
    },
    {
      "epoch": 5.060858424087123,
      "grad_norm": 2.303654432296753,
      "learning_rate": 7.82617979927397e-06,
      "loss": 0.4909,
      "step": 316000
    },
    {
      "epoch": 5.068866111467008,
      "grad_norm": 5.940292835235596,
      "learning_rate": 7.759449071108265e-06,
      "loss": 0.4763,
      "step": 316500
    },
    {
      "epoch": 5.076873798846893,
      "grad_norm": 9.555686950683594,
      "learning_rate": 7.692718342942559e-06,
      "loss": 0.4842,
      "step": 317000
    },
    {
      "epoch": 5.084881486226777,
      "grad_norm": 3.485595226287842,
      "learning_rate": 7.6259876147768525e-06,
      "loss": 0.476,
      "step": 317500
    },
    {
      "epoch": 5.092889173606663,
      "grad_norm": 5.489136219024658,
      "learning_rate": 7.559256886611147e-06,
      "loss": 0.476,
      "step": 318000
    },
    {
      "epoch": 5.1008968609865475,
      "grad_norm": 2.3655288219451904,
      "learning_rate": 7.492526158445441e-06,
      "loss": 0.4758,
      "step": 318500
    },
    {
      "epoch": 5.108904548366432,
      "grad_norm": 4.3599138259887695,
      "learning_rate": 7.425795430279736e-06,
      "loss": 0.4769,
      "step": 319000
    },
    {
      "epoch": 5.116912235746317,
      "grad_norm": 5.789588928222656,
      "learning_rate": 7.35906470211403e-06,
      "loss": 0.4758,
      "step": 319500
    },
    {
      "epoch": 5.1249199231262015,
      "grad_norm": 8.993159294128418,
      "learning_rate": 7.2923339739483245e-06,
      "loss": 0.4885,
      "step": 320000
    },
    {
      "epoch": 5.132927610506086,
      "grad_norm": 3.0439155101776123,
      "learning_rate": 7.225603245782618e-06,
      "loss": 0.4626,
      "step": 320500
    },
    {
      "epoch": 5.140935297885971,
      "grad_norm": 6.192640781402588,
      "learning_rate": 7.158872517616913e-06,
      "loss": 0.4939,
      "step": 321000
    },
    {
      "epoch": 5.1489429852658555,
      "grad_norm": 9.164207458496094,
      "learning_rate": 7.092141789451207e-06,
      "loss": 0.4661,
      "step": 321500
    },
    {
      "epoch": 5.15695067264574,
      "grad_norm": 1.9001259803771973,
      "learning_rate": 7.025411061285502e-06,
      "loss": 0.4913,
      "step": 322000
    },
    {
      "epoch": 5.164958360025625,
      "grad_norm": 6.814502239227295,
      "learning_rate": 6.958680333119795e-06,
      "loss": 0.4723,
      "step": 322500
    },
    {
      "epoch": 5.1729660474055095,
      "grad_norm": 6.305384635925293,
      "learning_rate": 6.89194960495409e-06,
      "loss": 0.4839,
      "step": 323000
    },
    {
      "epoch": 5.180973734785394,
      "grad_norm": 5.89647912979126,
      "learning_rate": 6.825218876788383e-06,
      "loss": 0.485,
      "step": 323500
    },
    {
      "epoch": 5.188981422165279,
      "grad_norm": 7.292586803436279,
      "learning_rate": 6.758488148622678e-06,
      "loss": 0.4818,
      "step": 324000
    },
    {
      "epoch": 5.1969891095451635,
      "grad_norm": 4.718400478363037,
      "learning_rate": 6.691757420456972e-06,
      "loss": 0.4807,
      "step": 324500
    },
    {
      "epoch": 5.204996796925048,
      "grad_norm": 8.413100242614746,
      "learning_rate": 6.625026692291267e-06,
      "loss": 0.4828,
      "step": 325000
    },
    {
      "epoch": 5.213004484304933,
      "grad_norm": 2.8939242362976074,
      "learning_rate": 6.5582959641255605e-06,
      "loss": 0.4929,
      "step": 325500
    },
    {
      "epoch": 5.2210121716848175,
      "grad_norm": 12.550703048706055,
      "learning_rate": 6.491565235959855e-06,
      "loss": 0.474,
      "step": 326000
    },
    {
      "epoch": 5.229019859064702,
      "grad_norm": 3.062390089035034,
      "learning_rate": 6.424834507794149e-06,
      "loss": 0.4923,
      "step": 326500
    },
    {
      "epoch": 5.237027546444587,
      "grad_norm": 18.374021530151367,
      "learning_rate": 6.358103779628444e-06,
      "loss": 0.4872,
      "step": 327000
    },
    {
      "epoch": 5.2450352338244715,
      "grad_norm": 7.918933391571045,
      "learning_rate": 6.291373051462737e-06,
      "loss": 0.4979,
      "step": 327500
    },
    {
      "epoch": 5.253042921204356,
      "grad_norm": 12.665647506713867,
      "learning_rate": 6.224642323297032e-06,
      "loss": 0.4813,
      "step": 328000
    },
    {
      "epoch": 5.261050608584241,
      "grad_norm": 21.97609519958496,
      "learning_rate": 6.157911595131326e-06,
      "loss": 0.4872,
      "step": 328500
    },
    {
      "epoch": 5.2690582959641254,
      "grad_norm": 8.657448768615723,
      "learning_rate": 6.091180866965621e-06,
      "loss": 0.4765,
      "step": 329000
    },
    {
      "epoch": 5.27706598334401,
      "grad_norm": 3.231367588043213,
      "learning_rate": 6.024450138799915e-06,
      "loss": 0.5089,
      "step": 329500
    },
    {
      "epoch": 5.285073670723895,
      "grad_norm": 3.2561051845550537,
      "learning_rate": 5.957719410634209e-06,
      "loss": 0.4941,
      "step": 330000
    },
    {
      "epoch": 5.293081358103779,
      "grad_norm": 3.5132923126220703,
      "learning_rate": 5.8909886824685035e-06,
      "loss": 0.4806,
      "step": 330500
    },
    {
      "epoch": 5.301089045483664,
      "grad_norm": 6.223061561584473,
      "learning_rate": 5.824257954302797e-06,
      "loss": 0.475,
      "step": 331000
    },
    {
      "epoch": 5.309096732863549,
      "grad_norm": 26.55878257751465,
      "learning_rate": 5.757527226137092e-06,
      "loss": 0.4877,
      "step": 331500
    },
    {
      "epoch": 5.317104420243433,
      "grad_norm": 2.6483941078186035,
      "learning_rate": 5.690796497971386e-06,
      "loss": 0.4824,
      "step": 332000
    },
    {
      "epoch": 5.325112107623318,
      "grad_norm": 5.1695027351379395,
      "learning_rate": 5.62406576980568e-06,
      "loss": 0.4866,
      "step": 332500
    },
    {
      "epoch": 5.333119795003203,
      "grad_norm": 4.2939677238464355,
      "learning_rate": 5.5573350416399746e-06,
      "loss": 0.4687,
      "step": 333000
    },
    {
      "epoch": 5.341127482383087,
      "grad_norm": 4.238346576690674,
      "learning_rate": 5.4906043134742684e-06,
      "loss": 0.4876,
      "step": 333500
    },
    {
      "epoch": 5.349135169762972,
      "grad_norm": 3.1566333770751953,
      "learning_rate": 5.423873585308563e-06,
      "loss": 0.4821,
      "step": 334000
    },
    {
      "epoch": 5.357142857142857,
      "grad_norm": 3.7363898754119873,
      "learning_rate": 5.357142857142857e-06,
      "loss": 0.485,
      "step": 334500
    },
    {
      "epoch": 5.365150544522741,
      "grad_norm": 12.400586128234863,
      "learning_rate": 5.290412128977151e-06,
      "loss": 0.4834,
      "step": 335000
    },
    {
      "epoch": 5.373158231902627,
      "grad_norm": 5.882442951202393,
      "learning_rate": 5.2236814008114465e-06,
      "loss": 0.4908,
      "step": 335500
    },
    {
      "epoch": 5.381165919282511,
      "grad_norm": 3.5278327465057373,
      "learning_rate": 5.15695067264574e-06,
      "loss": 0.4872,
      "step": 336000
    },
    {
      "epoch": 5.389173606662396,
      "grad_norm": 7.061009883880615,
      "learning_rate": 5.090219944480035e-06,
      "loss": 0.4968,
      "step": 336500
    },
    {
      "epoch": 5.397181294042281,
      "grad_norm": 2.266859769821167,
      "learning_rate": 5.023489216314329e-06,
      "loss": 0.4856,
      "step": 337000
    },
    {
      "epoch": 5.405188981422166,
      "grad_norm": 2.6333773136138916,
      "learning_rate": 4.956758488148623e-06,
      "loss": 0.477,
      "step": 337500
    },
    {
      "epoch": 5.41319666880205,
      "grad_norm": 17.05234718322754,
      "learning_rate": 4.8900277599829176e-06,
      "loss": 0.4915,
      "step": 338000
    },
    {
      "epoch": 5.421204356181935,
      "grad_norm": 5.005670547485352,
      "learning_rate": 4.8232970318172114e-06,
      "loss": 0.4647,
      "step": 338500
    },
    {
      "epoch": 5.42921204356182,
      "grad_norm": 6.949012756347656,
      "learning_rate": 4.756566303651506e-06,
      "loss": 0.4815,
      "step": 339000
    },
    {
      "epoch": 5.437219730941704,
      "grad_norm": 6.24033784866333,
      "learning_rate": 4.6898355754858e-06,
      "loss": 0.4803,
      "step": 339500
    },
    {
      "epoch": 5.445227418321589,
      "grad_norm": 6.081872940063477,
      "learning_rate": 4.623104847320094e-06,
      "loss": 0.4711,
      "step": 340000
    },
    {
      "epoch": 5.453235105701474,
      "grad_norm": 7.6153059005737305,
      "learning_rate": 4.556374119154389e-06,
      "loss": 0.4592,
      "step": 340500
    },
    {
      "epoch": 5.461242793081358,
      "grad_norm": 3.346057176589966,
      "learning_rate": 4.4896433909886825e-06,
      "loss": 0.4764,
      "step": 341000
    },
    {
      "epoch": 5.469250480461243,
      "grad_norm": 10.272503852844238,
      "learning_rate": 4.422912662822977e-06,
      "loss": 0.4707,
      "step": 341500
    },
    {
      "epoch": 5.477258167841128,
      "grad_norm": 6.078878879547119,
      "learning_rate": 4.356181934657271e-06,
      "loss": 0.4614,
      "step": 342000
    },
    {
      "epoch": 5.485265855221012,
      "grad_norm": 5.830956935882568,
      "learning_rate": 4.289451206491565e-06,
      "loss": 0.4695,
      "step": 342500
    },
    {
      "epoch": 5.493273542600897,
      "grad_norm": 6.5341081619262695,
      "learning_rate": 4.22272047832586e-06,
      "loss": 0.4879,
      "step": 343000
    },
    {
      "epoch": 5.501281229980782,
      "grad_norm": 5.779694080352783,
      "learning_rate": 4.155989750160154e-06,
      "loss": 0.4615,
      "step": 343500
    },
    {
      "epoch": 5.509288917360666,
      "grad_norm": 7.372576713562012,
      "learning_rate": 4.089259021994448e-06,
      "loss": 0.4628,
      "step": 344000
    },
    {
      "epoch": 5.517296604740551,
      "grad_norm": 2.799283266067505,
      "learning_rate": 4.022528293828742e-06,
      "loss": 0.4847,
      "step": 344500
    },
    {
      "epoch": 5.525304292120436,
      "grad_norm": 7.743137836456299,
      "learning_rate": 3.955797565663037e-06,
      "loss": 0.4707,
      "step": 345000
    },
    {
      "epoch": 5.53331197950032,
      "grad_norm": 16.805219650268555,
      "learning_rate": 3.889066837497331e-06,
      "loss": 0.4804,
      "step": 345500
    },
    {
      "epoch": 5.541319666880205,
      "grad_norm": 5.464966773986816,
      "learning_rate": 3.822336109331625e-06,
      "loss": 0.4594,
      "step": 346000
    },
    {
      "epoch": 5.54932735426009,
      "grad_norm": 6.363125801086426,
      "learning_rate": 3.7556053811659194e-06,
      "loss": 0.481,
      "step": 346500
    },
    {
      "epoch": 5.557335041639974,
      "grad_norm": 1.7123464345932007,
      "learning_rate": 3.6888746530002133e-06,
      "loss": 0.4647,
      "step": 347000
    },
    {
      "epoch": 5.565342729019859,
      "grad_norm": 4.194815635681152,
      "learning_rate": 3.6221439248345076e-06,
      "loss": 0.484,
      "step": 347500
    },
    {
      "epoch": 5.573350416399744,
      "grad_norm": 8.106250762939453,
      "learning_rate": 3.555413196668802e-06,
      "loss": 0.4689,
      "step": 348000
    },
    {
      "epoch": 5.581358103779628,
      "grad_norm": 5.541666507720947,
      "learning_rate": 3.488682468503097e-06,
      "loss": 0.4838,
      "step": 348500
    },
    {
      "epoch": 5.589365791159513,
      "grad_norm": 4.804227352142334,
      "learning_rate": 3.421951740337391e-06,
      "loss": 0.4787,
      "step": 349000
    },
    {
      "epoch": 5.597373478539398,
      "grad_norm": 9.19482135772705,
      "learning_rate": 3.355221012171685e-06,
      "loss": 0.4809,
      "step": 349500
    },
    {
      "epoch": 5.605381165919282,
      "grad_norm": 13.677319526672363,
      "learning_rate": 3.2884902840059795e-06,
      "loss": 0.4796,
      "step": 350000
    },
    {
      "epoch": 5.613388853299167,
      "grad_norm": 5.262714385986328,
      "learning_rate": 3.2217595558402738e-06,
      "loss": 0.4725,
      "step": 350500
    },
    {
      "epoch": 5.621396540679052,
      "grad_norm": 4.824803829193115,
      "learning_rate": 3.155028827674568e-06,
      "loss": 0.461,
      "step": 351000
    },
    {
      "epoch": 5.629404228058936,
      "grad_norm": 4.22475528717041,
      "learning_rate": 3.088298099508862e-06,
      "loss": 0.4873,
      "step": 351500
    },
    {
      "epoch": 5.637411915438821,
      "grad_norm": 16.183687210083008,
      "learning_rate": 3.0215673713431563e-06,
      "loss": 0.4811,
      "step": 352000
    },
    {
      "epoch": 5.645419602818706,
      "grad_norm": 50.56525802612305,
      "learning_rate": 2.9548366431774506e-06,
      "loss": 0.4707,
      "step": 352500
    },
    {
      "epoch": 5.653427290198591,
      "grad_norm": 6.292402744293213,
      "learning_rate": 2.888105915011745e-06,
      "loss": 0.4789,
      "step": 353000
    },
    {
      "epoch": 5.661434977578475,
      "grad_norm": 6.319599151611328,
      "learning_rate": 2.821375186846039e-06,
      "loss": 0.473,
      "step": 353500
    },
    {
      "epoch": 5.6694426649583605,
      "grad_norm": 10.424015998840332,
      "learning_rate": 2.7546444586803334e-06,
      "loss": 0.4886,
      "step": 354000
    },
    {
      "epoch": 5.677450352338244,
      "grad_norm": 2.838763475418091,
      "learning_rate": 2.6879137305146273e-06,
      "loss": 0.4942,
      "step": 354500
    },
    {
      "epoch": 5.68545803971813,
      "grad_norm": 6.149264812469482,
      "learning_rate": 2.6211830023489216e-06,
      "loss": 0.4716,
      "step": 355000
    },
    {
      "epoch": 5.6934657270980145,
      "grad_norm": 5.771134853363037,
      "learning_rate": 2.554452274183216e-06,
      "loss": 0.4864,
      "step": 355500
    },
    {
      "epoch": 5.701473414477899,
      "grad_norm": 5.413075923919678,
      "learning_rate": 2.4877215460175102e-06,
      "loss": 0.4853,
      "step": 356000
    },
    {
      "epoch": 5.709481101857784,
      "grad_norm": 6.618795871734619,
      "learning_rate": 2.4209908178518045e-06,
      "loss": 0.4904,
      "step": 356500
    },
    {
      "epoch": 5.7174887892376685,
      "grad_norm": 4.308932781219482,
      "learning_rate": 2.354260089686099e-06,
      "loss": 0.4802,
      "step": 357000
    },
    {
      "epoch": 5.725496476617553,
      "grad_norm": 4.6922688484191895,
      "learning_rate": 2.2875293615203927e-06,
      "loss": 0.4766,
      "step": 357500
    },
    {
      "epoch": 5.733504163997438,
      "grad_norm": 24.19145393371582,
      "learning_rate": 2.2207986333546874e-06,
      "loss": 0.4717,
      "step": 358000
    },
    {
      "epoch": 5.7415118513773225,
      "grad_norm": 5.315155029296875,
      "learning_rate": 2.1540679051889817e-06,
      "loss": 0.4844,
      "step": 358500
    },
    {
      "epoch": 5.749519538757207,
      "grad_norm": 8.700489044189453,
      "learning_rate": 2.087337177023276e-06,
      "loss": 0.4593,
      "step": 359000
    },
    {
      "epoch": 5.757527226137092,
      "grad_norm": 8.896708488464355,
      "learning_rate": 2.02060644885757e-06,
      "loss": 0.467,
      "step": 359500
    },
    {
      "epoch": 5.7655349135169764,
      "grad_norm": 3.5301380157470703,
      "learning_rate": 1.953875720691864e-06,
      "loss": 0.4581,
      "step": 360000
    },
    {
      "epoch": 5.773542600896861,
      "grad_norm": 5.9156012535095215,
      "learning_rate": 1.8871449925261585e-06,
      "loss": 0.489,
      "step": 360500
    },
    {
      "epoch": 5.781550288276746,
      "grad_norm": 6.3766679763793945,
      "learning_rate": 1.8204142643604528e-06,
      "loss": 0.4834,
      "step": 361000
    },
    {
      "epoch": 5.78955797565663,
      "grad_norm": 10.973319053649902,
      "learning_rate": 1.753683536194747e-06,
      "loss": 0.475,
      "step": 361500
    },
    {
      "epoch": 5.797565663036515,
      "grad_norm": 12.462100982666016,
      "learning_rate": 1.6869528080290412e-06,
      "loss": 0.4852,
      "step": 362000
    },
    {
      "epoch": 5.8055733504164,
      "grad_norm": 6.3160319328308105,
      "learning_rate": 1.6202220798633355e-06,
      "loss": 0.4711,
      "step": 362500
    },
    {
      "epoch": 5.813581037796284,
      "grad_norm": 3.182410717010498,
      "learning_rate": 1.5534913516976298e-06,
      "loss": 0.4811,
      "step": 363000
    },
    {
      "epoch": 5.821588725176169,
      "grad_norm": 4.807241439819336,
      "learning_rate": 1.486760623531924e-06,
      "loss": 0.4726,
      "step": 363500
    },
    {
      "epoch": 5.829596412556054,
      "grad_norm": 18.34586524963379,
      "learning_rate": 1.4200298953662184e-06,
      "loss": 0.4883,
      "step": 364000
    },
    {
      "epoch": 5.837604099935938,
      "grad_norm": 7.108320713043213,
      "learning_rate": 1.3532991672005125e-06,
      "loss": 0.4748,
      "step": 364500
    },
    {
      "epoch": 5.845611787315823,
      "grad_norm": 3.7425782680511475,
      "learning_rate": 1.2865684390348068e-06,
      "loss": 0.4697,
      "step": 365000
    },
    {
      "epoch": 5.853619474695708,
      "grad_norm": 5.137964725494385,
      "learning_rate": 1.219837710869101e-06,
      "loss": 0.471,
      "step": 365500
    },
    {
      "epoch": 5.861627162075592,
      "grad_norm": 27.71477508544922,
      "learning_rate": 1.1531069827033954e-06,
      "loss": 0.4742,
      "step": 366000
    },
    {
      "epoch": 5.869634849455477,
      "grad_norm": 5.923786640167236,
      "learning_rate": 1.0863762545376897e-06,
      "loss": 0.4885,
      "step": 366500
    },
    {
      "epoch": 5.877642536835362,
      "grad_norm": 8.95864486694336,
      "learning_rate": 1.0196455263719838e-06,
      "loss": 0.4821,
      "step": 367000
    },
    {
      "epoch": 5.885650224215246,
      "grad_norm": 14.131387710571289,
      "learning_rate": 9.529147982062781e-07,
      "loss": 0.4662,
      "step": 367500
    },
    {
      "epoch": 5.893657911595131,
      "grad_norm": 6.35711145401001,
      "learning_rate": 8.861840700405722e-07,
      "loss": 0.4589,
      "step": 368000
    },
    {
      "epoch": 5.901665598975016,
      "grad_norm": 10.677464485168457,
      "learning_rate": 8.194533418748665e-07,
      "loss": 0.4768,
      "step": 368500
    },
    {
      "epoch": 5.9096732863549,
      "grad_norm": 3.2761917114257812,
      "learning_rate": 7.527226137091608e-07,
      "loss": 0.4695,
      "step": 369000
    },
    {
      "epoch": 5.917680973734785,
      "grad_norm": 2.721064329147339,
      "learning_rate": 6.859918855434551e-07,
      "loss": 0.4737,
      "step": 369500
    },
    {
      "epoch": 5.92568866111467,
      "grad_norm": 4.378329753875732,
      "learning_rate": 6.192611573777493e-07,
      "loss": 0.4817,
      "step": 370000
    },
    {
      "epoch": 5.933696348494554,
      "grad_norm": 2.491569995880127,
      "learning_rate": 5.525304292120435e-07,
      "loss": 0.4673,
      "step": 370500
    },
    {
      "epoch": 5.941704035874439,
      "grad_norm": 5.123173236846924,
      "learning_rate": 4.857997010463378e-07,
      "loss": 0.4583,
      "step": 371000
    },
    {
      "epoch": 5.949711723254325,
      "grad_norm": 5.359287261962891,
      "learning_rate": 4.1906897288063213e-07,
      "loss": 0.4648,
      "step": 371500
    },
    {
      "epoch": 5.957719410634208,
      "grad_norm": 3.161585569381714,
      "learning_rate": 3.5233824471492633e-07,
      "loss": 0.4603,
      "step": 372000
    },
    {
      "epoch": 5.965727098014094,
      "grad_norm": 6.793612480163574,
      "learning_rate": 2.8560751654922063e-07,
      "loss": 0.4769,
      "step": 372500
    },
    {
      "epoch": 5.973734785393978,
      "grad_norm": 8.137609481811523,
      "learning_rate": 2.1887678838351487e-07,
      "loss": 0.4651,
      "step": 373000
    },
    {
      "epoch": 5.981742472773863,
      "grad_norm": 4.73988676071167,
      "learning_rate": 1.521460602178091e-07,
      "loss": 0.483,
      "step": 373500
    },
    {
      "epoch": 5.989750160153748,
      "grad_norm": 7.941307067871094,
      "learning_rate": 8.541533205210335e-08,
      "loss": 0.4651,
      "step": 374000
    },
    {
      "epoch": 5.997757847533633,
      "grad_norm": 14.444925308227539,
      "learning_rate": 1.868460388639761e-08,
      "loss": 0.4623,
      "step": 374500
    },
    {
      "epoch": 6.00480384307446,
      "grad_norm": 6.489445209503174,
      "learning_rate": 1.246997598078463e-05,
      "loss": 0.5379,
      "step": 375000
    },
    {
      "epoch": 6.012810248198559,
      "grad_norm": 6.349989891052246,
      "learning_rate": 1.2419935948759007e-05,
      "loss": 0.5272,
      "step": 375500
    },
    {
      "epoch": 6.020816653322658,
      "grad_norm": 40.217594146728516,
      "learning_rate": 1.2369895916733387e-05,
      "loss": 0.5368,
      "step": 376000
    },
    {
      "epoch": 6.028823058446758,
      "grad_norm": 2.6078624725341797,
      "learning_rate": 1.2319855884707766e-05,
      "loss": 0.5249,
      "step": 376500
    },
    {
      "epoch": 6.036829463570856,
      "grad_norm": 43.8788948059082,
      "learning_rate": 1.2269815852682147e-05,
      "loss": 0.5207,
      "step": 377000
    },
    {
      "epoch": 6.044835868694956,
      "grad_norm": 7.018585205078125,
      "learning_rate": 1.2219775820656525e-05,
      "loss": 0.5215,
      "step": 377500
    },
    {
      "epoch": 6.0528422738190555,
      "grad_norm": 3.2927608489990234,
      "learning_rate": 1.2169735788630905e-05,
      "loss": 0.5322,
      "step": 378000
    },
    {
      "epoch": 6.060848678943154,
      "grad_norm": 13.082660675048828,
      "learning_rate": 1.2119695756605285e-05,
      "loss": 0.5408,
      "step": 378500
    },
    {
      "epoch": 6.068855084067254,
      "grad_norm": 6.770926475524902,
      "learning_rate": 1.2069655724579665e-05,
      "loss": 0.5605,
      "step": 379000
    },
    {
      "epoch": 6.076861489191353,
      "grad_norm": 5.610046863555908,
      "learning_rate": 1.2019615692554043e-05,
      "loss": 0.5361,
      "step": 379500
    },
    {
      "epoch": 6.084867894315452,
      "grad_norm": 9.491530418395996,
      "learning_rate": 1.1969575660528423e-05,
      "loss": 0.5279,
      "step": 380000
    },
    {
      "epoch": 6.0928742994395515,
      "grad_norm": 3.8144562244415283,
      "learning_rate": 1.1919535628502803e-05,
      "loss": 0.5225,
      "step": 380500
    },
    {
      "epoch": 6.100880704563651,
      "grad_norm": 4.924041748046875,
      "learning_rate": 1.1869495596477183e-05,
      "loss": 0.5188,
      "step": 381000
    },
    {
      "epoch": 6.108887109687751,
      "grad_norm": 7.597010612487793,
      "learning_rate": 1.1819455564451563e-05,
      "loss": 0.5261,
      "step": 381500
    },
    {
      "epoch": 6.116893514811849,
      "grad_norm": 5.621057510375977,
      "learning_rate": 1.1769415532425941e-05,
      "loss": 0.5477,
      "step": 382000
    },
    {
      "epoch": 6.124899919935949,
      "grad_norm": 2.5233571529388428,
      "learning_rate": 1.1719375500400321e-05,
      "loss": 0.5258,
      "step": 382500
    },
    {
      "epoch": 6.132906325060048,
      "grad_norm": 6.6468095779418945,
      "learning_rate": 1.16693354683747e-05,
      "loss": 0.5163,
      "step": 383000
    },
    {
      "epoch": 6.140912730184147,
      "grad_norm": 2.9157867431640625,
      "learning_rate": 1.161929543634908e-05,
      "loss": 0.5333,
      "step": 383500
    },
    {
      "epoch": 6.148919135308247,
      "grad_norm": 4.376986980438232,
      "learning_rate": 1.1569255404323459e-05,
      "loss": 0.5226,
      "step": 384000
    },
    {
      "epoch": 6.156925540432346,
      "grad_norm": 2.7422232627868652,
      "learning_rate": 1.1519215372297839e-05,
      "loss": 0.5315,
      "step": 384500
    },
    {
      "epoch": 6.164931945556445,
      "grad_norm": 2.8726320266723633,
      "learning_rate": 1.1469175340272219e-05,
      "loss": 0.5227,
      "step": 385000
    },
    {
      "epoch": 6.172938350680544,
      "grad_norm": 8.765520095825195,
      "learning_rate": 1.1419135308246599e-05,
      "loss": 0.5303,
      "step": 385500
    },
    {
      "epoch": 6.180944755804644,
      "grad_norm": 2.08339262008667,
      "learning_rate": 1.1369095276220977e-05,
      "loss": 0.5355,
      "step": 386000
    },
    {
      "epoch": 6.188951160928743,
      "grad_norm": 5.077881336212158,
      "learning_rate": 1.1319055244195357e-05,
      "loss": 0.5194,
      "step": 386500
    },
    {
      "epoch": 6.196957566052842,
      "grad_norm": 3.342532157897949,
      "learning_rate": 1.1269015212169737e-05,
      "loss": 0.5305,
      "step": 387000
    },
    {
      "epoch": 6.204963971176942,
      "grad_norm": 3.671801805496216,
      "learning_rate": 1.1218975180144117e-05,
      "loss": 0.5232,
      "step": 387500
    },
    {
      "epoch": 6.21297037630104,
      "grad_norm": 2.9417855739593506,
      "learning_rate": 1.1168935148118495e-05,
      "loss": 0.5197,
      "step": 388000
    },
    {
      "epoch": 6.22097678142514,
      "grad_norm": 3.8245928287506104,
      "learning_rate": 1.1118895116092875e-05,
      "loss": 0.5157,
      "step": 388500
    },
    {
      "epoch": 6.2289831865492395,
      "grad_norm": 2.8356211185455322,
      "learning_rate": 1.1068855084067255e-05,
      "loss": 0.5145,
      "step": 389000
    },
    {
      "epoch": 6.236989591673339,
      "grad_norm": 7.132727146148682,
      "learning_rate": 1.1018815052041634e-05,
      "loss": 0.5182,
      "step": 389500
    },
    {
      "epoch": 6.244995996797438,
      "grad_norm": 6.18961238861084,
      "learning_rate": 1.0968775020016013e-05,
      "loss": 0.5359,
      "step": 390000
    },
    {
      "epoch": 6.253002401921537,
      "grad_norm": 6.944939136505127,
      "learning_rate": 1.0918734987990393e-05,
      "loss": 0.5126,
      "step": 390500
    },
    {
      "epoch": 6.261008807045637,
      "grad_norm": 3.3385353088378906,
      "learning_rate": 1.0868694955964772e-05,
      "loss": 0.5255,
      "step": 391000
    },
    {
      "epoch": 6.269015212169736,
      "grad_norm": 2.9821386337280273,
      "learning_rate": 1.0818654923939152e-05,
      "loss": 0.5452,
      "step": 391500
    },
    {
      "epoch": 6.277021617293835,
      "grad_norm": 5.394263744354248,
      "learning_rate": 1.076861489191353e-05,
      "loss": 0.5523,
      "step": 392000
    },
    {
      "epoch": 6.285028022417935,
      "grad_norm": 6.355091094970703,
      "learning_rate": 1.071857485988791e-05,
      "loss": 0.5255,
      "step": 392500
    },
    {
      "epoch": 6.293034427542033,
      "grad_norm": 2.474242925643921,
      "learning_rate": 1.066853482786229e-05,
      "loss": 0.5084,
      "step": 393000
    },
    {
      "epoch": 6.301040832666133,
      "grad_norm": 20.60873794555664,
      "learning_rate": 1.061849479583667e-05,
      "loss": 0.5162,
      "step": 393500
    },
    {
      "epoch": 6.3090472377902325,
      "grad_norm": 2.262011766433716,
      "learning_rate": 1.0568454763811048e-05,
      "loss": 0.5168,
      "step": 394000
    },
    {
      "epoch": 6.317053642914331,
      "grad_norm": 5.4063639640808105,
      "learning_rate": 1.051841473178543e-05,
      "loss": 0.5109,
      "step": 394500
    },
    {
      "epoch": 6.325060048038431,
      "grad_norm": 3.3394858837127686,
      "learning_rate": 1.0468374699759808e-05,
      "loss": 0.5238,
      "step": 395000
    },
    {
      "epoch": 6.33306645316253,
      "grad_norm": 2.4958882331848145,
      "learning_rate": 1.0418334667734188e-05,
      "loss": 0.5026,
      "step": 395500
    },
    {
      "epoch": 6.341072858286629,
      "grad_norm": 11.630367279052734,
      "learning_rate": 1.0368294635708566e-05,
      "loss": 0.5007,
      "step": 396000
    },
    {
      "epoch": 6.3490792634107285,
      "grad_norm": 2.7932183742523193,
      "learning_rate": 1.0318254603682948e-05,
      "loss": 0.5009,
      "step": 396500
    },
    {
      "epoch": 6.357085668534828,
      "grad_norm": 5.2203779220581055,
      "learning_rate": 1.0268214571657326e-05,
      "loss": 0.51,
      "step": 397000
    },
    {
      "epoch": 6.365092073658927,
      "grad_norm": 3.786004066467285,
      "learning_rate": 1.0218174539631706e-05,
      "loss": 0.5355,
      "step": 397500
    },
    {
      "epoch": 6.373098478783026,
      "grad_norm": 8.400335311889648,
      "learning_rate": 1.0168134507606084e-05,
      "loss": 0.5198,
      "step": 398000
    },
    {
      "epoch": 6.381104883907126,
      "grad_norm": 3.023146390914917,
      "learning_rate": 1.0118094475580466e-05,
      "loss": 0.5183,
      "step": 398500
    },
    {
      "epoch": 6.389111289031225,
      "grad_norm": 3.6771199703216553,
      "learning_rate": 1.0068054443554844e-05,
      "loss": 0.5431,
      "step": 399000
    },
    {
      "epoch": 6.397117694155324,
      "grad_norm": 3.7018086910247803,
      "learning_rate": 1.0018014411529224e-05,
      "loss": 0.5158,
      "step": 399500
    },
    {
      "epoch": 6.405124099279424,
      "grad_norm": 4.45550537109375,
      "learning_rate": 9.967974379503602e-06,
      "loss": 0.539,
      "step": 400000
    },
    {
      "epoch": 6.413130504403523,
      "grad_norm": 6.945709228515625,
      "learning_rate": 9.917934347477984e-06,
      "loss": 0.5236,
      "step": 400500
    },
    {
      "epoch": 6.421136909527622,
      "grad_norm": 3.0125768184661865,
      "learning_rate": 9.867894315452362e-06,
      "loss": 0.5283,
      "step": 401000
    },
    {
      "epoch": 6.429143314651721,
      "grad_norm": 2.2521491050720215,
      "learning_rate": 9.817854283426742e-06,
      "loss": 0.5218,
      "step": 401500
    },
    {
      "epoch": 6.437149719775821,
      "grad_norm": 10.816206932067871,
      "learning_rate": 9.76781425140112e-06,
      "loss": 0.5233,
      "step": 402000
    },
    {
      "epoch": 6.44515612489992,
      "grad_norm": 5.16963005065918,
      "learning_rate": 9.717774219375502e-06,
      "loss": 0.5233,
      "step": 402500
    },
    {
      "epoch": 6.453162530024019,
      "grad_norm": 11.20669937133789,
      "learning_rate": 9.66773418734988e-06,
      "loss": 0.5232,
      "step": 403000
    },
    {
      "epoch": 6.461168935148119,
      "grad_norm": 1.7946531772613525,
      "learning_rate": 9.61769415532426e-06,
      "loss": 0.515,
      "step": 403500
    },
    {
      "epoch": 6.469175340272217,
      "grad_norm": 4.266251087188721,
      "learning_rate": 9.56765412329864e-06,
      "loss": 0.5263,
      "step": 404000
    },
    {
      "epoch": 6.477181745396317,
      "grad_norm": 4.580138206481934,
      "learning_rate": 9.51761409127302e-06,
      "loss": 0.5195,
      "step": 404500
    },
    {
      "epoch": 6.4851881505204165,
      "grad_norm": 4.827428340911865,
      "learning_rate": 9.467574059247398e-06,
      "loss": 0.5305,
      "step": 405000
    },
    {
      "epoch": 6.493194555644515,
      "grad_norm": 7.84635591506958,
      "learning_rate": 9.417534027221778e-06,
      "loss": 0.5382,
      "step": 405500
    },
    {
      "epoch": 6.501200960768615,
      "grad_norm": 2.3029398918151855,
      "learning_rate": 9.367493995196158e-06,
      "loss": 0.5221,
      "step": 406000
    },
    {
      "epoch": 6.509207365892714,
      "grad_norm": 2.2033228874206543,
      "learning_rate": 9.317453963170538e-06,
      "loss": 0.5023,
      "step": 406500
    },
    {
      "epoch": 6.517213771016813,
      "grad_norm": 5.614926338195801,
      "learning_rate": 9.267413931144916e-06,
      "loss": 0.5311,
      "step": 407000
    },
    {
      "epoch": 6.5252201761409125,
      "grad_norm": 6.790267467498779,
      "learning_rate": 9.217373899119296e-06,
      "loss": 0.5045,
      "step": 407500
    },
    {
      "epoch": 6.533226581265012,
      "grad_norm": 4.905937194824219,
      "learning_rate": 9.167333867093676e-06,
      "loss": 0.5229,
      "step": 408000
    },
    {
      "epoch": 6.541232986389112,
      "grad_norm": 2.6999659538269043,
      "learning_rate": 9.117293835068055e-06,
      "loss": 0.5145,
      "step": 408500
    },
    {
      "epoch": 6.54923939151321,
      "grad_norm": 2.935917615890503,
      "learning_rate": 9.067253803042435e-06,
      "loss": 0.5436,
      "step": 409000
    },
    {
      "epoch": 6.55724579663731,
      "grad_norm": 2.4706993103027344,
      "learning_rate": 9.017213771016814e-06,
      "loss": 0.5125,
      "step": 409500
    },
    {
      "epoch": 6.5652522017614094,
      "grad_norm": 13.596161842346191,
      "learning_rate": 8.967173738991193e-06,
      "loss": 0.5277,
      "step": 410000
    },
    {
      "epoch": 6.573258606885508,
      "grad_norm": 6.709422588348389,
      "learning_rate": 8.917133706965573e-06,
      "loss": 0.5261,
      "step": 410500
    },
    {
      "epoch": 6.581265012009608,
      "grad_norm": 4.697625160217285,
      "learning_rate": 8.867093674939953e-06,
      "loss": 0.5182,
      "step": 411000
    },
    {
      "epoch": 6.589271417133707,
      "grad_norm": 9.661803245544434,
      "learning_rate": 8.817053642914331e-06,
      "loss": 0.528,
      "step": 411500
    },
    {
      "epoch": 6.597277822257806,
      "grad_norm": 56.60419464111328,
      "learning_rate": 8.767013610888711e-06,
      "loss": 0.5273,
      "step": 412000
    },
    {
      "epoch": 6.6052842273819055,
      "grad_norm": 37.21442794799805,
      "learning_rate": 8.716973578863091e-06,
      "loss": 0.5365,
      "step": 412500
    },
    {
      "epoch": 6.613290632506005,
      "grad_norm": 3.222123861312866,
      "learning_rate": 8.666933546837471e-06,
      "loss": 0.5282,
      "step": 413000
    },
    {
      "epoch": 6.621297037630104,
      "grad_norm": 5.840672969818115,
      "learning_rate": 8.61689351481185e-06,
      "loss": 0.5148,
      "step": 413500
    },
    {
      "epoch": 6.629303442754203,
      "grad_norm": 2.300248861312866,
      "learning_rate": 8.56685348278623e-06,
      "loss": 0.5222,
      "step": 414000
    },
    {
      "epoch": 6.637309847878303,
      "grad_norm": 2.0543291568756104,
      "learning_rate": 8.516813450760609e-06,
      "loss": 0.5204,
      "step": 414500
    },
    {
      "epoch": 6.645316253002402,
      "grad_norm": 2.254826307296753,
      "learning_rate": 8.466773418734989e-06,
      "loss": 0.5029,
      "step": 415000
    },
    {
      "epoch": 6.653322658126501,
      "grad_norm": 6.052958965301514,
      "learning_rate": 8.416733386709367e-06,
      "loss": 0.5221,
      "step": 415500
    },
    {
      "epoch": 6.661329063250601,
      "grad_norm": 2.3017160892486572,
      "learning_rate": 8.366693354683747e-06,
      "loss": 0.5204,
      "step": 416000
    },
    {
      "epoch": 6.669335468374699,
      "grad_norm": 2.088209629058838,
      "learning_rate": 8.316653322658127e-06,
      "loss": 0.5127,
      "step": 416500
    },
    {
      "epoch": 6.677341873498799,
      "grad_norm": 4.783473014831543,
      "learning_rate": 8.266613290632507e-06,
      "loss": 0.5173,
      "step": 417000
    },
    {
      "epoch": 6.685348278622898,
      "grad_norm": 37.68980407714844,
      "learning_rate": 8.216573258606885e-06,
      "loss": 0.5111,
      "step": 417500
    },
    {
      "epoch": 6.693354683746998,
      "grad_norm": 3.5437917709350586,
      "learning_rate": 8.166533226581267e-06,
      "loss": 0.5016,
      "step": 418000
    },
    {
      "epoch": 6.701361088871097,
      "grad_norm": 3.13859486579895,
      "learning_rate": 8.116493194555645e-06,
      "loss": 0.5157,
      "step": 418500
    },
    {
      "epoch": 6.709367493995196,
      "grad_norm": 6.429101467132568,
      "learning_rate": 8.066453162530025e-06,
      "loss": 0.4979,
      "step": 419000
    },
    {
      "epoch": 6.717373899119296,
      "grad_norm": 8.047125816345215,
      "learning_rate": 8.016413130504403e-06,
      "loss": 0.525,
      "step": 419500
    },
    {
      "epoch": 6.725380304243394,
      "grad_norm": 5.135283470153809,
      "learning_rate": 7.966373098478785e-06,
      "loss": 0.52,
      "step": 420000
    },
    {
      "epoch": 6.733386709367494,
      "grad_norm": 6.651882171630859,
      "learning_rate": 7.916333066453163e-06,
      "loss": 0.5067,
      "step": 420500
    },
    {
      "epoch": 6.7413931144915935,
      "grad_norm": 2.0228075981140137,
      "learning_rate": 7.866293034427543e-06,
      "loss": 0.5098,
      "step": 421000
    },
    {
      "epoch": 6.749399519615692,
      "grad_norm": 7.062763690948486,
      "learning_rate": 7.816253002401921e-06,
      "loss": 0.5137,
      "step": 421500
    },
    {
      "epoch": 6.757405924739792,
      "grad_norm": 3.899059295654297,
      "learning_rate": 7.766212970376303e-06,
      "loss": 0.529,
      "step": 422000
    },
    {
      "epoch": 6.765412329863891,
      "grad_norm": 5.508819580078125,
      "learning_rate": 7.71617293835068e-06,
      "loss": 0.5338,
      "step": 422500
    },
    {
      "epoch": 6.77341873498799,
      "grad_norm": 4.613231658935547,
      "learning_rate": 7.66613290632506e-06,
      "loss": 0.5017,
      "step": 423000
    },
    {
      "epoch": 6.7814251401120895,
      "grad_norm": 6.159217834472656,
      "learning_rate": 7.61609287429944e-06,
      "loss": 0.5162,
      "step": 423500
    },
    {
      "epoch": 6.789431545236189,
      "grad_norm": 1.4645326137542725,
      "learning_rate": 7.56605284227382e-06,
      "loss": 0.5012,
      "step": 424000
    },
    {
      "epoch": 6.797437950360289,
      "grad_norm": 26.76519775390625,
      "learning_rate": 7.5160128102481995e-06,
      "loss": 0.4923,
      "step": 424500
    },
    {
      "epoch": 6.805444355484387,
      "grad_norm": 5.777685642242432,
      "learning_rate": 7.4659727782225786e-06,
      "loss": 0.5096,
      "step": 425000
    },
    {
      "epoch": 6.813450760608487,
      "grad_norm": 2.376065254211426,
      "learning_rate": 7.415932746196958e-06,
      "loss": 0.5161,
      "step": 425500
    },
    {
      "epoch": 6.821457165732586,
      "grad_norm": 2.2009823322296143,
      "learning_rate": 7.365892714171338e-06,
      "loss": 0.5137,
      "step": 426000
    },
    {
      "epoch": 6.829463570856685,
      "grad_norm": 3.0841612815856934,
      "learning_rate": 7.315852682145717e-06,
      "loss": 0.527,
      "step": 426500
    },
    {
      "epoch": 6.837469975980785,
      "grad_norm": 5.224691390991211,
      "learning_rate": 7.2658126501200965e-06,
      "loss": 0.4949,
      "step": 427000
    },
    {
      "epoch": 6.845476381104884,
      "grad_norm": 5.193749904632568,
      "learning_rate": 7.2157726180944755e-06,
      "loss": 0.5025,
      "step": 427500
    },
    {
      "epoch": 6.853482786228983,
      "grad_norm": 2.7499608993530273,
      "learning_rate": 7.165732586068856e-06,
      "loss": 0.5126,
      "step": 428000
    },
    {
      "epoch": 6.861489191353082,
      "grad_norm": 2.0115108489990234,
      "learning_rate": 7.115692554043235e-06,
      "loss": 0.5355,
      "step": 428500
    },
    {
      "epoch": 6.869495596477182,
      "grad_norm": 4.5382208824157715,
      "learning_rate": 7.065652522017614e-06,
      "loss": 0.5162,
      "step": 429000
    },
    {
      "epoch": 6.877502001601281,
      "grad_norm": 4.257728099822998,
      "learning_rate": 7.015612489991993e-06,
      "loss": 0.5288,
      "step": 429500
    },
    {
      "epoch": 6.88550840672538,
      "grad_norm": 4.9157843589782715,
      "learning_rate": 6.965572457966374e-06,
      "loss": 0.5029,
      "step": 430000
    },
    {
      "epoch": 6.89351481184948,
      "grad_norm": 4.047584533691406,
      "learning_rate": 6.915532425940753e-06,
      "loss": 0.5033,
      "step": 430500
    },
    {
      "epoch": 6.901521216973579,
      "grad_norm": 6.434427738189697,
      "learning_rate": 6.865492393915132e-06,
      "loss": 0.4939,
      "step": 431000
    },
    {
      "epoch": 6.909527622097678,
      "grad_norm": 6.3542890548706055,
      "learning_rate": 6.815452361889511e-06,
      "loss": 0.5222,
      "step": 431500
    },
    {
      "epoch": 6.917534027221778,
      "grad_norm": 1.3974214792251587,
      "learning_rate": 6.765412329863892e-06,
      "loss": 0.5145,
      "step": 432000
    },
    {
      "epoch": 6.925540432345876,
      "grad_norm": 2.307971239089966,
      "learning_rate": 6.715372297838271e-06,
      "loss": 0.5124,
      "step": 432500
    },
    {
      "epoch": 6.933546837469976,
      "grad_norm": 4.259541988372803,
      "learning_rate": 6.66533226581265e-06,
      "loss": 0.5159,
      "step": 433000
    },
    {
      "epoch": 6.941553242594075,
      "grad_norm": 6.875511646270752,
      "learning_rate": 6.615292233787029e-06,
      "loss": 0.5184,
      "step": 433500
    },
    {
      "epoch": 6.949559647718175,
      "grad_norm": 4.843863010406494,
      "learning_rate": 6.56525220176141e-06,
      "loss": 0.5237,
      "step": 434000
    },
    {
      "epoch": 6.957566052842274,
      "grad_norm": 4.169986248016357,
      "learning_rate": 6.515212169735789e-06,
      "loss": 0.504,
      "step": 434500
    },
    {
      "epoch": 6.965572457966373,
      "grad_norm": 4.665787696838379,
      "learning_rate": 6.465172137710168e-06,
      "loss": 0.5032,
      "step": 435000
    },
    {
      "epoch": 6.973578863090473,
      "grad_norm": 14.001666069030762,
      "learning_rate": 6.415132105684547e-06,
      "loss": 0.5141,
      "step": 435500
    },
    {
      "epoch": 6.981585268214571,
      "grad_norm": 5.4337639808654785,
      "learning_rate": 6.365092073658928e-06,
      "loss": 0.5104,
      "step": 436000
    },
    {
      "epoch": 6.989591673338671,
      "grad_norm": 2.7248177528381348,
      "learning_rate": 6.315052041633307e-06,
      "loss": 0.5072,
      "step": 436500
    },
    {
      "epoch": 6.9975980784627705,
      "grad_norm": 4.6811394691467285,
      "learning_rate": 6.265012009607686e-06,
      "loss": 0.5099,
      "step": 437000
    },
    {
      "epoch": 7.005604483586869,
      "grad_norm": 5.723573207855225,
      "learning_rate": 6.214971977582066e-06,
      "loss": 0.4821,
      "step": 437500
    },
    {
      "epoch": 7.013610888710969,
      "grad_norm": 2.651501417160034,
      "learning_rate": 6.164931945556445e-06,
      "loss": 0.4996,
      "step": 438000
    },
    {
      "epoch": 7.021617293835068,
      "grad_norm": 4.492900848388672,
      "learning_rate": 6.114891913530825e-06,
      "loss": 0.4988,
      "step": 438500
    },
    {
      "epoch": 7.029623698959167,
      "grad_norm": 11.724690437316895,
      "learning_rate": 6.064851881505204e-06,
      "loss": 0.4999,
      "step": 439000
    },
    {
      "epoch": 7.0376301040832665,
      "grad_norm": 4.99686336517334,
      "learning_rate": 6.014811849479584e-06,
      "loss": 0.4803,
      "step": 439500
    },
    {
      "epoch": 7.045636509207366,
      "grad_norm": 16.415504455566406,
      "learning_rate": 5.964771817453964e-06,
      "loss": 0.4881,
      "step": 440000
    },
    {
      "epoch": 7.053642914331466,
      "grad_norm": 8.051546096801758,
      "learning_rate": 5.914731785428343e-06,
      "loss": 0.4964,
      "step": 440500
    },
    {
      "epoch": 7.061649319455564,
      "grad_norm": 6.402773857116699,
      "learning_rate": 5.864691753402723e-06,
      "loss": 0.4758,
      "step": 441000
    },
    {
      "epoch": 7.069655724579664,
      "grad_norm": 5.922656536102295,
      "learning_rate": 5.814651721377102e-06,
      "loss": 0.494,
      "step": 441500
    },
    {
      "epoch": 7.077662129703763,
      "grad_norm": 3.2784242630004883,
      "learning_rate": 5.764611689351482e-06,
      "loss": 0.5033,
      "step": 442000
    },
    {
      "epoch": 7.085668534827862,
      "grad_norm": 5.247826099395752,
      "learning_rate": 5.714571657325861e-06,
      "loss": 0.5071,
      "step": 442500
    },
    {
      "epoch": 7.093674939951962,
      "grad_norm": 8.058418273925781,
      "learning_rate": 5.6645316253002406e-06,
      "loss": 0.4986,
      "step": 443000
    },
    {
      "epoch": 7.101681345076061,
      "grad_norm": 4.1964311599731445,
      "learning_rate": 5.61449159327462e-06,
      "loss": 0.4924,
      "step": 443500
    },
    {
      "epoch": 7.10968775020016,
      "grad_norm": 3.90336275100708,
      "learning_rate": 5.5644515612489995e-06,
      "loss": 0.486,
      "step": 444000
    },
    {
      "epoch": 7.117694155324259,
      "grad_norm": 6.021664619445801,
      "learning_rate": 5.5144115292233786e-06,
      "loss": 0.5054,
      "step": 444500
    },
    {
      "epoch": 7.125700560448359,
      "grad_norm": 3.214731216430664,
      "learning_rate": 5.4643714971977585e-06,
      "loss": 0.4844,
      "step": 445000
    },
    {
      "epoch": 7.133706965572458,
      "grad_norm": 6.584140300750732,
      "learning_rate": 5.4143314651721375e-06,
      "loss": 0.4949,
      "step": 445500
    },
    {
      "epoch": 7.141713370696557,
      "grad_norm": 11.01934814453125,
      "learning_rate": 5.364291433146517e-06,
      "loss": 0.505,
      "step": 446000
    },
    {
      "epoch": 7.149719775820657,
      "grad_norm": 3.481618642807007,
      "learning_rate": 5.3142514011208965e-06,
      "loss": 0.4808,
      "step": 446500
    },
    {
      "epoch": 7.157726180944755,
      "grad_norm": 2.2123208045959473,
      "learning_rate": 5.264211369095276e-06,
      "loss": 0.4962,
      "step": 447000
    },
    {
      "epoch": 7.165732586068855,
      "grad_norm": 4.900078296661377,
      "learning_rate": 5.214171337069656e-06,
      "loss": 0.4923,
      "step": 447500
    },
    {
      "epoch": 7.1737389911929546,
      "grad_norm": 3.543626546859741,
      "learning_rate": 5.164131305044035e-06,
      "loss": 0.5017,
      "step": 448000
    },
    {
      "epoch": 7.181745396317053,
      "grad_norm": 2.8294904232025146,
      "learning_rate": 5.114091273018415e-06,
      "loss": 0.4854,
      "step": 448500
    },
    {
      "epoch": 7.189751801441153,
      "grad_norm": 19.949251174926758,
      "learning_rate": 5.064051240992794e-06,
      "loss": 0.4844,
      "step": 449000
    },
    {
      "epoch": 7.197758206565252,
      "grad_norm": 10.137494087219238,
      "learning_rate": 5.014011208967174e-06,
      "loss": 0.4916,
      "step": 449500
    },
    {
      "epoch": 7.205764611689352,
      "grad_norm": 3.297030448913574,
      "learning_rate": 4.963971176941554e-06,
      "loss": 0.4892,
      "step": 450000
    },
    {
      "epoch": 7.213771016813451,
      "grad_norm": 5.850214004516602,
      "learning_rate": 4.913931144915933e-06,
      "loss": 0.4977,
      "step": 450500
    },
    {
      "epoch": 7.22177742193755,
      "grad_norm": 5.350966930389404,
      "learning_rate": 4.863891112890313e-06,
      "loss": 0.4753,
      "step": 451000
    },
    {
      "epoch": 7.22978382706165,
      "grad_norm": 7.149205684661865,
      "learning_rate": 4.813851080864692e-06,
      "loss": 0.492,
      "step": 451500
    },
    {
      "epoch": 7.237790232185748,
      "grad_norm": 25.004972457885742,
      "learning_rate": 4.763811048839072e-06,
      "loss": 0.4895,
      "step": 452000
    },
    {
      "epoch": 7.245796637309848,
      "grad_norm": 4.553618907928467,
      "learning_rate": 4.713771016813451e-06,
      "loss": 0.4872,
      "step": 452500
    },
    {
      "epoch": 7.2538030424339475,
      "grad_norm": 3.799992084503174,
      "learning_rate": 4.663730984787831e-06,
      "loss": 0.4824,
      "step": 453000
    },
    {
      "epoch": 7.261809447558046,
      "grad_norm": 8.544367790222168,
      "learning_rate": 4.61369095276221e-06,
      "loss": 0.4888,
      "step": 453500
    },
    {
      "epoch": 7.269815852682146,
      "grad_norm": 10.197831153869629,
      "learning_rate": 4.56365092073659e-06,
      "loss": 0.4888,
      "step": 454000
    },
    {
      "epoch": 7.277822257806245,
      "grad_norm": 7.2693023681640625,
      "learning_rate": 4.513610888710969e-06,
      "loss": 0.4963,
      "step": 454500
    },
    {
      "epoch": 7.285828662930344,
      "grad_norm": 8.08735179901123,
      "learning_rate": 4.463570856685349e-06,
      "loss": 0.4922,
      "step": 455000
    },
    {
      "epoch": 7.2938350680544435,
      "grad_norm": 3.0737500190734863,
      "learning_rate": 4.413530824659728e-06,
      "loss": 0.4901,
      "step": 455500
    },
    {
      "epoch": 7.301841473178543,
      "grad_norm": 2.078023672103882,
      "learning_rate": 4.363490792634108e-06,
      "loss": 0.4906,
      "step": 456000
    },
    {
      "epoch": 7.309847878302642,
      "grad_norm": 2.4707329273223877,
      "learning_rate": 4.313450760608487e-06,
      "loss": 0.4926,
      "step": 456500
    },
    {
      "epoch": 7.317854283426741,
      "grad_norm": 3.314307928085327,
      "learning_rate": 4.263410728582867e-06,
      "loss": 0.4961,
      "step": 457000
    },
    {
      "epoch": 7.325860688550841,
      "grad_norm": 4.165566444396973,
      "learning_rate": 4.213370696557246e-06,
      "loss": 0.509,
      "step": 457500
    },
    {
      "epoch": 7.3338670936749395,
      "grad_norm": 6.085403919219971,
      "learning_rate": 4.163330664531626e-06,
      "loss": 0.4887,
      "step": 458000
    },
    {
      "epoch": 7.341873498799039,
      "grad_norm": 2.8519351482391357,
      "learning_rate": 4.113290632506005e-06,
      "loss": 0.5058,
      "step": 458500
    },
    {
      "epoch": 7.349879903923139,
      "grad_norm": 4.056154727935791,
      "learning_rate": 4.063250600480385e-06,
      "loss": 0.4827,
      "step": 459000
    },
    {
      "epoch": 7.357886309047238,
      "grad_norm": 5.6699700355529785,
      "learning_rate": 4.013210568454764e-06,
      "loss": 0.4987,
      "step": 459500
    },
    {
      "epoch": 7.365892714171337,
      "grad_norm": 8.525906562805176,
      "learning_rate": 3.963170536429144e-06,
      "loss": 0.4915,
      "step": 460000
    },
    {
      "epoch": 7.373899119295436,
      "grad_norm": 6.894628524780273,
      "learning_rate": 3.913130504403523e-06,
      "loss": 0.476,
      "step": 460500
    },
    {
      "epoch": 7.381905524419536,
      "grad_norm": 12.734676361083984,
      "learning_rate": 3.8630904723779026e-06,
      "loss": 0.4879,
      "step": 461000
    },
    {
      "epoch": 7.389911929543635,
      "grad_norm": 3.6362645626068115,
      "learning_rate": 3.8130504403522816e-06,
      "loss": 0.5053,
      "step": 461500
    },
    {
      "epoch": 7.397918334667734,
      "grad_norm": 2.1059722900390625,
      "learning_rate": 3.7630104083266615e-06,
      "loss": 0.5075,
      "step": 462000
    },
    {
      "epoch": 7.405924739791834,
      "grad_norm": 3.650115489959717,
      "learning_rate": 3.7129703763010406e-06,
      "loss": 0.4867,
      "step": 462500
    },
    {
      "epoch": 7.413931144915932,
      "grad_norm": 9.553009986877441,
      "learning_rate": 3.6629303442754205e-06,
      "loss": 0.5014,
      "step": 463000
    },
    {
      "epoch": 7.421937550040032,
      "grad_norm": 6.380794048309326,
      "learning_rate": 3.6128903122498004e-06,
      "loss": 0.5016,
      "step": 463500
    },
    {
      "epoch": 7.4299439551641315,
      "grad_norm": 3.5514321327209473,
      "learning_rate": 3.5628502802241794e-06,
      "loss": 0.4779,
      "step": 464000
    },
    {
      "epoch": 7.43795036028823,
      "grad_norm": 2.6235086917877197,
      "learning_rate": 3.5128102481985593e-06,
      "loss": 0.4915,
      "step": 464500
    },
    {
      "epoch": 7.44595676541233,
      "grad_norm": 2.2690625190734863,
      "learning_rate": 3.4627702161729384e-06,
      "loss": 0.4903,
      "step": 465000
    },
    {
      "epoch": 7.453963170536429,
      "grad_norm": 3.8353350162506104,
      "learning_rate": 3.4127301841473183e-06,
      "loss": 0.4844,
      "step": 465500
    },
    {
      "epoch": 7.461969575660529,
      "grad_norm": 8.132308006286621,
      "learning_rate": 3.3626901521216973e-06,
      "loss": 0.4951,
      "step": 466000
    },
    {
      "epoch": 7.4699759807846275,
      "grad_norm": 7.289320945739746,
      "learning_rate": 3.3126501200960772e-06,
      "loss": 0.4867,
      "step": 466500
    },
    {
      "epoch": 7.477982385908727,
      "grad_norm": 4.406745433807373,
      "learning_rate": 3.2626100880704563e-06,
      "loss": 0.4691,
      "step": 467000
    },
    {
      "epoch": 7.485988791032827,
      "grad_norm": 5.706809997558594,
      "learning_rate": 3.212570056044836e-06,
      "loss": 0.4883,
      "step": 467500
    },
    {
      "epoch": 7.493995196156925,
      "grad_norm": 2.5611534118652344,
      "learning_rate": 3.1625300240192156e-06,
      "loss": 0.4855,
      "step": 468000
    },
    {
      "epoch": 7.502001601281025,
      "grad_norm": 9.908842086791992,
      "learning_rate": 3.112489991993595e-06,
      "loss": 0.4859,
      "step": 468500
    },
    {
      "epoch": 7.5100080064051244,
      "grad_norm": 5.333086013793945,
      "learning_rate": 3.0624499599679746e-06,
      "loss": 0.5068,
      "step": 469000
    },
    {
      "epoch": 7.518014411529223,
      "grad_norm": 9.416672706604004,
      "learning_rate": 3.012409927942354e-06,
      "loss": 0.4908,
      "step": 469500
    },
    {
      "epoch": 7.526020816653323,
      "grad_norm": 8.861029624938965,
      "learning_rate": 2.9623698959167336e-06,
      "loss": 0.4855,
      "step": 470000
    },
    {
      "epoch": 7.534027221777422,
      "grad_norm": 3.687864303588867,
      "learning_rate": 2.912329863891113e-06,
      "loss": 0.4917,
      "step": 470500
    },
    {
      "epoch": 7.542033626901521,
      "grad_norm": 2.4257428646087646,
      "learning_rate": 2.8622898318654925e-06,
      "loss": 0.493,
      "step": 471000
    },
    {
      "epoch": 7.5500400320256205,
      "grad_norm": 4.991277694702148,
      "learning_rate": 2.812249799839872e-06,
      "loss": 0.4808,
      "step": 471500
    },
    {
      "epoch": 7.55804643714972,
      "grad_norm": 8.524785041809082,
      "learning_rate": 2.7622097678142515e-06,
      "loss": 0.4961,
      "step": 472000
    },
    {
      "epoch": 7.566052842273819,
      "grad_norm": 4.375730991363525,
      "learning_rate": 2.712169735788631e-06,
      "loss": 0.4964,
      "step": 472500
    },
    {
      "epoch": 7.574059247397918,
      "grad_norm": 7.496983528137207,
      "learning_rate": 2.6621297037630104e-06,
      "loss": 0.4957,
      "step": 473000
    },
    {
      "epoch": 7.582065652522018,
      "grad_norm": 13.325799942016602,
      "learning_rate": 2.61208967173739e-06,
      "loss": 0.4858,
      "step": 473500
    },
    {
      "epoch": 7.5900720576461165,
      "grad_norm": 3.2725422382354736,
      "learning_rate": 2.5620496397117694e-06,
      "loss": 0.4972,
      "step": 474000
    },
    {
      "epoch": 7.598078462770216,
      "grad_norm": 5.577103137969971,
      "learning_rate": 2.5120096076861493e-06,
      "loss": 0.4817,
      "step": 474500
    },
    {
      "epoch": 7.606084867894316,
      "grad_norm": 3.363316059112549,
      "learning_rate": 2.4619695756605287e-06,
      "loss": 0.4732,
      "step": 475000
    },
    {
      "epoch": 7.614091273018415,
      "grad_norm": 4.087360382080078,
      "learning_rate": 2.4119295436349082e-06,
      "loss": 0.4862,
      "step": 475500
    },
    {
      "epoch": 7.622097678142514,
      "grad_norm": 2.6109509468078613,
      "learning_rate": 2.3618895116092877e-06,
      "loss": 0.4783,
      "step": 476000
    },
    {
      "epoch": 7.630104083266613,
      "grad_norm": 3.063369035720825,
      "learning_rate": 2.311849479583667e-06,
      "loss": 0.4938,
      "step": 476500
    },
    {
      "epoch": 7.638110488390713,
      "grad_norm": 4.789166450500488,
      "learning_rate": 2.2618094475580466e-06,
      "loss": 0.4951,
      "step": 477000
    },
    {
      "epoch": 7.646116893514812,
      "grad_norm": 3.6916799545288086,
      "learning_rate": 2.211769415532426e-06,
      "loss": 0.5002,
      "step": 477500
    },
    {
      "epoch": 7.654123298638911,
      "grad_norm": 5.895282745361328,
      "learning_rate": 2.1617293835068056e-06,
      "loss": 0.4888,
      "step": 478000
    },
    {
      "epoch": 7.662129703763011,
      "grad_norm": 8.633671760559082,
      "learning_rate": 2.111689351481185e-06,
      "loss": 0.487,
      "step": 478500
    },
    {
      "epoch": 7.670136108887109,
      "grad_norm": 6.217255115509033,
      "learning_rate": 2.0616493194555646e-06,
      "loss": 0.4876,
      "step": 479000
    },
    {
      "epoch": 7.678142514011209,
      "grad_norm": 2.4622929096221924,
      "learning_rate": 2.011609287429944e-06,
      "loss": 0.4776,
      "step": 479500
    },
    {
      "epoch": 7.6861489191353085,
      "grad_norm": 5.315605640411377,
      "learning_rate": 1.9615692554043235e-06,
      "loss": 0.4853,
      "step": 480000
    },
    {
      "epoch": 7.694155324259407,
      "grad_norm": 2.914400815963745,
      "learning_rate": 1.911529223378703e-06,
      "loss": 0.5023,
      "step": 480500
    },
    {
      "epoch": 7.702161729383507,
      "grad_norm": 3.776603937149048,
      "learning_rate": 1.8614891913530827e-06,
      "loss": 0.4847,
      "step": 481000
    },
    {
      "epoch": 7.710168134507606,
      "grad_norm": 3.52908992767334,
      "learning_rate": 1.8114491593274621e-06,
      "loss": 0.4703,
      "step": 481500
    },
    {
      "epoch": 7.718174539631706,
      "grad_norm": 5.109096050262451,
      "learning_rate": 1.7614091273018416e-06,
      "loss": 0.4935,
      "step": 482000
    },
    {
      "epoch": 7.7261809447558045,
      "grad_norm": 5.676068305969238,
      "learning_rate": 1.711369095276221e-06,
      "loss": 0.5047,
      "step": 482500
    },
    {
      "epoch": 7.734187349879904,
      "grad_norm": 6.8995561599731445,
      "learning_rate": 1.6613290632506006e-06,
      "loss": 0.4905,
      "step": 483000
    },
    {
      "epoch": 7.742193755004003,
      "grad_norm": 5.377361297607422,
      "learning_rate": 1.61128903122498e-06,
      "loss": 0.4825,
      "step": 483500
    },
    {
      "epoch": 7.750200160128102,
      "grad_norm": 6.044355392456055,
      "learning_rate": 1.5612489991993595e-06,
      "loss": 0.4984,
      "step": 484000
    },
    {
      "epoch": 7.758206565252202,
      "grad_norm": 6.4942426681518555,
      "learning_rate": 1.511208967173739e-06,
      "loss": 0.5001,
      "step": 484500
    },
    {
      "epoch": 7.766212970376301,
      "grad_norm": 4.0898566246032715,
      "learning_rate": 1.4611689351481185e-06,
      "loss": 0.4912,
      "step": 485000
    },
    {
      "epoch": 7.7742193755004,
      "grad_norm": 1.9056086540222168,
      "learning_rate": 1.4111289031224982e-06,
      "loss": 0.4761,
      "step": 485500
    },
    {
      "epoch": 7.7822257806245,
      "grad_norm": 5.4719414710998535,
      "learning_rate": 1.3610888710968776e-06,
      "loss": 0.4778,
      "step": 486000
    },
    {
      "epoch": 7.790232185748599,
      "grad_norm": 6.162261962890625,
      "learning_rate": 1.3110488390712571e-06,
      "loss": 0.4871,
      "step": 486500
    },
    {
      "epoch": 7.798238590872698,
      "grad_norm": 3.378077268600464,
      "learning_rate": 1.2610088070456366e-06,
      "loss": 0.4855,
      "step": 487000
    },
    {
      "epoch": 7.806244995996797,
      "grad_norm": 3.5417532920837402,
      "learning_rate": 1.210968775020016e-06,
      "loss": 0.487,
      "step": 487500
    },
    {
      "epoch": 7.814251401120897,
      "grad_norm": 3.862640619277954,
      "learning_rate": 1.1609287429943956e-06,
      "loss": 0.4837,
      "step": 488000
    },
    {
      "epoch": 7.822257806244996,
      "grad_norm": 4.693258762359619,
      "learning_rate": 1.110888710968775e-06,
      "loss": 0.4727,
      "step": 488500
    },
    {
      "epoch": 7.830264211369095,
      "grad_norm": 3.7918732166290283,
      "learning_rate": 1.0608486789431545e-06,
      "loss": 0.4952,
      "step": 489000
    },
    {
      "epoch": 7.838270616493195,
      "grad_norm": 3.156740427017212,
      "learning_rate": 1.010808646917534e-06,
      "loss": 0.4922,
      "step": 489500
    },
    {
      "epoch": 7.8462770216172935,
      "grad_norm": 4.752453804016113,
      "learning_rate": 9.607686148919135e-07,
      "loss": 0.4835,
      "step": 490000
    },
    {
      "epoch": 7.854283426741393,
      "grad_norm": 4.838400840759277,
      "learning_rate": 9.10728582866293e-07,
      "loss": 0.4933,
      "step": 490500
    },
    {
      "epoch": 7.862289831865493,
      "grad_norm": 5.5190253257751465,
      "learning_rate": 8.606885508406725e-07,
      "loss": 0.4785,
      "step": 491000
    },
    {
      "epoch": 7.870296236989592,
      "grad_norm": 5.464784145355225,
      "learning_rate": 8.10648518815052e-07,
      "loss": 0.4814,
      "step": 491500
    },
    {
      "epoch": 7.878302642113691,
      "grad_norm": 4.060845851898193,
      "learning_rate": 7.606084867894316e-07,
      "loss": 0.4945,
      "step": 492000
    },
    {
      "epoch": 7.88630904723779,
      "grad_norm": 9.155220031738281,
      "learning_rate": 7.105684547638111e-07,
      "loss": 0.4799,
      "step": 492500
    },
    {
      "epoch": 7.894315452361889,
      "grad_norm": 3.786975145339966,
      "learning_rate": 6.605284227381905e-07,
      "loss": 0.4791,
      "step": 493000
    },
    {
      "epoch": 7.902321857485989,
      "grad_norm": 3.649984121322632,
      "learning_rate": 6.1048839071257e-07,
      "loss": 0.4806,
      "step": 493500
    },
    {
      "epoch": 7.910328262610088,
      "grad_norm": 3.9725847244262695,
      "learning_rate": 5.604483586869496e-07,
      "loss": 0.499,
      "step": 494000
    },
    {
      "epoch": 7.918334667734188,
      "grad_norm": 2.515819549560547,
      "learning_rate": 5.104083266613292e-07,
      "loss": 0.4677,
      "step": 494500
    },
    {
      "epoch": 7.926341072858286,
      "grad_norm": 6.1911187171936035,
      "learning_rate": 4.6036829463570865e-07,
      "loss": 0.4789,
      "step": 495000
    },
    {
      "epoch": 7.934347477982386,
      "grad_norm": 3.498521327972412,
      "learning_rate": 4.103282626100881e-07,
      "loss": 0.4991,
      "step": 495500
    },
    {
      "epoch": 7.9423538831064855,
      "grad_norm": 8.299165725708008,
      "learning_rate": 3.602882305844676e-07,
      "loss": 0.5002,
      "step": 496000
    },
    {
      "epoch": 7.950360288230584,
      "grad_norm": 7.060730934143066,
      "learning_rate": 3.1024819855884713e-07,
      "loss": 0.4861,
      "step": 496500
    },
    {
      "epoch": 7.958366693354684,
      "grad_norm": 3.928236722946167,
      "learning_rate": 2.602081665332266e-07,
      "loss": 0.4796,
      "step": 497000
    },
    {
      "epoch": 7.966373098478783,
      "grad_norm": 3.667032480239868,
      "learning_rate": 2.1016813450760608e-07,
      "loss": 0.4817,
      "step": 497500
    },
    {
      "epoch": 7.974379503602882,
      "grad_norm": 2.2309393882751465,
      "learning_rate": 1.6012810248198559e-07,
      "loss": 0.4854,
      "step": 498000
    },
    {
      "epoch": 7.9823859087269815,
      "grad_norm": 4.940009117126465,
      "learning_rate": 1.100880704563651e-07,
      "loss": 0.4922,
      "step": 498500
    },
    {
      "epoch": 7.990392313851081,
      "grad_norm": 2.6009674072265625,
      "learning_rate": 6.004803843074459e-08,
      "loss": 0.4795,
      "step": 499000
    },
    {
      "epoch": 7.99839871897518,
      "grad_norm": 3.869534492492676,
      "learning_rate": 1.0008006405124099e-08,
      "loss": 0.4731,
      "step": 499500
    },
    {
      "epoch": 8.004995116952978,
      "grad_norm": 2.7771759033203125,
      "learning_rate": 9.975024415235106e-06,
      "loss": 0.5098,
      "step": 500000
    },
    {
      "epoch": 8.013000112069932,
      "grad_norm": 5.492103099822998,
      "learning_rate": 9.934999439650343e-06,
      "loss": 0.4895,
      "step": 500500
    },
    {
      "epoch": 8.021005107186884,
      "grad_norm": 2.8790574073791504,
      "learning_rate": 9.894974464065577e-06,
      "loss": 0.5192,
      "step": 501000
    },
    {
      "epoch": 8.029010102303838,
      "grad_norm": 5.331564903259277,
      "learning_rate": 9.854949488480813e-06,
      "loss": 0.5069,
      "step": 501500
    },
    {
      "epoch": 8.03701509742079,
      "grad_norm": 4.0715742111206055,
      "learning_rate": 9.814924512896048e-06,
      "loss": 0.4991,
      "step": 502000
    },
    {
      "epoch": 8.045020092537744,
      "grad_norm": 24.474014282226562,
      "learning_rate": 9.774899537311283e-06,
      "loss": 0.4858,
      "step": 502500
    },
    {
      "epoch": 8.053025087654696,
      "grad_norm": 6.774330139160156,
      "learning_rate": 9.734874561726517e-06,
      "loss": 0.4887,
      "step": 503000
    },
    {
      "epoch": 8.06103008277165,
      "grad_norm": 1.5921388864517212,
      "learning_rate": 9.694849586141753e-06,
      "loss": 0.5031,
      "step": 503500
    },
    {
      "epoch": 8.069035077888602,
      "grad_norm": 4.278860569000244,
      "learning_rate": 9.654824610556988e-06,
      "loss": 0.4904,
      "step": 504000
    },
    {
      "epoch": 8.077040073005556,
      "grad_norm": 5.852538108825684,
      "learning_rate": 9.614799634972224e-06,
      "loss": 0.503,
      "step": 504500
    },
    {
      "epoch": 8.085045068122508,
      "grad_norm": 4.747588634490967,
      "learning_rate": 9.574774659387459e-06,
      "loss": 0.484,
      "step": 505000
    },
    {
      "epoch": 8.093050063239462,
      "grad_norm": 5.4175190925598145,
      "learning_rate": 9.534749683802693e-06,
      "loss": 0.4913,
      "step": 505500
    },
    {
      "epoch": 8.101055058356414,
      "grad_norm": 5.375087738037109,
      "learning_rate": 9.494724708217928e-06,
      "loss": 0.4855,
      "step": 506000
    },
    {
      "epoch": 8.109060053473367,
      "grad_norm": 5.458198547363281,
      "learning_rate": 9.454699732633164e-06,
      "loss": 0.5046,
      "step": 506500
    },
    {
      "epoch": 8.11706504859032,
      "grad_norm": 5.271208763122559,
      "learning_rate": 9.414674757048399e-06,
      "loss": 0.5039,
      "step": 507000
    },
    {
      "epoch": 8.125070043707273,
      "grad_norm": 13.290353775024414,
      "learning_rate": 9.374649781463633e-06,
      "loss": 0.5041,
      "step": 507500
    },
    {
      "epoch": 8.133075038824225,
      "grad_norm": 2.620098114013672,
      "learning_rate": 9.33462480587887e-06,
      "loss": 0.5068,
      "step": 508000
    },
    {
      "epoch": 8.14108003394118,
      "grad_norm": 2.294750452041626,
      "learning_rate": 9.294599830294104e-06,
      "loss": 0.5088,
      "step": 508500
    },
    {
      "epoch": 8.149085029058133,
      "grad_norm": 5.6077680587768555,
      "learning_rate": 9.25457485470934e-06,
      "loss": 0.5025,
      "step": 509000
    },
    {
      "epoch": 8.157090024175085,
      "grad_norm": 9.501940727233887,
      "learning_rate": 9.214549879124575e-06,
      "loss": 0.5069,
      "step": 509500
    },
    {
      "epoch": 8.165095019292039,
      "grad_norm": 3.791991710662842,
      "learning_rate": 9.17452490353981e-06,
      "loss": 0.5027,
      "step": 510000
    },
    {
      "epoch": 8.173100014408991,
      "grad_norm": 7.266530513763428,
      "learning_rate": 9.134499927955044e-06,
      "loss": 0.5059,
      "step": 510500
    },
    {
      "epoch": 8.181105009525945,
      "grad_norm": 3.4328770637512207,
      "learning_rate": 9.09447495237028e-06,
      "loss": 0.4941,
      "step": 511000
    },
    {
      "epoch": 8.189110004642897,
      "grad_norm": 6.154504299163818,
      "learning_rate": 9.054449976785515e-06,
      "loss": 0.4997,
      "step": 511500
    },
    {
      "epoch": 8.197114999759851,
      "grad_norm": 5.507376670837402,
      "learning_rate": 9.014425001200748e-06,
      "loss": 0.4878,
      "step": 512000
    },
    {
      "epoch": 8.205119994876803,
      "grad_norm": 2.0356967449188232,
      "learning_rate": 8.974400025615986e-06,
      "loss": 0.5227,
      "step": 512500
    },
    {
      "epoch": 8.213124989993757,
      "grad_norm": 7.342750549316406,
      "learning_rate": 8.93437505003122e-06,
      "loss": 0.5007,
      "step": 513000
    },
    {
      "epoch": 8.221129985110709,
      "grad_norm": 4.225399494171143,
      "learning_rate": 8.894350074446455e-06,
      "loss": 0.5089,
      "step": 513500
    },
    {
      "epoch": 8.229134980227663,
      "grad_norm": 1.5860844850540161,
      "learning_rate": 8.85432509886169e-06,
      "loss": 0.5094,
      "step": 514000
    },
    {
      "epoch": 8.237139975344615,
      "grad_norm": 3.7166249752044678,
      "learning_rate": 8.814300123276926e-06,
      "loss": 0.482,
      "step": 514500
    },
    {
      "epoch": 8.245144970461569,
      "grad_norm": 2.8957979679107666,
      "learning_rate": 8.77427514769216e-06,
      "loss": 0.4966,
      "step": 515000
    },
    {
      "epoch": 8.25314996557852,
      "grad_norm": 3.8355772495269775,
      "learning_rate": 8.734250172107395e-06,
      "loss": 0.4982,
      "step": 515500
    },
    {
      "epoch": 8.261154960695475,
      "grad_norm": 2.8366918563842773,
      "learning_rate": 8.69422519652263e-06,
      "loss": 0.4877,
      "step": 516000
    },
    {
      "epoch": 8.269159955812427,
      "grad_norm": 5.329589366912842,
      "learning_rate": 8.654200220937866e-06,
      "loss": 0.4833,
      "step": 516500
    },
    {
      "epoch": 8.27716495092938,
      "grad_norm": 5.198570728302002,
      "learning_rate": 8.614175245353101e-06,
      "loss": 0.4882,
      "step": 517000
    },
    {
      "epoch": 8.285169946046333,
      "grad_norm": 3.366004467010498,
      "learning_rate": 8.574150269768335e-06,
      "loss": 0.5019,
      "step": 517500
    },
    {
      "epoch": 8.293174941163286,
      "grad_norm": 6.711418151855469,
      "learning_rate": 8.53412529418357e-06,
      "loss": 0.5024,
      "step": 518000
    },
    {
      "epoch": 8.301179936280239,
      "grad_norm": 11.152918815612793,
      "learning_rate": 8.494100318598806e-06,
      "loss": 0.5049,
      "step": 518500
    },
    {
      "epoch": 8.309184931397192,
      "grad_norm": 0.9452989101409912,
      "learning_rate": 8.454075343014041e-06,
      "loss": 0.4841,
      "step": 519000
    },
    {
      "epoch": 8.317189926514144,
      "grad_norm": 2.1003482341766357,
      "learning_rate": 8.414050367429275e-06,
      "loss": 0.4956,
      "step": 519500
    },
    {
      "epoch": 8.325194921631098,
      "grad_norm": 4.711812973022461,
      "learning_rate": 8.374025391844512e-06,
      "loss": 0.5092,
      "step": 520000
    },
    {
      "epoch": 8.33319991674805,
      "grad_norm": 4.536991596221924,
      "learning_rate": 8.334000416259746e-06,
      "loss": 0.4894,
      "step": 520500
    },
    {
      "epoch": 8.341204911865004,
      "grad_norm": 3.0835304260253906,
      "learning_rate": 8.293975440674981e-06,
      "loss": 0.4965,
      "step": 521000
    },
    {
      "epoch": 8.349209906981956,
      "grad_norm": 7.2053141593933105,
      "learning_rate": 8.253950465090217e-06,
      "loss": 0.4828,
      "step": 521500
    },
    {
      "epoch": 8.35721490209891,
      "grad_norm": 2.8884363174438477,
      "learning_rate": 8.213925489505452e-06,
      "loss": 0.501,
      "step": 522000
    },
    {
      "epoch": 8.365219897215862,
      "grad_norm": 5.250162601470947,
      "learning_rate": 8.173900513920688e-06,
      "loss": 0.4917,
      "step": 522500
    },
    {
      "epoch": 8.373224892332816,
      "grad_norm": 3.177393674850464,
      "learning_rate": 8.133875538335922e-06,
      "loss": 0.4911,
      "step": 523000
    },
    {
      "epoch": 8.381229887449768,
      "grad_norm": 4.187519073486328,
      "learning_rate": 8.093850562751157e-06,
      "loss": 0.4736,
      "step": 523500
    },
    {
      "epoch": 8.389234882566722,
      "grad_norm": 3.3228065967559814,
      "learning_rate": 8.053825587166392e-06,
      "loss": 0.4963,
      "step": 524000
    },
    {
      "epoch": 8.397239877683674,
      "grad_norm": 2.5538551807403564,
      "learning_rate": 8.013800611581628e-06,
      "loss": 0.4939,
      "step": 524500
    },
    {
      "epoch": 8.405244872800628,
      "grad_norm": 5.338902473449707,
      "learning_rate": 7.973775635996862e-06,
      "loss": 0.4902,
      "step": 525000
    },
    {
      "epoch": 8.41324986791758,
      "grad_norm": 5.2537360191345215,
      "learning_rate": 7.933750660412099e-06,
      "loss": 0.5013,
      "step": 525500
    },
    {
      "epoch": 8.421254863034534,
      "grad_norm": 5.288268566131592,
      "learning_rate": 7.893725684827333e-06,
      "loss": 0.4856,
      "step": 526000
    },
    {
      "epoch": 8.429259858151486,
      "grad_norm": 4.101529121398926,
      "learning_rate": 7.853700709242568e-06,
      "loss": 0.4931,
      "step": 526500
    },
    {
      "epoch": 8.43726485326844,
      "grad_norm": 1.439257025718689,
      "learning_rate": 7.813675733657803e-06,
      "loss": 0.4826,
      "step": 527000
    },
    {
      "epoch": 8.445269848385392,
      "grad_norm": 3.9271974563598633,
      "learning_rate": 7.773650758073037e-06,
      "loss": 0.5018,
      "step": 527500
    },
    {
      "epoch": 8.453274843502346,
      "grad_norm": 4.346601486206055,
      "learning_rate": 7.733625782488273e-06,
      "loss": 0.5011,
      "step": 528000
    },
    {
      "epoch": 8.461279838619298,
      "grad_norm": 7.46856164932251,
      "learning_rate": 7.693600806903508e-06,
      "loss": 0.5006,
      "step": 528500
    },
    {
      "epoch": 8.469284833736252,
      "grad_norm": 4.327626705169678,
      "learning_rate": 7.653575831318744e-06,
      "loss": 0.4862,
      "step": 529000
    },
    {
      "epoch": 8.477289828853204,
      "grad_norm": 2.1458826065063477,
      "learning_rate": 7.613550855733978e-06,
      "loss": 0.4774,
      "step": 529500
    },
    {
      "epoch": 8.485294823970158,
      "grad_norm": 7.645746231079102,
      "learning_rate": 7.573525880149214e-06,
      "loss": 0.4941,
      "step": 530000
    },
    {
      "epoch": 8.49329981908711,
      "grad_norm": 7.12299108505249,
      "learning_rate": 7.533500904564448e-06,
      "loss": 0.4854,
      "step": 530500
    },
    {
      "epoch": 8.501304814204063,
      "grad_norm": 5.790500640869141,
      "learning_rate": 7.493475928979684e-06,
      "loss": 0.4985,
      "step": 531000
    },
    {
      "epoch": 8.509309809321016,
      "grad_norm": 3.539513349533081,
      "learning_rate": 7.453450953394919e-06,
      "loss": 0.4895,
      "step": 531500
    },
    {
      "epoch": 8.51731480443797,
      "grad_norm": 3.2272512912750244,
      "learning_rate": 7.413425977810154e-06,
      "loss": 0.4864,
      "step": 532000
    },
    {
      "epoch": 8.525319799554921,
      "grad_norm": 4.972077369689941,
      "learning_rate": 7.373401002225388e-06,
      "loss": 0.4915,
      "step": 532500
    },
    {
      "epoch": 8.533324794671875,
      "grad_norm": 2.6587910652160645,
      "learning_rate": 7.333376026640625e-06,
      "loss": 0.4922,
      "step": 533000
    },
    {
      "epoch": 8.541329789788827,
      "grad_norm": 6.827407360076904,
      "learning_rate": 7.293351051055859e-06,
      "loss": 0.4934,
      "step": 533500
    },
    {
      "epoch": 8.549334784905781,
      "grad_norm": 6.616888523101807,
      "learning_rate": 7.253326075471094e-06,
      "loss": 0.4897,
      "step": 534000
    },
    {
      "epoch": 8.557339780022733,
      "grad_norm": 4.761914253234863,
      "learning_rate": 7.21330109988633e-06,
      "loss": 0.5015,
      "step": 534500
    },
    {
      "epoch": 8.565344775139687,
      "grad_norm": 3.8315141201019287,
      "learning_rate": 7.173276124301565e-06,
      "loss": 0.4845,
      "step": 535000
    },
    {
      "epoch": 8.573349770256641,
      "grad_norm": 3.9321212768554688,
      "learning_rate": 7.133251148716799e-06,
      "loss": 0.4945,
      "step": 535500
    },
    {
      "epoch": 8.581354765373593,
      "grad_norm": 3.6174209117889404,
      "learning_rate": 7.093226173132035e-06,
      "loss": 0.4914,
      "step": 536000
    },
    {
      "epoch": 8.589359760490545,
      "grad_norm": 3.189551830291748,
      "learning_rate": 7.05320119754727e-06,
      "loss": 0.4859,
      "step": 536500
    },
    {
      "epoch": 8.597364755607499,
      "grad_norm": 2.857147693634033,
      "learning_rate": 7.013176221962505e-06,
      "loss": 0.4969,
      "step": 537000
    },
    {
      "epoch": 8.605369750724453,
      "grad_norm": 4.1896891593933105,
      "learning_rate": 6.97315124637774e-06,
      "loss": 0.5074,
      "step": 537500
    },
    {
      "epoch": 8.613374745841405,
      "grad_norm": 2.847764015197754,
      "learning_rate": 6.933126270792975e-06,
      "loss": 0.4877,
      "step": 538000
    },
    {
      "epoch": 8.621379740958359,
      "grad_norm": 3.1568760871887207,
      "learning_rate": 6.8931012952082094e-06,
      "loss": 0.5059,
      "step": 538500
    },
    {
      "epoch": 8.62938473607531,
      "grad_norm": 3.4201784133911133,
      "learning_rate": 6.853076319623446e-06,
      "loss": 0.4962,
      "step": 539000
    },
    {
      "epoch": 8.637389731192265,
      "grad_norm": 5.628586769104004,
      "learning_rate": 6.81305134403868e-06,
      "loss": 0.4897,
      "step": 539500
    },
    {
      "epoch": 8.645394726309217,
      "grad_norm": 6.3902740478515625,
      "learning_rate": 6.773026368453915e-06,
      "loss": 0.4918,
      "step": 540000
    },
    {
      "epoch": 8.65339972142617,
      "grad_norm": 5.102650165557861,
      "learning_rate": 6.733001392869151e-06,
      "loss": 0.4963,
      "step": 540500
    },
    {
      "epoch": 8.661404716543123,
      "grad_norm": 2.6327521800994873,
      "learning_rate": 6.692976417284386e-06,
      "loss": 0.4865,
      "step": 541000
    },
    {
      "epoch": 8.669409711660077,
      "grad_norm": 6.855027198791504,
      "learning_rate": 6.6529514416996204e-06,
      "loss": 0.4842,
      "step": 541500
    },
    {
      "epoch": 8.677414706777029,
      "grad_norm": 3.2498292922973633,
      "learning_rate": 6.612926466114856e-06,
      "loss": 0.5014,
      "step": 542000
    },
    {
      "epoch": 8.685419701893982,
      "grad_norm": 5.861929893493652,
      "learning_rate": 6.572901490530091e-06,
      "loss": 0.4882,
      "step": 542500
    },
    {
      "epoch": 8.693424697010935,
      "grad_norm": 3.877939224243164,
      "learning_rate": 6.532876514945326e-06,
      "loss": 0.4827,
      "step": 543000
    },
    {
      "epoch": 8.701429692127888,
      "grad_norm": 2.6562209129333496,
      "learning_rate": 6.492851539360561e-06,
      "loss": 0.4974,
      "step": 543500
    },
    {
      "epoch": 8.70943468724484,
      "grad_norm": 5.546886444091797,
      "learning_rate": 6.452826563775796e-06,
      "loss": 0.4969,
      "step": 544000
    },
    {
      "epoch": 8.717439682361794,
      "grad_norm": 3.7795774936676025,
      "learning_rate": 6.4128015881910306e-06,
      "loss": 0.4948,
      "step": 544500
    },
    {
      "epoch": 8.725444677478746,
      "grad_norm": 3.297623872756958,
      "learning_rate": 6.372776612606267e-06,
      "loss": 0.4946,
      "step": 545000
    },
    {
      "epoch": 8.7334496725957,
      "grad_norm": 11.015005111694336,
      "learning_rate": 6.3327516370215015e-06,
      "loss": 0.49,
      "step": 545500
    },
    {
      "epoch": 8.741454667712652,
      "grad_norm": 2.9758784770965576,
      "learning_rate": 6.292726661436738e-06,
      "loss": 0.4748,
      "step": 546000
    },
    {
      "epoch": 8.749459662829606,
      "grad_norm": 6.482463359832764,
      "learning_rate": 6.252701685851972e-06,
      "loss": 0.4792,
      "step": 546500
    },
    {
      "epoch": 8.757464657946558,
      "grad_norm": 6.36815881729126,
      "learning_rate": 6.212676710267207e-06,
      "loss": 0.504,
      "step": 547000
    },
    {
      "epoch": 8.765469653063512,
      "grad_norm": 4.044981956481934,
      "learning_rate": 6.172651734682442e-06,
      "loss": 0.4956,
      "step": 547500
    },
    {
      "epoch": 8.773474648180464,
      "grad_norm": 3.266382932662964,
      "learning_rate": 6.132626759097677e-06,
      "loss": 0.4793,
      "step": 548000
    },
    {
      "epoch": 8.781479643297418,
      "grad_norm": 2.6614930629730225,
      "learning_rate": 6.0926017835129125e-06,
      "loss": 0.5081,
      "step": 548500
    },
    {
      "epoch": 8.78948463841437,
      "grad_norm": 1.4395400285720825,
      "learning_rate": 6.052576807928147e-06,
      "loss": 0.5005,
      "step": 549000
    },
    {
      "epoch": 8.797489633531324,
      "grad_norm": 3.141099452972412,
      "learning_rate": 6.0125518323433825e-06,
      "loss": 0.4989,
      "step": 549500
    },
    {
      "epoch": 8.805494628648276,
      "grad_norm": 3.2387592792510986,
      "learning_rate": 5.972526856758617e-06,
      "loss": 0.4998,
      "step": 550000
    },
    {
      "epoch": 8.81349962376523,
      "grad_norm": 3.3831276893615723,
      "learning_rate": 5.9325018811738526e-06,
      "loss": 0.4891,
      "step": 550500
    },
    {
      "epoch": 8.821504618882182,
      "grad_norm": 3.7181332111358643,
      "learning_rate": 5.892476905589088e-06,
      "loss": 0.4857,
      "step": 551000
    },
    {
      "epoch": 8.829509613999136,
      "grad_norm": 6.97683572769165,
      "learning_rate": 5.8524519300043235e-06,
      "loss": 0.4826,
      "step": 551500
    },
    {
      "epoch": 8.837514609116088,
      "grad_norm": 3.7650938034057617,
      "learning_rate": 5.812426954419558e-06,
      "loss": 0.4858,
      "step": 552000
    },
    {
      "epoch": 8.845519604233042,
      "grad_norm": 4.233370304107666,
      "learning_rate": 5.7724019788347935e-06,
      "loss": 0.4864,
      "step": 552500
    },
    {
      "epoch": 8.853524599349994,
      "grad_norm": 2.540419816970825,
      "learning_rate": 5.732377003250029e-06,
      "loss": 0.4826,
      "step": 553000
    },
    {
      "epoch": 8.861529594466948,
      "grad_norm": 1.8219695091247559,
      "learning_rate": 5.6923520276652636e-06,
      "loss": 0.4903,
      "step": 553500
    },
    {
      "epoch": 8.8695345895839,
      "grad_norm": 3.8486826419830322,
      "learning_rate": 5.652327052080498e-06,
      "loss": 0.4944,
      "step": 554000
    },
    {
      "epoch": 8.877539584700854,
      "grad_norm": 3.778198480606079,
      "learning_rate": 5.612302076495734e-06,
      "loss": 0.4939,
      "step": 554500
    },
    {
      "epoch": 8.885544579817806,
      "grad_norm": 1.765631079673767,
      "learning_rate": 5.572277100910968e-06,
      "loss": 0.4935,
      "step": 555000
    },
    {
      "epoch": 8.89354957493476,
      "grad_norm": 2.6258296966552734,
      "learning_rate": 5.532252125326204e-06,
      "loss": 0.505,
      "step": 555500
    },
    {
      "epoch": 8.901554570051712,
      "grad_norm": 2.284971237182617,
      "learning_rate": 5.492227149741439e-06,
      "loss": 0.4844,
      "step": 556000
    },
    {
      "epoch": 8.909559565168665,
      "grad_norm": 5.508540153503418,
      "learning_rate": 5.452202174156674e-06,
      "loss": 0.4933,
      "step": 556500
    },
    {
      "epoch": 8.917564560285618,
      "grad_norm": 5.419959545135498,
      "learning_rate": 5.412177198571909e-06,
      "loss": 0.4893,
      "step": 557000
    },
    {
      "epoch": 8.925569555402571,
      "grad_norm": 2.9101147651672363,
      "learning_rate": 5.372152222987145e-06,
      "loss": 0.4891,
      "step": 557500
    },
    {
      "epoch": 8.933574550519523,
      "grad_norm": 5.546329498291016,
      "learning_rate": 5.332127247402379e-06,
      "loss": 0.4988,
      "step": 558000
    },
    {
      "epoch": 8.941579545636477,
      "grad_norm": 3.219303607940674,
      "learning_rate": 5.292102271817615e-06,
      "loss": 0.4872,
      "step": 558500
    },
    {
      "epoch": 8.94958454075343,
      "grad_norm": 6.004509925842285,
      "learning_rate": 5.25207729623285e-06,
      "loss": 0.4992,
      "step": 559000
    },
    {
      "epoch": 8.957589535870383,
      "grad_norm": 1.5979821681976318,
      "learning_rate": 5.212052320648085e-06,
      "loss": 0.4865,
      "step": 559500
    },
    {
      "epoch": 8.965594530987335,
      "grad_norm": 2.82795786857605,
      "learning_rate": 5.172027345063319e-06,
      "loss": 0.4842,
      "step": 560000
    },
    {
      "epoch": 8.97359952610429,
      "grad_norm": 4.638966083526611,
      "learning_rate": 5.132002369478555e-06,
      "loss": 0.4847,
      "step": 560500
    },
    {
      "epoch": 8.981604521221243,
      "grad_norm": 2.6528031826019287,
      "learning_rate": 5.091977393893789e-06,
      "loss": 0.4933,
      "step": 561000
    },
    {
      "epoch": 8.989609516338195,
      "grad_norm": 9.159281730651855,
      "learning_rate": 5.051952418309025e-06,
      "loss": 0.4853,
      "step": 561500
    },
    {
      "epoch": 8.997614511455147,
      "grad_norm": 7.687154293060303,
      "learning_rate": 5.01192744272426e-06,
      "loss": 0.4997,
      "step": 562000
    },
    {
      "epoch": 9.005619506572101,
      "grad_norm": 2.4097981452941895,
      "learning_rate": 4.971902467139495e-06,
      "loss": 0.4815,
      "step": 562500
    },
    {
      "epoch": 9.013624501689055,
      "grad_norm": 5.464820861816406,
      "learning_rate": 4.93187749155473e-06,
      "loss": 0.4604,
      "step": 563000
    },
    {
      "epoch": 9.021629496806007,
      "grad_norm": 4.6026763916015625,
      "learning_rate": 4.891852515969966e-06,
      "loss": 0.4703,
      "step": 563500
    },
    {
      "epoch": 9.02963449192296,
      "grad_norm": 8.211807250976562,
      "learning_rate": 4.851827540385201e-06,
      "loss": 0.4929,
      "step": 564000
    },
    {
      "epoch": 9.037639487039913,
      "grad_norm": 4.25120735168457,
      "learning_rate": 4.811802564800436e-06,
      "loss": 0.467,
      "step": 564500
    },
    {
      "epoch": 9.045644482156867,
      "grad_norm": 1.394412636756897,
      "learning_rate": 4.771777589215671e-06,
      "loss": 0.4895,
      "step": 565000
    },
    {
      "epoch": 9.053649477273819,
      "grad_norm": 4.328395366668701,
      "learning_rate": 4.731752613630906e-06,
      "loss": 0.4678,
      "step": 565500
    },
    {
      "epoch": 9.061654472390773,
      "grad_norm": 10.474742889404297,
      "learning_rate": 4.691727638046141e-06,
      "loss": 0.4769,
      "step": 566000
    },
    {
      "epoch": 9.069659467507725,
      "grad_norm": 4.242556571960449,
      "learning_rate": 4.651702662461376e-06,
      "loss": 0.4518,
      "step": 566500
    },
    {
      "epoch": 9.077664462624679,
      "grad_norm": 2.4093215465545654,
      "learning_rate": 4.611677686876611e-06,
      "loss": 0.4844,
      "step": 567000
    },
    {
      "epoch": 9.08566945774163,
      "grad_norm": 4.535050868988037,
      "learning_rate": 4.571652711291846e-06,
      "loss": 0.4464,
      "step": 567500
    },
    {
      "epoch": 9.093674452858584,
      "grad_norm": 2.679534673690796,
      "learning_rate": 4.531627735707081e-06,
      "loss": 0.4774,
      "step": 568000
    },
    {
      "epoch": 9.101679447975537,
      "grad_norm": 3.8758440017700195,
      "learning_rate": 4.491602760122317e-06,
      "loss": 0.4734,
      "step": 568500
    },
    {
      "epoch": 9.10968444309249,
      "grad_norm": 1.6881544589996338,
      "learning_rate": 4.451577784537551e-06,
      "loss": 0.4869,
      "step": 569000
    },
    {
      "epoch": 9.117689438209442,
      "grad_norm": 2.8555192947387695,
      "learning_rate": 4.411552808952787e-06,
      "loss": 0.4644,
      "step": 569500
    },
    {
      "epoch": 9.125694433326396,
      "grad_norm": 3.8599517345428467,
      "learning_rate": 4.371527833368022e-06,
      "loss": 0.4681,
      "step": 570000
    },
    {
      "epoch": 9.133699428443348,
      "grad_norm": 5.235069274902344,
      "learning_rate": 4.331502857783257e-06,
      "loss": 0.4732,
      "step": 570500
    },
    {
      "epoch": 9.141704423560302,
      "grad_norm": 18.592805862426758,
      "learning_rate": 4.291477882198492e-06,
      "loss": 0.4693,
      "step": 571000
    },
    {
      "epoch": 9.149709418677254,
      "grad_norm": 4.036069869995117,
      "learning_rate": 4.251452906613727e-06,
      "loss": 0.4599,
      "step": 571500
    },
    {
      "epoch": 9.157714413794208,
      "grad_norm": 4.694085121154785,
      "learning_rate": 4.211427931028962e-06,
      "loss": 0.4699,
      "step": 572000
    },
    {
      "epoch": 9.16571940891116,
      "grad_norm": 2.268165111541748,
      "learning_rate": 4.171402955444197e-06,
      "loss": 0.4716,
      "step": 572500
    },
    {
      "epoch": 9.173724404028114,
      "grad_norm": 4.289149284362793,
      "learning_rate": 4.1313779798594325e-06,
      "loss": 0.4518,
      "step": 573000
    },
    {
      "epoch": 9.181729399145066,
      "grad_norm": 4.3679423332214355,
      "learning_rate": 4.091353004274667e-06,
      "loss": 0.4806,
      "step": 573500
    },
    {
      "epoch": 9.18973439426202,
      "grad_norm": 3.934788703918457,
      "learning_rate": 4.0513280286899025e-06,
      "loss": 0.4546,
      "step": 574000
    },
    {
      "epoch": 9.197739389378972,
      "grad_norm": 4.782047748565674,
      "learning_rate": 4.011303053105138e-06,
      "loss": 0.4638,
      "step": 574500
    },
    {
      "epoch": 9.205744384495926,
      "grad_norm": 5.130120754241943,
      "learning_rate": 3.9712780775203726e-06,
      "loss": 0.4748,
      "step": 575000
    },
    {
      "epoch": 9.213749379612878,
      "grad_norm": 6.774563312530518,
      "learning_rate": 3.931253101935608e-06,
      "loss": 0.475,
      "step": 575500
    },
    {
      "epoch": 9.221754374729832,
      "grad_norm": 3.1757047176361084,
      "learning_rate": 3.8912281263508435e-06,
      "loss": 0.4616,
      "step": 576000
    },
    {
      "epoch": 9.229759369846784,
      "grad_norm": 3.631648302078247,
      "learning_rate": 3.851203150766079e-06,
      "loss": 0.4721,
      "step": 576500
    },
    {
      "epoch": 9.237764364963738,
      "grad_norm": 3.496119499206543,
      "learning_rate": 3.811178175181313e-06,
      "loss": 0.4941,
      "step": 577000
    },
    {
      "epoch": 9.24576936008069,
      "grad_norm": 2.6507108211517334,
      "learning_rate": 3.7711531995965485e-06,
      "loss": 0.468,
      "step": 577500
    },
    {
      "epoch": 9.253774355197644,
      "grad_norm": 6.1918487548828125,
      "learning_rate": 3.731128224011784e-06,
      "loss": 0.4683,
      "step": 578000
    },
    {
      "epoch": 9.261779350314596,
      "grad_norm": 3.940629005432129,
      "learning_rate": 3.6911032484270186e-06,
      "loss": 0.4736,
      "step": 578500
    },
    {
      "epoch": 9.26978434543155,
      "grad_norm": 6.41928768157959,
      "learning_rate": 3.6510782728422536e-06,
      "loss": 0.4752,
      "step": 579000
    },
    {
      "epoch": 9.277789340548502,
      "grad_norm": 6.0830254554748535,
      "learning_rate": 3.611053297257489e-06,
      "loss": 0.4704,
      "step": 579500
    },
    {
      "epoch": 9.285794335665456,
      "grad_norm": 5.9330291748046875,
      "learning_rate": 3.5710283216727236e-06,
      "loss": 0.4768,
      "step": 580000
    },
    {
      "epoch": 9.293799330782408,
      "grad_norm": 2.4136297702789307,
      "learning_rate": 3.531003346087959e-06,
      "loss": 0.4711,
      "step": 580500
    },
    {
      "epoch": 9.301804325899361,
      "grad_norm": 3.2298715114593506,
      "learning_rate": 3.4909783705031945e-06,
      "loss": 0.4816,
      "step": 581000
    },
    {
      "epoch": 9.309809321016314,
      "grad_norm": 5.600350856781006,
      "learning_rate": 3.450953394918429e-06,
      "loss": 0.4707,
      "step": 581500
    },
    {
      "epoch": 9.317814316133267,
      "grad_norm": 9.208223342895508,
      "learning_rate": 3.410928419333664e-06,
      "loss": 0.4726,
      "step": 582000
    },
    {
      "epoch": 9.32581931125022,
      "grad_norm": 7.945375442504883,
      "learning_rate": 3.3709034437488996e-06,
      "loss": 0.491,
      "step": 582500
    },
    {
      "epoch": 9.333824306367173,
      "grad_norm": 2.277402877807617,
      "learning_rate": 3.3308784681641342e-06,
      "loss": 0.4599,
      "step": 583000
    },
    {
      "epoch": 9.341829301484125,
      "grad_norm": 4.153587341308594,
      "learning_rate": 3.2908534925793697e-06,
      "loss": 0.4806,
      "step": 583500
    },
    {
      "epoch": 9.34983429660108,
      "grad_norm": 3.106391668319702,
      "learning_rate": 3.250828516994605e-06,
      "loss": 0.4699,
      "step": 584000
    },
    {
      "epoch": 9.357839291718031,
      "grad_norm": 2.8011720180511475,
      "learning_rate": 3.2108035414098397e-06,
      "loss": 0.4826,
      "step": 584500
    },
    {
      "epoch": 9.365844286834985,
      "grad_norm": 2.748116970062256,
      "learning_rate": 3.1707785658250747e-06,
      "loss": 0.4801,
      "step": 585000
    },
    {
      "epoch": 9.373849281951937,
      "grad_norm": 4.512584686279297,
      "learning_rate": 3.13075359024031e-06,
      "loss": 0.4657,
      "step": 585500
    },
    {
      "epoch": 9.381854277068891,
      "grad_norm": 1.7563374042510986,
      "learning_rate": 3.090728614655545e-06,
      "loss": 0.4702,
      "step": 586000
    },
    {
      "epoch": 9.389859272185843,
      "grad_norm": 5.8658952713012695,
      "learning_rate": 3.0507036390707802e-06,
      "loss": 0.4525,
      "step": 586500
    },
    {
      "epoch": 9.397864267302797,
      "grad_norm": 11.258188247680664,
      "learning_rate": 3.0106786634860157e-06,
      "loss": 0.4667,
      "step": 587000
    },
    {
      "epoch": 9.405869262419749,
      "grad_norm": 6.395565986633301,
      "learning_rate": 2.9706536879012507e-06,
      "loss": 0.4729,
      "step": 587500
    },
    {
      "epoch": 9.413874257536703,
      "grad_norm": 5.351189136505127,
      "learning_rate": 2.9306287123164853e-06,
      "loss": 0.4743,
      "step": 588000
    },
    {
      "epoch": 9.421879252653657,
      "grad_norm": 4.434619426727295,
      "learning_rate": 2.8906037367317207e-06,
      "loss": 0.4908,
      "step": 588500
    },
    {
      "epoch": 9.429884247770609,
      "grad_norm": 7.456082344055176,
      "learning_rate": 2.8505787611469558e-06,
      "loss": 0.4668,
      "step": 589000
    },
    {
      "epoch": 9.437889242887561,
      "grad_norm": 6.488839626312256,
      "learning_rate": 2.810553785562191e-06,
      "loss": 0.4711,
      "step": 589500
    },
    {
      "epoch": 9.445894238004515,
      "grad_norm": 1.6692560911178589,
      "learning_rate": 2.7705288099774262e-06,
      "loss": 0.4731,
      "step": 590000
    },
    {
      "epoch": 9.453899233121469,
      "grad_norm": 1.4633636474609375,
      "learning_rate": 2.7305038343926613e-06,
      "loss": 0.465,
      "step": 590500
    },
    {
      "epoch": 9.46190422823842,
      "grad_norm": 5.855940341949463,
      "learning_rate": 2.6904788588078963e-06,
      "loss": 0.477,
      "step": 591000
    },
    {
      "epoch": 9.469909223355375,
      "grad_norm": 3.4103918075561523,
      "learning_rate": 2.6504538832231313e-06,
      "loss": 0.475,
      "step": 591500
    },
    {
      "epoch": 9.477914218472327,
      "grad_norm": 6.7579193115234375,
      "learning_rate": 2.6104289076383663e-06,
      "loss": 0.4666,
      "step": 592000
    },
    {
      "epoch": 9.48591921358928,
      "grad_norm": 8.343798637390137,
      "learning_rate": 2.570403932053602e-06,
      "loss": 0.4622,
      "step": 592500
    },
    {
      "epoch": 9.493924208706233,
      "grad_norm": 1.9842435121536255,
      "learning_rate": 2.530378956468837e-06,
      "loss": 0.4791,
      "step": 593000
    },
    {
      "epoch": 9.501929203823186,
      "grad_norm": 3.267239809036255,
      "learning_rate": 2.490353980884072e-06,
      "loss": 0.4578,
      "step": 593500
    },
    {
      "epoch": 9.509934198940138,
      "grad_norm": 4.570045471191406,
      "learning_rate": 2.450329005299307e-06,
      "loss": 0.4739,
      "step": 594000
    },
    {
      "epoch": 9.517939194057092,
      "grad_norm": 5.533812522888184,
      "learning_rate": 2.410304029714542e-06,
      "loss": 0.4818,
      "step": 594500
    },
    {
      "epoch": 9.525944189174044,
      "grad_norm": 6.560555458068848,
      "learning_rate": 2.370279054129777e-06,
      "loss": 0.4634,
      "step": 595000
    },
    {
      "epoch": 9.533949184290998,
      "grad_norm": 2.9336700439453125,
      "learning_rate": 2.3302540785450124e-06,
      "loss": 0.4894,
      "step": 595500
    },
    {
      "epoch": 9.54195417940795,
      "grad_norm": 5.1958441734313965,
      "learning_rate": 2.2902291029602474e-06,
      "loss": 0.458,
      "step": 596000
    },
    {
      "epoch": 9.549959174524904,
      "grad_norm": 5.030179977416992,
      "learning_rate": 2.2502041273754824e-06,
      "loss": 0.4685,
      "step": 596500
    },
    {
      "epoch": 9.557964169641856,
      "grad_norm": 9.254241943359375,
      "learning_rate": 2.2101791517907174e-06,
      "loss": 0.4795,
      "step": 597000
    },
    {
      "epoch": 9.56596916475881,
      "grad_norm": 1.3662525415420532,
      "learning_rate": 2.1701541762059525e-06,
      "loss": 0.4618,
      "step": 597500
    },
    {
      "epoch": 9.573974159875762,
      "grad_norm": 1.8477216958999634,
      "learning_rate": 2.130129200621188e-06,
      "loss": 0.4759,
      "step": 598000
    },
    {
      "epoch": 9.581979154992716,
      "grad_norm": 12.616325378417969,
      "learning_rate": 2.090104225036423e-06,
      "loss": 0.4621,
      "step": 598500
    },
    {
      "epoch": 9.589984150109668,
      "grad_norm": 8.563385009765625,
      "learning_rate": 2.050079249451658e-06,
      "loss": 0.4684,
      "step": 599000
    },
    {
      "epoch": 9.597989145226622,
      "grad_norm": 6.354208469390869,
      "learning_rate": 2.010054273866893e-06,
      "loss": 0.4599,
      "step": 599500
    },
    {
      "epoch": 9.605994140343574,
      "grad_norm": 3.267643690109253,
      "learning_rate": 1.970029298282128e-06,
      "loss": 0.4798,
      "step": 600000
    },
    {
      "epoch": 9.613999135460528,
      "grad_norm": 7.755107879638672,
      "learning_rate": 1.930004322697363e-06,
      "loss": 0.469,
      "step": 600500
    },
    {
      "epoch": 9.62200413057748,
      "grad_norm": 3.0060315132141113,
      "learning_rate": 1.8899793471125985e-06,
      "loss": 0.48,
      "step": 601000
    },
    {
      "epoch": 9.630009125694434,
      "grad_norm": 4.045262336730957,
      "learning_rate": 1.8499543715278335e-06,
      "loss": 0.4736,
      "step": 601500
    },
    {
      "epoch": 9.638014120811386,
      "grad_norm": 1.688410758972168,
      "learning_rate": 1.8099293959430683e-06,
      "loss": 0.4726,
      "step": 602000
    },
    {
      "epoch": 9.64601911592834,
      "grad_norm": 4.575070381164551,
      "learning_rate": 1.7699044203583038e-06,
      "loss": 0.4874,
      "step": 602500
    },
    {
      "epoch": 9.654024111045292,
      "grad_norm": 3.515615463256836,
      "learning_rate": 1.7298794447735388e-06,
      "loss": 0.4803,
      "step": 603000
    },
    {
      "epoch": 9.662029106162246,
      "grad_norm": 4.670191287994385,
      "learning_rate": 1.689854469188774e-06,
      "loss": 0.4677,
      "step": 603500
    },
    {
      "epoch": 9.670034101279198,
      "grad_norm": 4.802608013153076,
      "learning_rate": 1.649829493604009e-06,
      "loss": 0.4619,
      "step": 604000
    },
    {
      "epoch": 9.678039096396152,
      "grad_norm": 4.881260871887207,
      "learning_rate": 1.609804518019244e-06,
      "loss": 0.4763,
      "step": 604500
    },
    {
      "epoch": 9.686044091513104,
      "grad_norm": 5.823531150817871,
      "learning_rate": 1.5697795424344793e-06,
      "loss": 0.4696,
      "step": 605000
    },
    {
      "epoch": 9.694049086630057,
      "grad_norm": 7.500627040863037,
      "learning_rate": 1.5297545668497143e-06,
      "loss": 0.4692,
      "step": 605500
    },
    {
      "epoch": 9.70205408174701,
      "grad_norm": 2.5319697856903076,
      "learning_rate": 1.4897295912649493e-06,
      "loss": 0.4795,
      "step": 606000
    },
    {
      "epoch": 9.710059076863963,
      "grad_norm": 2.562856435775757,
      "learning_rate": 1.4497046156801844e-06,
      "loss": 0.4824,
      "step": 606500
    },
    {
      "epoch": 9.718064071980915,
      "grad_norm": 4.74647331237793,
      "learning_rate": 1.4096796400954196e-06,
      "loss": 0.4699,
      "step": 607000
    },
    {
      "epoch": 9.72606906709787,
      "grad_norm": 6.522175312042236,
      "learning_rate": 1.3696546645106546e-06,
      "loss": 0.4709,
      "step": 607500
    },
    {
      "epoch": 9.734074062214821,
      "grad_norm": 4.2286834716796875,
      "learning_rate": 1.3296296889258899e-06,
      "loss": 0.4649,
      "step": 608000
    },
    {
      "epoch": 9.742079057331775,
      "grad_norm": 2.6113765239715576,
      "learning_rate": 1.289604713341125e-06,
      "loss": 0.4777,
      "step": 608500
    },
    {
      "epoch": 9.750084052448727,
      "grad_norm": 6.711157321929932,
      "learning_rate": 1.24957973775636e-06,
      "loss": 0.4679,
      "step": 609000
    },
    {
      "epoch": 9.758089047565681,
      "grad_norm": 4.964043140411377,
      "learning_rate": 1.2095547621715952e-06,
      "loss": 0.4729,
      "step": 609500
    },
    {
      "epoch": 9.766094042682633,
      "grad_norm": 4.073361396789551,
      "learning_rate": 1.1695297865868302e-06,
      "loss": 0.4751,
      "step": 610000
    },
    {
      "epoch": 9.774099037799587,
      "grad_norm": 2.15155291557312,
      "learning_rate": 1.1295048110020654e-06,
      "loss": 0.4823,
      "step": 610500
    },
    {
      "epoch": 9.78210403291654,
      "grad_norm": 4.466656684875488,
      "learning_rate": 1.0894798354173004e-06,
      "loss": 0.4688,
      "step": 611000
    },
    {
      "epoch": 9.790109028033493,
      "grad_norm": 2.6313233375549316,
      "learning_rate": 1.0494548598325357e-06,
      "loss": 0.4842,
      "step": 611500
    },
    {
      "epoch": 9.798114023150445,
      "grad_norm": 2.9496915340423584,
      "learning_rate": 1.0094298842477707e-06,
      "loss": 0.4675,
      "step": 612000
    },
    {
      "epoch": 9.806119018267399,
      "grad_norm": 2.68385910987854,
      "learning_rate": 9.694049086630057e-07,
      "loss": 0.4738,
      "step": 612500
    },
    {
      "epoch": 9.814124013384351,
      "grad_norm": 6.514122486114502,
      "learning_rate": 9.293799330782409e-07,
      "loss": 0.4747,
      "step": 613000
    },
    {
      "epoch": 9.822129008501305,
      "grad_norm": 4.166613578796387,
      "learning_rate": 8.893549574934761e-07,
      "loss": 0.466,
      "step": 613500
    },
    {
      "epoch": 9.830134003618259,
      "grad_norm": 2.332275629043579,
      "learning_rate": 8.49329981908711e-07,
      "loss": 0.4506,
      "step": 614000
    },
    {
      "epoch": 9.83813899873521,
      "grad_norm": 6.57948637008667,
      "learning_rate": 8.093050063239461e-07,
      "loss": 0.4633,
      "step": 614500
    },
    {
      "epoch": 9.846143993852163,
      "grad_norm": 3.06020450592041,
      "learning_rate": 7.692800307391813e-07,
      "loss": 0.4726,
      "step": 615000
    },
    {
      "epoch": 9.854148988969117,
      "grad_norm": 6.5656609535217285,
      "learning_rate": 7.292550551544164e-07,
      "loss": 0.4673,
      "step": 615500
    },
    {
      "epoch": 9.86215398408607,
      "grad_norm": 3.4577085971832275,
      "learning_rate": 6.892300795696514e-07,
      "loss": 0.4846,
      "step": 616000
    },
    {
      "epoch": 9.870158979203023,
      "grad_norm": 4.1815667152404785,
      "learning_rate": 6.492051039848867e-07,
      "loss": 0.4707,
      "step": 616500
    },
    {
      "epoch": 9.878163974319975,
      "grad_norm": 2.2495129108428955,
      "learning_rate": 6.091801284001217e-07,
      "loss": 0.4688,
      "step": 617000
    },
    {
      "epoch": 9.886168969436929,
      "grad_norm": 1.3799896240234375,
      "learning_rate": 5.691551528153568e-07,
      "loss": 0.4843,
      "step": 617500
    },
    {
      "epoch": 9.894173964553882,
      "grad_norm": 5.703238487243652,
      "learning_rate": 5.291301772305919e-07,
      "loss": 0.4693,
      "step": 618000
    },
    {
      "epoch": 9.902178959670835,
      "grad_norm": 4.997560024261475,
      "learning_rate": 4.89105201645827e-07,
      "loss": 0.4522,
      "step": 618500
    },
    {
      "epoch": 9.910183954787788,
      "grad_norm": 5.771718978881836,
      "learning_rate": 4.4908022606106215e-07,
      "loss": 0.4797,
      "step": 619000
    },
    {
      "epoch": 9.91818894990474,
      "grad_norm": 3.537688970565796,
      "learning_rate": 4.090552504762972e-07,
      "loss": 0.4705,
      "step": 619500
    },
    {
      "epoch": 9.926193945021694,
      "grad_norm": 3.232119560241699,
      "learning_rate": 3.6903027489153236e-07,
      "loss": 0.4657,
      "step": 620000
    },
    {
      "epoch": 9.934198940138646,
      "grad_norm": 4.550255298614502,
      "learning_rate": 3.2900529930676743e-07,
      "loss": 0.4732,
      "step": 620500
    },
    {
      "epoch": 9.9422039352556,
      "grad_norm": 10.766828536987305,
      "learning_rate": 2.889803237220025e-07,
      "loss": 0.4616,
      "step": 621000
    },
    {
      "epoch": 9.950208930372552,
      "grad_norm": 2.8056442737579346,
      "learning_rate": 2.4895534813723764e-07,
      "loss": 0.4678,
      "step": 621500
    },
    {
      "epoch": 9.958213925489506,
      "grad_norm": 5.562440872192383,
      "learning_rate": 2.0893037255247274e-07,
      "loss": 0.4583,
      "step": 622000
    },
    {
      "epoch": 9.966218920606458,
      "grad_norm": 4.468179702758789,
      "learning_rate": 1.6890539696770785e-07,
      "loss": 0.4575,
      "step": 622500
    },
    {
      "epoch": 9.974223915723412,
      "grad_norm": 5.999910831451416,
      "learning_rate": 1.2888042138294295e-07,
      "loss": 0.4704,
      "step": 623000
    },
    {
      "epoch": 9.982228910840364,
      "grad_norm": 3.432342767715454,
      "learning_rate": 8.885544579817807e-08,
      "loss": 0.4826,
      "step": 623500
    },
    {
      "epoch": 9.990233905957318,
      "grad_norm": 8.023146629333496,
      "learning_rate": 4.883047021341317e-08,
      "loss": 0.4816,
      "step": 624000
    },
    {
      "epoch": 9.99823890107427,
      "grad_norm": 4.069036483764648,
      "learning_rate": 8.805494628648276e-09,
      "loss": 0.4757,
      "step": 624500
    },
    {
      "epoch": 10.006724518876686,
      "grad_norm": 5.470510482788086,
      "learning_rate": 8.30531450468048e-06,
      "loss": 0.4887,
      "step": 625000
    },
    {
      "epoch": 10.014729898491787,
      "grad_norm": 5.849669933319092,
      "learning_rate": 8.271958756284222e-06,
      "loss": 0.4726,
      "step": 625500
    },
    {
      "epoch": 10.022735278106888,
      "grad_norm": 3.0656845569610596,
      "learning_rate": 8.238603007887968e-06,
      "loss": 0.4867,
      "step": 626000
    },
    {
      "epoch": 10.030740657721989,
      "grad_norm": 5.8193559646606445,
      "learning_rate": 8.205247259491712e-06,
      "loss": 0.4827,
      "step": 626500
    },
    {
      "epoch": 10.038746037337091,
      "grad_norm": 3.2153804302215576,
      "learning_rate": 8.171891511095458e-06,
      "loss": 0.468,
      "step": 627000
    },
    {
      "epoch": 10.046751416952192,
      "grad_norm": 3.9710168838500977,
      "learning_rate": 8.1385357626992e-06,
      "loss": 0.4968,
      "step": 627500
    },
    {
      "epoch": 10.054756796567293,
      "grad_norm": 4.087085247039795,
      "learning_rate": 8.105180014302944e-06,
      "loss": 0.4811,
      "step": 628000
    },
    {
      "epoch": 10.062762176182394,
      "grad_norm": 5.567630767822266,
      "learning_rate": 8.07182426590669e-06,
      "loss": 0.4803,
      "step": 628500
    },
    {
      "epoch": 10.070767555797495,
      "grad_norm": 12.435041427612305,
      "learning_rate": 8.038468517510434e-06,
      "loss": 0.4813,
      "step": 629000
    },
    {
      "epoch": 10.078772935412598,
      "grad_norm": 4.341611862182617,
      "learning_rate": 8.005112769114178e-06,
      "loss": 0.4777,
      "step": 629500
    },
    {
      "epoch": 10.086778315027699,
      "grad_norm": 3.9880082607269287,
      "learning_rate": 7.971757020717922e-06,
      "loss": 0.4926,
      "step": 630000
    },
    {
      "epoch": 10.0947836946428,
      "grad_norm": 2.713810443878174,
      "learning_rate": 7.938401272321668e-06,
      "loss": 0.4972,
      "step": 630500
    },
    {
      "epoch": 10.1027890742579,
      "grad_norm": 4.235696315765381,
      "learning_rate": 7.90504552392541e-06,
      "loss": 0.4949,
      "step": 631000
    },
    {
      "epoch": 10.110794453873003,
      "grad_norm": 4.516049385070801,
      "learning_rate": 7.871689775529156e-06,
      "loss": 0.4726,
      "step": 631500
    },
    {
      "epoch": 10.118799833488104,
      "grad_norm": 11.413517951965332,
      "learning_rate": 7.8383340271329e-06,
      "loss": 0.4833,
      "step": 632000
    },
    {
      "epoch": 10.126805213103205,
      "grad_norm": 9.600536346435547,
      "learning_rate": 7.804978278736646e-06,
      "loss": 0.4947,
      "step": 632500
    },
    {
      "epoch": 10.134810592718306,
      "grad_norm": 2.280620813369751,
      "learning_rate": 7.771622530340388e-06,
      "loss": 0.495,
      "step": 633000
    },
    {
      "epoch": 10.142815972333407,
      "grad_norm": 3.509176254272461,
      "learning_rate": 7.738266781944134e-06,
      "loss": 0.4874,
      "step": 633500
    },
    {
      "epoch": 10.15082135194851,
      "grad_norm": 14.705697059631348,
      "learning_rate": 7.704911033547878e-06,
      "loss": 0.4697,
      "step": 634000
    },
    {
      "epoch": 10.158826731563611,
      "grad_norm": 6.356698513031006,
      "learning_rate": 7.671555285151622e-06,
      "loss": 0.4944,
      "step": 634500
    },
    {
      "epoch": 10.166832111178712,
      "grad_norm": 5.07946252822876,
      "learning_rate": 7.638199536755366e-06,
      "loss": 0.4879,
      "step": 635000
    },
    {
      "epoch": 10.174837490793813,
      "grad_norm": 4.1223273277282715,
      "learning_rate": 7.604843788359112e-06,
      "loss": 0.4913,
      "step": 635500
    },
    {
      "epoch": 10.182842870408916,
      "grad_norm": 2.325040817260742,
      "learning_rate": 7.571488039962855e-06,
      "loss": 0.4773,
      "step": 636000
    },
    {
      "epoch": 10.190848250024017,
      "grad_norm": 3.7124764919281006,
      "learning_rate": 7.538132291566599e-06,
      "loss": 0.4714,
      "step": 636500
    },
    {
      "epoch": 10.198853629639117,
      "grad_norm": 2.9581682682037354,
      "learning_rate": 7.504776543170344e-06,
      "loss": 0.4949,
      "step": 637000
    },
    {
      "epoch": 10.206859009254218,
      "grad_norm": 3.0793323516845703,
      "learning_rate": 7.471420794774088e-06,
      "loss": 0.4803,
      "step": 637500
    },
    {
      "epoch": 10.21486438886932,
      "grad_norm": 5.4981231689453125,
      "learning_rate": 7.438065046377833e-06,
      "loss": 0.4947,
      "step": 638000
    },
    {
      "epoch": 10.222869768484422,
      "grad_norm": 6.394739627838135,
      "learning_rate": 7.404709297981577e-06,
      "loss": 0.5003,
      "step": 638500
    },
    {
      "epoch": 10.230875148099523,
      "grad_norm": 4.7628173828125,
      "learning_rate": 7.371353549585322e-06,
      "loss": 0.4812,
      "step": 639000
    },
    {
      "epoch": 10.238880527714624,
      "grad_norm": 6.17342472076416,
      "learning_rate": 7.337997801189066e-06,
      "loss": 0.4872,
      "step": 639500
    },
    {
      "epoch": 10.246885907329725,
      "grad_norm": 3.2952792644500732,
      "learning_rate": 7.304642052792811e-06,
      "loss": 0.4848,
      "step": 640000
    },
    {
      "epoch": 10.254891286944828,
      "grad_norm": 3.848728895187378,
      "learning_rate": 7.271286304396554e-06,
      "loss": 0.4761,
      "step": 640500
    },
    {
      "epoch": 10.262896666559929,
      "grad_norm": 6.78574800491333,
      "learning_rate": 7.2379305560003e-06,
      "loss": 0.4709,
      "step": 641000
    },
    {
      "epoch": 10.27090204617503,
      "grad_norm": 4.5487589836120605,
      "learning_rate": 7.204574807604043e-06,
      "loss": 0.4827,
      "step": 641500
    },
    {
      "epoch": 10.27890742579013,
      "grad_norm": 2.680243492126465,
      "learning_rate": 7.171219059207789e-06,
      "loss": 0.4963,
      "step": 642000
    },
    {
      "epoch": 10.286912805405233,
      "grad_norm": 4.999107360839844,
      "learning_rate": 7.137863310811532e-06,
      "loss": 0.4937,
      "step": 642500
    },
    {
      "epoch": 10.294918185020334,
      "grad_norm": 4.567774772644043,
      "learning_rate": 7.104507562415277e-06,
      "loss": 0.4858,
      "step": 643000
    },
    {
      "epoch": 10.302923564635435,
      "grad_norm": 4.230068683624268,
      "learning_rate": 7.071151814019021e-06,
      "loss": 0.4719,
      "step": 643500
    },
    {
      "epoch": 10.310928944250536,
      "grad_norm": 3.5749969482421875,
      "learning_rate": 7.037796065622765e-06,
      "loss": 0.4943,
      "step": 644000
    },
    {
      "epoch": 10.318934323865637,
      "grad_norm": 3.1133077144622803,
      "learning_rate": 7.00444031722651e-06,
      "loss": 0.4943,
      "step": 644500
    },
    {
      "epoch": 10.32693970348074,
      "grad_norm": 1.8893510103225708,
      "learning_rate": 6.971084568830254e-06,
      "loss": 0.4777,
      "step": 645000
    },
    {
      "epoch": 10.33494508309584,
      "grad_norm": 4.108112335205078,
      "learning_rate": 6.937728820433999e-06,
      "loss": 0.469,
      "step": 645500
    },
    {
      "epoch": 10.342950462710942,
      "grad_norm": 3.5651087760925293,
      "learning_rate": 6.904373072037742e-06,
      "loss": 0.4864,
      "step": 646000
    },
    {
      "epoch": 10.350955842326043,
      "grad_norm": 5.0339508056640625,
      "learning_rate": 6.871017323641488e-06,
      "loss": 0.4964,
      "step": 646500
    },
    {
      "epoch": 10.358961221941144,
      "grad_norm": 3.0170187950134277,
      "learning_rate": 6.837661575245231e-06,
      "loss": 0.501,
      "step": 647000
    },
    {
      "epoch": 10.366966601556246,
      "grad_norm": 2.4703023433685303,
      "learning_rate": 6.804305826848977e-06,
      "loss": 0.4801,
      "step": 647500
    },
    {
      "epoch": 10.374971981171347,
      "grad_norm": 3.62662410736084,
      "learning_rate": 6.77095007845272e-06,
      "loss": 0.4879,
      "step": 648000
    },
    {
      "epoch": 10.382977360786448,
      "grad_norm": 6.640106678009033,
      "learning_rate": 6.737594330056465e-06,
      "loss": 0.477,
      "step": 648500
    },
    {
      "epoch": 10.39098274040155,
      "grad_norm": 3.143436908721924,
      "learning_rate": 6.704238581660209e-06,
      "loss": 0.4834,
      "step": 649000
    },
    {
      "epoch": 10.398988120016652,
      "grad_norm": 3.9954755306243896,
      "learning_rate": 6.670882833263954e-06,
      "loss": 0.4858,
      "step": 649500
    },
    {
      "epoch": 10.406993499631753,
      "grad_norm": 6.465503215789795,
      "learning_rate": 6.637527084867698e-06,
      "loss": 0.4956,
      "step": 650000
    },
    {
      "epoch": 10.414998879246854,
      "grad_norm": 8.54849910736084,
      "learning_rate": 6.604171336471443e-06,
      "loss": 0.4728,
      "step": 650500
    },
    {
      "epoch": 10.423004258861955,
      "grad_norm": 1.4915413856506348,
      "learning_rate": 6.570815588075187e-06,
      "loss": 0.4936,
      "step": 651000
    },
    {
      "epoch": 10.431009638477057,
      "grad_norm": 6.542646884918213,
      "learning_rate": 6.537459839678932e-06,
      "loss": 0.4738,
      "step": 651500
    },
    {
      "epoch": 10.439015018092158,
      "grad_norm": 4.499027729034424,
      "learning_rate": 6.504104091282676e-06,
      "loss": 0.4592,
      "step": 652000
    },
    {
      "epoch": 10.44702039770726,
      "grad_norm": 4.921358585357666,
      "learning_rate": 6.470748342886419e-06,
      "loss": 0.4739,
      "step": 652500
    },
    {
      "epoch": 10.45502577732236,
      "grad_norm": 4.252784252166748,
      "learning_rate": 6.437392594490165e-06,
      "loss": 0.4832,
      "step": 653000
    },
    {
      "epoch": 10.463031156937461,
      "grad_norm": 11.324130058288574,
      "learning_rate": 6.404036846093908e-06,
      "loss": 0.468,
      "step": 653500
    },
    {
      "epoch": 10.471036536552564,
      "grad_norm": 1.852501392364502,
      "learning_rate": 6.370681097697653e-06,
      "loss": 0.481,
      "step": 654000
    },
    {
      "epoch": 10.479041916167665,
      "grad_norm": 2.10073184967041,
      "learning_rate": 6.337325349301397e-06,
      "loss": 0.4625,
      "step": 654500
    },
    {
      "epoch": 10.487047295782766,
      "grad_norm": 7.275043964385986,
      "learning_rate": 6.303969600905142e-06,
      "loss": 0.4795,
      "step": 655000
    },
    {
      "epoch": 10.495052675397867,
      "grad_norm": 3.0701944828033447,
      "learning_rate": 6.270613852508886e-06,
      "loss": 0.4863,
      "step": 655500
    },
    {
      "epoch": 10.50305805501297,
      "grad_norm": 4.170275688171387,
      "learning_rate": 6.23725810411263e-06,
      "loss": 0.466,
      "step": 656000
    },
    {
      "epoch": 10.51106343462807,
      "grad_norm": 5.434985160827637,
      "learning_rate": 6.203902355716375e-06,
      "loss": 0.4603,
      "step": 656500
    },
    {
      "epoch": 10.519068814243171,
      "grad_norm": 4.634799003601074,
      "learning_rate": 6.170546607320119e-06,
      "loss": 0.4769,
      "step": 657000
    },
    {
      "epoch": 10.527074193858272,
      "grad_norm": 7.567322731018066,
      "learning_rate": 6.137190858923864e-06,
      "loss": 0.4801,
      "step": 657500
    },
    {
      "epoch": 10.535079573473373,
      "grad_norm": 2.7263762950897217,
      "learning_rate": 6.103835110527608e-06,
      "loss": 0.4833,
      "step": 658000
    },
    {
      "epoch": 10.543084953088476,
      "grad_norm": 6.523097515106201,
      "learning_rate": 6.070479362131353e-06,
      "loss": 0.4704,
      "step": 658500
    },
    {
      "epoch": 10.551090332703577,
      "grad_norm": 4.007072448730469,
      "learning_rate": 6.037123613735097e-06,
      "loss": 0.4683,
      "step": 659000
    },
    {
      "epoch": 10.559095712318678,
      "grad_norm": 2.6567680835723877,
      "learning_rate": 6.003767865338841e-06,
      "loss": 0.4739,
      "step": 659500
    },
    {
      "epoch": 10.567101091933779,
      "grad_norm": 4.53572940826416,
      "learning_rate": 5.970412116942586e-06,
      "loss": 0.4749,
      "step": 660000
    },
    {
      "epoch": 10.575106471548882,
      "grad_norm": 6.863628387451172,
      "learning_rate": 5.93705636854633e-06,
      "loss": 0.4878,
      "step": 660500
    },
    {
      "epoch": 10.583111851163983,
      "grad_norm": 5.49797248840332,
      "learning_rate": 5.903700620150075e-06,
      "loss": 0.4849,
      "step": 661000
    },
    {
      "epoch": 10.591117230779084,
      "grad_norm": 5.693562030792236,
      "learning_rate": 5.870344871753819e-06,
      "loss": 0.4632,
      "step": 661500
    },
    {
      "epoch": 10.599122610394184,
      "grad_norm": 2.6533448696136475,
      "learning_rate": 5.836989123357564e-06,
      "loss": 0.4942,
      "step": 662000
    },
    {
      "epoch": 10.607127990009285,
      "grad_norm": 38.63438415527344,
      "learning_rate": 5.803633374961308e-06,
      "loss": 0.4844,
      "step": 662500
    },
    {
      "epoch": 10.615133369624388,
      "grad_norm": 5.793990135192871,
      "learning_rate": 5.770277626565052e-06,
      "loss": 0.4747,
      "step": 663000
    },
    {
      "epoch": 10.623138749239489,
      "grad_norm": 2.2866604328155518,
      "learning_rate": 5.736921878168797e-06,
      "loss": 0.4712,
      "step": 663500
    },
    {
      "epoch": 10.63114412885459,
      "grad_norm": 4.985671520233154,
      "learning_rate": 5.703566129772541e-06,
      "loss": 0.4863,
      "step": 664000
    },
    {
      "epoch": 10.639149508469691,
      "grad_norm": 3.3653392791748047,
      "learning_rate": 5.670210381376285e-06,
      "loss": 0.4788,
      "step": 664500
    },
    {
      "epoch": 10.647154888084794,
      "grad_norm": 4.534169673919678,
      "learning_rate": 5.636854632980029e-06,
      "loss": 0.4858,
      "step": 665000
    },
    {
      "epoch": 10.655160267699895,
      "grad_norm": 3.6741433143615723,
      "learning_rate": 5.603498884583774e-06,
      "loss": 0.4782,
      "step": 665500
    },
    {
      "epoch": 10.663165647314996,
      "grad_norm": 2.126056671142578,
      "learning_rate": 5.570143136187518e-06,
      "loss": 0.4904,
      "step": 666000
    },
    {
      "epoch": 10.671171026930097,
      "grad_norm": 2.3533291816711426,
      "learning_rate": 5.536787387791263e-06,
      "loss": 0.4947,
      "step": 666500
    },
    {
      "epoch": 10.6791764065452,
      "grad_norm": 3.3691720962524414,
      "learning_rate": 5.503431639395007e-06,
      "loss": 0.4838,
      "step": 667000
    },
    {
      "epoch": 10.6871817861603,
      "grad_norm": 2.700721502304077,
      "learning_rate": 5.470075890998752e-06,
      "loss": 0.4815,
      "step": 667500
    },
    {
      "epoch": 10.695187165775401,
      "grad_norm": 5.718003749847412,
      "learning_rate": 5.436720142602496e-06,
      "loss": 0.4975,
      "step": 668000
    },
    {
      "epoch": 10.703192545390502,
      "grad_norm": 7.783771991729736,
      "learning_rate": 5.40336439420624e-06,
      "loss": 0.4836,
      "step": 668500
    },
    {
      "epoch": 10.711197925005603,
      "grad_norm": 4.181576728820801,
      "learning_rate": 5.370008645809985e-06,
      "loss": 0.4876,
      "step": 669000
    },
    {
      "epoch": 10.719203304620706,
      "grad_norm": 3.7221415042877197,
      "learning_rate": 5.336652897413729e-06,
      "loss": 0.4947,
      "step": 669500
    },
    {
      "epoch": 10.727208684235807,
      "grad_norm": 5.659352779388428,
      "learning_rate": 5.303297149017474e-06,
      "loss": 0.4662,
      "step": 670000
    },
    {
      "epoch": 10.735214063850908,
      "grad_norm": 4.674367427825928,
      "learning_rate": 5.269941400621218e-06,
      "loss": 0.482,
      "step": 670500
    },
    {
      "epoch": 10.743219443466009,
      "grad_norm": 2.8349506855010986,
      "learning_rate": 5.236585652224963e-06,
      "loss": 0.5012,
      "step": 671000
    },
    {
      "epoch": 10.75122482308111,
      "grad_norm": 1.9274868965148926,
      "learning_rate": 5.203229903828707e-06,
      "loss": 0.4783,
      "step": 671500
    },
    {
      "epoch": 10.759230202696212,
      "grad_norm": 2.7143983840942383,
      "learning_rate": 5.169874155432451e-06,
      "loss": 0.4763,
      "step": 672000
    },
    {
      "epoch": 10.767235582311313,
      "grad_norm": 4.437305927276611,
      "learning_rate": 5.136518407036195e-06,
      "loss": 0.4839,
      "step": 672500
    },
    {
      "epoch": 10.775240961926414,
      "grad_norm": 1.6178510189056396,
      "learning_rate": 5.10316265863994e-06,
      "loss": 0.483,
      "step": 673000
    },
    {
      "epoch": 10.783246341541515,
      "grad_norm": 6.458110809326172,
      "learning_rate": 5.069806910243684e-06,
      "loss": 0.4729,
      "step": 673500
    },
    {
      "epoch": 10.791251721156618,
      "grad_norm": 5.580428123474121,
      "learning_rate": 5.036451161847428e-06,
      "loss": 0.4697,
      "step": 674000
    },
    {
      "epoch": 10.799257100771719,
      "grad_norm": 2.648955821990967,
      "learning_rate": 5.003095413451173e-06,
      "loss": 0.4883,
      "step": 674500
    },
    {
      "epoch": 10.80726248038682,
      "grad_norm": 4.612020492553711,
      "learning_rate": 4.969739665054917e-06,
      "loss": 0.4894,
      "step": 675000
    },
    {
      "epoch": 10.81526786000192,
      "grad_norm": 3.313974380493164,
      "learning_rate": 4.936383916658662e-06,
      "loss": 0.4724,
      "step": 675500
    },
    {
      "epoch": 10.823273239617023,
      "grad_norm": 5.438807010650635,
      "learning_rate": 4.903028168262406e-06,
      "loss": 0.5011,
      "step": 676000
    },
    {
      "epoch": 10.831278619232124,
      "grad_norm": 5.660580158233643,
      "learning_rate": 4.869672419866151e-06,
      "loss": 0.4893,
      "step": 676500
    },
    {
      "epoch": 10.839283998847225,
      "grad_norm": 9.417227745056152,
      "learning_rate": 4.836316671469895e-06,
      "loss": 0.4763,
      "step": 677000
    },
    {
      "epoch": 10.847289378462326,
      "grad_norm": 9.060622215270996,
      "learning_rate": 4.802960923073639e-06,
      "loss": 0.4681,
      "step": 677500
    },
    {
      "epoch": 10.855294758077427,
      "grad_norm": 6.9499311447143555,
      "learning_rate": 4.769605174677384e-06,
      "loss": 0.4747,
      "step": 678000
    },
    {
      "epoch": 10.86330013769253,
      "grad_norm": 9.39783763885498,
      "learning_rate": 4.736249426281128e-06,
      "loss": 0.4732,
      "step": 678500
    },
    {
      "epoch": 10.871305517307631,
      "grad_norm": 5.604684829711914,
      "learning_rate": 4.702893677884873e-06,
      "loss": 0.4729,
      "step": 679000
    },
    {
      "epoch": 10.879310896922732,
      "grad_norm": 3.858819007873535,
      "learning_rate": 4.669537929488617e-06,
      "loss": 0.4977,
      "step": 679500
    },
    {
      "epoch": 10.887316276537833,
      "grad_norm": 2.5261905193328857,
      "learning_rate": 4.6361821810923616e-06,
      "loss": 0.4863,
      "step": 680000
    },
    {
      "epoch": 10.895321656152936,
      "grad_norm": 2.3813841342926025,
      "learning_rate": 4.602826432696105e-06,
      "loss": 0.497,
      "step": 680500
    },
    {
      "epoch": 10.903327035768037,
      "grad_norm": 5.640920162200928,
      "learning_rate": 4.56947068429985e-06,
      "loss": 0.4864,
      "step": 681000
    },
    {
      "epoch": 10.911332415383137,
      "grad_norm": 4.5505900382995605,
      "learning_rate": 4.536114935903594e-06,
      "loss": 0.4729,
      "step": 681500
    },
    {
      "epoch": 10.919337794998238,
      "grad_norm": 2.4894585609436035,
      "learning_rate": 4.502759187507339e-06,
      "loss": 0.4915,
      "step": 682000
    },
    {
      "epoch": 10.92734317461334,
      "grad_norm": 2.2234435081481934,
      "learning_rate": 4.469403439111083e-06,
      "loss": 0.4963,
      "step": 682500
    },
    {
      "epoch": 10.935348554228442,
      "grad_norm": 6.8214616775512695,
      "learning_rate": 4.436047690714827e-06,
      "loss": 0.4923,
      "step": 683000
    },
    {
      "epoch": 10.943353933843543,
      "grad_norm": 5.412674427032471,
      "learning_rate": 4.402691942318572e-06,
      "loss": 0.4849,
      "step": 683500
    },
    {
      "epoch": 10.951359313458644,
      "grad_norm": 3.0863149166107178,
      "learning_rate": 4.369336193922316e-06,
      "loss": 0.4802,
      "step": 684000
    },
    {
      "epoch": 10.959364693073745,
      "grad_norm": 5.092045307159424,
      "learning_rate": 4.335980445526061e-06,
      "loss": 0.487,
      "step": 684500
    },
    {
      "epoch": 10.967370072688848,
      "grad_norm": 4.352598667144775,
      "learning_rate": 4.302624697129805e-06,
      "loss": 0.4758,
      "step": 685000
    },
    {
      "epoch": 10.975375452303949,
      "grad_norm": 3.978762626647949,
      "learning_rate": 4.2692689487335495e-06,
      "loss": 0.4792,
      "step": 685500
    },
    {
      "epoch": 10.98338083191905,
      "grad_norm": 3.0082077980041504,
      "learning_rate": 4.235913200337294e-06,
      "loss": 0.4777,
      "step": 686000
    },
    {
      "epoch": 10.99138621153415,
      "grad_norm": 3.8015143871307373,
      "learning_rate": 4.2025574519410385e-06,
      "loss": 0.4814,
      "step": 686500
    },
    {
      "epoch": 10.999391591149251,
      "grad_norm": 3.841181755065918,
      "learning_rate": 4.1692017035447825e-06,
      "loss": 0.4961,
      "step": 687000
    },
    {
      "epoch": 11.007396970764354,
      "grad_norm": 5.119665622711182,
      "learning_rate": 4.135845955148527e-06,
      "loss": 0.4729,
      "step": 687500
    },
    {
      "epoch": 11.015402350379455,
      "grad_norm": 1.910760521888733,
      "learning_rate": 4.1024902067522715e-06,
      "loss": 0.4679,
      "step": 688000
    },
    {
      "epoch": 11.023407729994556,
      "grad_norm": 4.830470085144043,
      "learning_rate": 4.0691344583560156e-06,
      "loss": 0.4703,
      "step": 688500
    },
    {
      "epoch": 11.031413109609657,
      "grad_norm": 2.897838830947876,
      "learning_rate": 4.03577870995976e-06,
      "loss": 0.4578,
      "step": 689000
    },
    {
      "epoch": 11.03941848922476,
      "grad_norm": 5.5505452156066895,
      "learning_rate": 4.002422961563504e-06,
      "loss": 0.4795,
      "step": 689500
    },
    {
      "epoch": 11.04742386883986,
      "grad_norm": 5.075151443481445,
      "learning_rate": 3.9690672131672486e-06,
      "loss": 0.4573,
      "step": 690000
    },
    {
      "epoch": 11.055429248454962,
      "grad_norm": 4.21185827255249,
      "learning_rate": 3.935711464770993e-06,
      "loss": 0.4717,
      "step": 690500
    },
    {
      "epoch": 11.063434628070063,
      "grad_norm": 4.3418684005737305,
      "learning_rate": 3.9023557163747375e-06,
      "loss": 0.4631,
      "step": 691000
    },
    {
      "epoch": 11.071440007685164,
      "grad_norm": 5.183919429779053,
      "learning_rate": 3.868999967978482e-06,
      "loss": 0.47,
      "step": 691500
    },
    {
      "epoch": 11.079445387300266,
      "grad_norm": 3.4222068786621094,
      "learning_rate": 3.8356442195822265e-06,
      "loss": 0.4643,
      "step": 692000
    },
    {
      "epoch": 11.087450766915367,
      "grad_norm": 9.471285820007324,
      "learning_rate": 3.8022884711859705e-06,
      "loss": 0.467,
      "step": 692500
    },
    {
      "epoch": 11.095456146530468,
      "grad_norm": 3.920438289642334,
      "learning_rate": 3.768932722789715e-06,
      "loss": 0.471,
      "step": 693000
    },
    {
      "epoch": 11.10346152614557,
      "grad_norm": 3.4069528579711914,
      "learning_rate": 3.7355769743934595e-06,
      "loss": 0.4711,
      "step": 693500
    },
    {
      "epoch": 11.111466905760672,
      "grad_norm": 3.922966957092285,
      "learning_rate": 3.7022212259972035e-06,
      "loss": 0.4666,
      "step": 694000
    },
    {
      "epoch": 11.119472285375773,
      "grad_norm": 3.797023296356201,
      "learning_rate": 3.668865477600948e-06,
      "loss": 0.4535,
      "step": 694500
    },
    {
      "epoch": 11.127477664990874,
      "grad_norm": 5.1602654457092285,
      "learning_rate": 3.6355097292046925e-06,
      "loss": 0.4538,
      "step": 695000
    },
    {
      "epoch": 11.135483044605975,
      "grad_norm": 5.081469535827637,
      "learning_rate": 3.602153980808437e-06,
      "loss": 0.4716,
      "step": 695500
    },
    {
      "epoch": 11.143488424221077,
      "grad_norm": 7.29184627532959,
      "learning_rate": 3.5687982324121815e-06,
      "loss": 0.4665,
      "step": 696000
    },
    {
      "epoch": 11.151493803836178,
      "grad_norm": 2.334758758544922,
      "learning_rate": 3.535442484015926e-06,
      "loss": 0.4652,
      "step": 696500
    },
    {
      "epoch": 11.15949918345128,
      "grad_norm": 4.820038795471191,
      "learning_rate": 3.5020867356196696e-06,
      "loss": 0.4594,
      "step": 697000
    },
    {
      "epoch": 11.16750456306638,
      "grad_norm": 2.4882376194000244,
      "learning_rate": 3.468730987223414e-06,
      "loss": 0.4638,
      "step": 697500
    },
    {
      "epoch": 11.175509942681481,
      "grad_norm": 2.6029882431030273,
      "learning_rate": 3.4353752388271585e-06,
      "loss": 0.4647,
      "step": 698000
    },
    {
      "epoch": 11.183515322296584,
      "grad_norm": 15.868964195251465,
      "learning_rate": 3.402019490430903e-06,
      "loss": 0.4667,
      "step": 698500
    },
    {
      "epoch": 11.191520701911685,
      "grad_norm": 15.805930137634277,
      "learning_rate": 3.3686637420346475e-06,
      "loss": 0.4627,
      "step": 699000
    },
    {
      "epoch": 11.199526081526786,
      "grad_norm": 8.922772407531738,
      "learning_rate": 3.3353079936383915e-06,
      "loss": 0.4646,
      "step": 699500
    },
    {
      "epoch": 11.207531461141887,
      "grad_norm": 17.772533416748047,
      "learning_rate": 3.301952245242136e-06,
      "loss": 0.4477,
      "step": 700000
    },
    {
      "epoch": 11.21553684075699,
      "grad_norm": 5.2743120193481445,
      "learning_rate": 3.2685964968458805e-06,
      "loss": 0.4735,
      "step": 700500
    },
    {
      "epoch": 11.22354222037209,
      "grad_norm": 6.8638105392456055,
      "learning_rate": 3.235240748449625e-06,
      "loss": 0.4647,
      "step": 701000
    },
    {
      "epoch": 11.231547599987191,
      "grad_norm": 4.4224958419799805,
      "learning_rate": 3.2018850000533694e-06,
      "loss": 0.4723,
      "step": 701500
    },
    {
      "epoch": 11.239552979602292,
      "grad_norm": 3.776517868041992,
      "learning_rate": 3.168529251657114e-06,
      "loss": 0.4465,
      "step": 702000
    },
    {
      "epoch": 11.247558359217393,
      "grad_norm": 7.283871650695801,
      "learning_rate": 3.1351735032608584e-06,
      "loss": 0.4753,
      "step": 702500
    },
    {
      "epoch": 11.255563738832496,
      "grad_norm": 2.0249743461608887,
      "learning_rate": 3.1018177548646024e-06,
      "loss": 0.4761,
      "step": 703000
    },
    {
      "epoch": 11.263569118447597,
      "grad_norm": 7.648217678070068,
      "learning_rate": 3.068462006468347e-06,
      "loss": 0.4671,
      "step": 703500
    },
    {
      "epoch": 11.271574498062698,
      "grad_norm": 3.2816147804260254,
      "learning_rate": 3.035106258072091e-06,
      "loss": 0.4776,
      "step": 704000
    },
    {
      "epoch": 11.279579877677799,
      "grad_norm": 7.362423419952393,
      "learning_rate": 3.0017505096758355e-06,
      "loss": 0.4663,
      "step": 704500
    },
    {
      "epoch": 11.287585257292902,
      "grad_norm": 4.628739833831787,
      "learning_rate": 2.96839476127958e-06,
      "loss": 0.4571,
      "step": 705000
    },
    {
      "epoch": 11.295590636908003,
      "grad_norm": 7.888611793518066,
      "learning_rate": 2.9350390128833244e-06,
      "loss": 0.4798,
      "step": 705500
    },
    {
      "epoch": 11.303596016523104,
      "grad_norm": 1.8295124769210815,
      "learning_rate": 2.901683264487069e-06,
      "loss": 0.454,
      "step": 706000
    },
    {
      "epoch": 11.311601396138204,
      "grad_norm": 4.377436637878418,
      "learning_rate": 2.8683275160908134e-06,
      "loss": 0.4567,
      "step": 706500
    },
    {
      "epoch": 11.319606775753305,
      "grad_norm": 49.420467376708984,
      "learning_rate": 2.8349717676945574e-06,
      "loss": 0.477,
      "step": 707000
    },
    {
      "epoch": 11.327612155368408,
      "grad_norm": 4.276576042175293,
      "learning_rate": 2.801616019298302e-06,
      "loss": 0.4678,
      "step": 707500
    },
    {
      "epoch": 11.335617534983509,
      "grad_norm": 5.855917930603027,
      "learning_rate": 2.7682602709020464e-06,
      "loss": 0.4686,
      "step": 708000
    },
    {
      "epoch": 11.34362291459861,
      "grad_norm": 3.1212024688720703,
      "learning_rate": 2.734904522505791e-06,
      "loss": 0.4526,
      "step": 708500
    },
    {
      "epoch": 11.351628294213711,
      "grad_norm": 3.0937023162841797,
      "learning_rate": 2.701548774109535e-06,
      "loss": 0.4586,
      "step": 709000
    },
    {
      "epoch": 11.359633673828814,
      "grad_norm": 3.2644004821777344,
      "learning_rate": 2.6681930257132794e-06,
      "loss": 0.4738,
      "step": 709500
    },
    {
      "epoch": 11.367639053443915,
      "grad_norm": 3.7352097034454346,
      "learning_rate": 2.634837277317024e-06,
      "loss": 0.457,
      "step": 710000
    },
    {
      "epoch": 11.375644433059016,
      "grad_norm": 6.106478691101074,
      "learning_rate": 2.6014815289207683e-06,
      "loss": 0.4798,
      "step": 710500
    },
    {
      "epoch": 11.383649812674117,
      "grad_norm": 2.472015142440796,
      "learning_rate": 2.5681257805245124e-06,
      "loss": 0.443,
      "step": 711000
    },
    {
      "epoch": 11.391655192289218,
      "grad_norm": 5.459307670593262,
      "learning_rate": 2.534770032128257e-06,
      "loss": 0.4634,
      "step": 711500
    },
    {
      "epoch": 11.39966057190432,
      "grad_norm": 5.820944309234619,
      "learning_rate": 2.5014142837320013e-06,
      "loss": 0.4559,
      "step": 712000
    },
    {
      "epoch": 11.407665951519421,
      "grad_norm": 5.839494228363037,
      "learning_rate": 2.468058535335746e-06,
      "loss": 0.4735,
      "step": 712500
    },
    {
      "epoch": 11.415671331134522,
      "grad_norm": 6.274693489074707,
      "learning_rate": 2.4347027869394903e-06,
      "loss": 0.4423,
      "step": 713000
    },
    {
      "epoch": 11.423676710749623,
      "grad_norm": 10.652609825134277,
      "learning_rate": 2.4013470385432344e-06,
      "loss": 0.4426,
      "step": 713500
    },
    {
      "epoch": 11.431682090364726,
      "grad_norm": 3.17940092086792,
      "learning_rate": 2.367991290146979e-06,
      "loss": 0.463,
      "step": 714000
    },
    {
      "epoch": 11.439687469979827,
      "grad_norm": 4.5497145652771,
      "learning_rate": 2.3346355417507233e-06,
      "loss": 0.4618,
      "step": 714500
    },
    {
      "epoch": 11.447692849594928,
      "grad_norm": 4.780947208404541,
      "learning_rate": 2.3012797933544674e-06,
      "loss": 0.4575,
      "step": 715000
    },
    {
      "epoch": 11.455698229210029,
      "grad_norm": 3.1102874279022217,
      "learning_rate": 2.267924044958212e-06,
      "loss": 0.4661,
      "step": 715500
    },
    {
      "epoch": 11.463703608825131,
      "grad_norm": 1.4264346361160278,
      "learning_rate": 2.2345682965619563e-06,
      "loss": 0.4588,
      "step": 716000
    },
    {
      "epoch": 11.471708988440232,
      "grad_norm": 2.5941224098205566,
      "learning_rate": 2.201212548165701e-06,
      "loss": 0.4558,
      "step": 716500
    },
    {
      "epoch": 11.479714368055333,
      "grad_norm": 3.6294565200805664,
      "learning_rate": 2.1678567997694453e-06,
      "loss": 0.4677,
      "step": 717000
    },
    {
      "epoch": 11.487719747670434,
      "grad_norm": 3.776902437210083,
      "learning_rate": 2.1345010513731898e-06,
      "loss": 0.4533,
      "step": 717500
    },
    {
      "epoch": 11.495725127285535,
      "grad_norm": 15.585212707519531,
      "learning_rate": 2.101145302976934e-06,
      "loss": 0.4653,
      "step": 718000
    },
    {
      "epoch": 11.503730506900638,
      "grad_norm": 4.023383140563965,
      "learning_rate": 2.0677895545806783e-06,
      "loss": 0.4674,
      "step": 718500
    },
    {
      "epoch": 11.511735886515739,
      "grad_norm": 3.1580965518951416,
      "learning_rate": 2.0344338061844228e-06,
      "loss": 0.4766,
      "step": 719000
    },
    {
      "epoch": 11.51974126613084,
      "grad_norm": 4.10944128036499,
      "learning_rate": 2.001078057788167e-06,
      "loss": 0.4604,
      "step": 719500
    },
    {
      "epoch": 11.52774664574594,
      "grad_norm": 5.155796051025391,
      "learning_rate": 1.9677223093919113e-06,
      "loss": 0.4782,
      "step": 720000
    },
    {
      "epoch": 11.535752025361042,
      "grad_norm": 2.278006076812744,
      "learning_rate": 1.9343665609956558e-06,
      "loss": 0.4674,
      "step": 720500
    },
    {
      "epoch": 11.543757404976144,
      "grad_norm": 1.8879839181900024,
      "learning_rate": 1.9010108125994003e-06,
      "loss": 0.4506,
      "step": 721000
    },
    {
      "epoch": 11.551762784591245,
      "grad_norm": 2.305877208709717,
      "learning_rate": 1.8676550642031447e-06,
      "loss": 0.4734,
      "step": 721500
    },
    {
      "epoch": 11.559768164206346,
      "grad_norm": 5.410774230957031,
      "learning_rate": 1.834299315806889e-06,
      "loss": 0.453,
      "step": 722000
    },
    {
      "epoch": 11.567773543821447,
      "grad_norm": 3.32371187210083,
      "learning_rate": 1.8009435674106335e-06,
      "loss": 0.4808,
      "step": 722500
    },
    {
      "epoch": 11.57577892343655,
      "grad_norm": 2.2067360877990723,
      "learning_rate": 1.767587819014378e-06,
      "loss": 0.4458,
      "step": 723000
    },
    {
      "epoch": 11.583784303051651,
      "grad_norm": 6.8165740966796875,
      "learning_rate": 1.734232070618122e-06,
      "loss": 0.4636,
      "step": 723500
    },
    {
      "epoch": 11.591789682666752,
      "grad_norm": 16.43808364868164,
      "learning_rate": 1.7008763222218665e-06,
      "loss": 0.4517,
      "step": 724000
    },
    {
      "epoch": 11.599795062281853,
      "grad_norm": 3.1241040229797363,
      "learning_rate": 1.6675205738256107e-06,
      "loss": 0.4577,
      "step": 724500
    },
    {
      "epoch": 11.607800441896956,
      "grad_norm": 2.6041512489318848,
      "learning_rate": 1.6341648254293552e-06,
      "loss": 0.4548,
      "step": 725000
    },
    {
      "epoch": 11.615805821512057,
      "grad_norm": 4.173609256744385,
      "learning_rate": 1.6008090770330997e-06,
      "loss": 0.4519,
      "step": 725500
    },
    {
      "epoch": 11.623811201127157,
      "grad_norm": 2.768186330795288,
      "learning_rate": 1.5674533286368442e-06,
      "loss": 0.4652,
      "step": 726000
    },
    {
      "epoch": 11.631816580742258,
      "grad_norm": 4.6106181144714355,
      "learning_rate": 1.5340975802405884e-06,
      "loss": 0.4712,
      "step": 726500
    },
    {
      "epoch": 11.63982196035736,
      "grad_norm": 3.180299997329712,
      "learning_rate": 1.5007418318443327e-06,
      "loss": 0.4616,
      "step": 727000
    },
    {
      "epoch": 11.647827339972462,
      "grad_norm": 5.230065822601318,
      "learning_rate": 1.4673860834480772e-06,
      "loss": 0.4532,
      "step": 727500
    },
    {
      "epoch": 11.655832719587563,
      "grad_norm": 6.898614883422852,
      "learning_rate": 1.4340303350518217e-06,
      "loss": 0.462,
      "step": 728000
    },
    {
      "epoch": 11.663838099202664,
      "grad_norm": 2.742816925048828,
      "learning_rate": 1.400674586655566e-06,
      "loss": 0.4573,
      "step": 728500
    },
    {
      "epoch": 11.671843478817765,
      "grad_norm": 4.231769561767578,
      "learning_rate": 1.3673188382593102e-06,
      "loss": 0.4684,
      "step": 729000
    },
    {
      "epoch": 11.679848858432868,
      "grad_norm": 2.4293899536132812,
      "learning_rate": 1.3339630898630547e-06,
      "loss": 0.4597,
      "step": 729500
    },
    {
      "epoch": 11.687854238047969,
      "grad_norm": 1.9994341135025024,
      "learning_rate": 1.3006073414667992e-06,
      "loss": 0.4423,
      "step": 730000
    },
    {
      "epoch": 11.69585961766307,
      "grad_norm": 2.4650657176971436,
      "learning_rate": 1.2672515930705434e-06,
      "loss": 0.4559,
      "step": 730500
    },
    {
      "epoch": 11.70386499727817,
      "grad_norm": 3.6515941619873047,
      "learning_rate": 1.233895844674288e-06,
      "loss": 0.4736,
      "step": 731000
    },
    {
      "epoch": 11.711870376893271,
      "grad_norm": 3.4739372730255127,
      "learning_rate": 1.2005400962780322e-06,
      "loss": 0.4652,
      "step": 731500
    },
    {
      "epoch": 11.719875756508374,
      "grad_norm": 3.75335693359375,
      "learning_rate": 1.1671843478817766e-06,
      "loss": 0.4646,
      "step": 732000
    },
    {
      "epoch": 11.727881136123475,
      "grad_norm": 6.142852783203125,
      "learning_rate": 1.133828599485521e-06,
      "loss": 0.4623,
      "step": 732500
    },
    {
      "epoch": 11.735886515738576,
      "grad_norm": 6.60758113861084,
      "learning_rate": 1.1004728510892654e-06,
      "loss": 0.4575,
      "step": 733000
    },
    {
      "epoch": 11.743891895353677,
      "grad_norm": 6.736484527587891,
      "learning_rate": 1.0671171026930099e-06,
      "loss": 0.463,
      "step": 733500
    },
    {
      "epoch": 11.75189727496878,
      "grad_norm": 3.305908203125,
      "learning_rate": 1.0337613542967541e-06,
      "loss": 0.4767,
      "step": 734000
    },
    {
      "epoch": 11.75990265458388,
      "grad_norm": 5.0193939208984375,
      "learning_rate": 1.0004056059004984e-06,
      "loss": 0.4726,
      "step": 734500
    },
    {
      "epoch": 11.767908034198982,
      "grad_norm": 7.0187530517578125,
      "learning_rate": 9.670498575042429e-07,
      "loss": 0.441,
      "step": 735000
    },
    {
      "epoch": 11.775913413814083,
      "grad_norm": 4.387503623962402,
      "learning_rate": 9.336941091079872e-07,
      "loss": 0.4713,
      "step": 735500
    },
    {
      "epoch": 11.783918793429184,
      "grad_norm": 5.752274036407471,
      "learning_rate": 9.003383607117317e-07,
      "loss": 0.47,
      "step": 736000
    },
    {
      "epoch": 11.791924173044286,
      "grad_norm": 7.158191680908203,
      "learning_rate": 8.66982612315476e-07,
      "loss": 0.4657,
      "step": 736500
    },
    {
      "epoch": 11.799929552659387,
      "grad_norm": 5.836354732513428,
      "learning_rate": 8.336268639192204e-07,
      "loss": 0.4713,
      "step": 737000
    },
    {
      "epoch": 11.807934932274488,
      "grad_norm": 5.177491664886475,
      "learning_rate": 8.002711155229648e-07,
      "loss": 0.4616,
      "step": 737500
    },
    {
      "epoch": 11.81594031188959,
      "grad_norm": 4.612493991851807,
      "learning_rate": 7.669153671267091e-07,
      "loss": 0.4567,
      "step": 738000
    },
    {
      "epoch": 11.823945691504692,
      "grad_norm": 4.43789529800415,
      "learning_rate": 7.335596187304536e-07,
      "loss": 0.462,
      "step": 738500
    },
    {
      "epoch": 11.831951071119793,
      "grad_norm": 5.048721790313721,
      "learning_rate": 7.00203870334198e-07,
      "loss": 0.4422,
      "step": 739000
    },
    {
      "epoch": 11.839956450734894,
      "grad_norm": 2.373218059539795,
      "learning_rate": 6.668481219379423e-07,
      "loss": 0.4666,
      "step": 739500
    },
    {
      "epoch": 11.847961830349995,
      "grad_norm": 2.8881359100341797,
      "learning_rate": 6.334923735416867e-07,
      "loss": 0.4625,
      "step": 740000
    },
    {
      "epoch": 11.855967209965097,
      "grad_norm": 17.842750549316406,
      "learning_rate": 6.001366251454311e-07,
      "loss": 0.4738,
      "step": 740500
    },
    {
      "epoch": 11.863972589580198,
      "grad_norm": 2.6128969192504883,
      "learning_rate": 5.667808767491754e-07,
      "loss": 0.4401,
      "step": 741000
    },
    {
      "epoch": 11.8719779691953,
      "grad_norm": 3.790968179702759,
      "learning_rate": 5.334251283529198e-07,
      "loss": 0.4584,
      "step": 741500
    },
    {
      "epoch": 11.8799833488104,
      "grad_norm": 4.096737861633301,
      "learning_rate": 5.000693799566642e-07,
      "loss": 0.4724,
      "step": 742000
    },
    {
      "epoch": 11.887988728425501,
      "grad_norm": 8.614896774291992,
      "learning_rate": 4.667136315604086e-07,
      "loss": 0.4574,
      "step": 742500
    },
    {
      "epoch": 11.895994108040604,
      "grad_norm": 2.55814266204834,
      "learning_rate": 4.3335788316415303e-07,
      "loss": 0.4832,
      "step": 743000
    },
    {
      "epoch": 11.903999487655705,
      "grad_norm": 1.9235997200012207,
      "learning_rate": 4.0000213476789735e-07,
      "loss": 0.465,
      "step": 743500
    },
    {
      "epoch": 11.912004867270806,
      "grad_norm": 2.985459327697754,
      "learning_rate": 3.666463863716418e-07,
      "loss": 0.4569,
      "step": 744000
    },
    {
      "epoch": 11.920010246885907,
      "grad_norm": 6.596078872680664,
      "learning_rate": 3.3329063797538615e-07,
      "loss": 0.4604,
      "step": 744500
    },
    {
      "epoch": 11.928015626501008,
      "grad_norm": 3.555687427520752,
      "learning_rate": 2.999348895791305e-07,
      "loss": 0.4566,
      "step": 745000
    },
    {
      "epoch": 11.93602100611611,
      "grad_norm": 5.632309436798096,
      "learning_rate": 2.665791411828749e-07,
      "loss": 0.4657,
      "step": 745500
    },
    {
      "epoch": 11.944026385731211,
      "grad_norm": 2.49829363822937,
      "learning_rate": 2.332233927866193e-07,
      "loss": 0.4645,
      "step": 746000
    },
    {
      "epoch": 11.952031765346312,
      "grad_norm": 2.8398094177246094,
      "learning_rate": 1.9986764439036366e-07,
      "loss": 0.4436,
      "step": 746500
    },
    {
      "epoch": 11.960037144961413,
      "grad_norm": 2.315342426300049,
      "learning_rate": 1.6651189599410803e-07,
      "loss": 0.4637,
      "step": 747000
    },
    {
      "epoch": 11.968042524576516,
      "grad_norm": 1.9280670881271362,
      "learning_rate": 1.3315614759785243e-07,
      "loss": 0.4705,
      "step": 747500
    },
    {
      "epoch": 11.976047904191617,
      "grad_norm": 4.379947662353516,
      "learning_rate": 9.98003992015968e-08,
      "loss": 0.4647,
      "step": 748000
    },
    {
      "epoch": 11.984053283806718,
      "grad_norm": 2.8935697078704834,
      "learning_rate": 6.644465080534119e-08,
      "loss": 0.4425,
      "step": 748500
    },
    {
      "epoch": 11.992058663421819,
      "grad_norm": 2.275716543197632,
      "learning_rate": 3.3088902409085575e-08,
      "loss": 0.457,
      "step": 749000
    },
    {
      "epoch": 11.999487680312516,
      "grad_norm": 2.252871036529541,
      "learning_rate": 7.144686856026732e-06,
      "loss": 0.2514,
      "step": 749500
    },
    {
      "epoch": 12.007492675429468,
      "grad_norm": 5.275096893310547,
      "learning_rate": 7.1160975877519e-06,
      "loss": 0.5127,
      "step": 750000
    },
    {
      "epoch": 12.015497670546422,
      "grad_norm": 5.4905195236206055,
      "learning_rate": 7.087508319477068e-06,
      "loss": 0.5084,
      "step": 750500
    },
    {
      "epoch": 12.023502665663374,
      "grad_norm": 3.2492966651916504,
      "learning_rate": 7.058919051202237e-06,
      "loss": 0.4994,
      "step": 751000
    },
    {
      "epoch": 12.031507660780328,
      "grad_norm": 3.9021596908569336,
      "learning_rate": 7.030329782927405e-06,
      "loss": 0.4953,
      "step": 751500
    },
    {
      "epoch": 12.03951265589728,
      "grad_norm": 6.368383407592773,
      "learning_rate": 7.0017405146525716e-06,
      "loss": 0.5188,
      "step": 752000
    },
    {
      "epoch": 12.047517651014234,
      "grad_norm": 4.213268756866455,
      "learning_rate": 6.97315124637774e-06,
      "loss": 0.5131,
      "step": 752500
    },
    {
      "epoch": 12.055522646131186,
      "grad_norm": 5.439957618713379,
      "learning_rate": 6.944561978102908e-06,
      "loss": 0.4952,
      "step": 753000
    },
    {
      "epoch": 12.06352764124814,
      "grad_norm": 5.0169854164123535,
      "learning_rate": 6.915972709828075e-06,
      "loss": 0.4836,
      "step": 753500
    },
    {
      "epoch": 12.071532636365092,
      "grad_norm": 6.658423900604248,
      "learning_rate": 6.887383441553244e-06,
      "loss": 0.4997,
      "step": 754000
    },
    {
      "epoch": 12.079537631482046,
      "grad_norm": 1.694502353668213,
      "learning_rate": 6.8587941732784115e-06,
      "loss": 0.5025,
      "step": 754500
    },
    {
      "epoch": 12.087542626598998,
      "grad_norm": 3.0809669494628906,
      "learning_rate": 6.83020490500358e-06,
      "loss": 0.4947,
      "step": 755000
    },
    {
      "epoch": 12.095547621715951,
      "grad_norm": 2.560542106628418,
      "learning_rate": 6.801615636728748e-06,
      "loss": 0.4719,
      "step": 755500
    },
    {
      "epoch": 12.103552616832904,
      "grad_norm": 3.0311100482940674,
      "learning_rate": 6.773026368453915e-06,
      "loss": 0.4964,
      "step": 756000
    },
    {
      "epoch": 12.111557611949857,
      "grad_norm": 2.6034305095672607,
      "learning_rate": 6.744437100179084e-06,
      "loss": 0.4979,
      "step": 756500
    },
    {
      "epoch": 12.11956260706681,
      "grad_norm": 7.889179229736328,
      "learning_rate": 6.715847831904251e-06,
      "loss": 0.5017,
      "step": 757000
    },
    {
      "epoch": 12.127567602183763,
      "grad_norm": 2.8547005653381348,
      "learning_rate": 6.68725856362942e-06,
      "loss": 0.4965,
      "step": 757500
    },
    {
      "epoch": 12.135572597300715,
      "grad_norm": 9.24945068359375,
      "learning_rate": 6.658669295354587e-06,
      "loss": 0.4798,
      "step": 758000
    },
    {
      "epoch": 12.14357759241767,
      "grad_norm": 6.368488788604736,
      "learning_rate": 6.630080027079755e-06,
      "loss": 0.4926,
      "step": 758500
    },
    {
      "epoch": 12.151582587534621,
      "grad_norm": 7.512279987335205,
      "learning_rate": 6.6014907588049235e-06,
      "loss": 0.5063,
      "step": 759000
    },
    {
      "epoch": 12.159587582651575,
      "grad_norm": 6.673985481262207,
      "learning_rate": 6.572901490530091e-06,
      "loss": 0.5091,
      "step": 759500
    },
    {
      "epoch": 12.167592577768527,
      "grad_norm": 5.019932746887207,
      "learning_rate": 6.54431222225526e-06,
      "loss": 0.498,
      "step": 760000
    },
    {
      "epoch": 12.175597572885481,
      "grad_norm": 3.9352540969848633,
      "learning_rate": 6.515722953980427e-06,
      "loss": 0.5063,
      "step": 760500
    },
    {
      "epoch": 12.183602568002433,
      "grad_norm": 4.431123733520508,
      "learning_rate": 6.487133685705595e-06,
      "loss": 0.5037,
      "step": 761000
    },
    {
      "epoch": 12.191607563119387,
      "grad_norm": 4.234749794006348,
      "learning_rate": 6.4585444174307634e-06,
      "loss": 0.4885,
      "step": 761500
    },
    {
      "epoch": 12.199612558236339,
      "grad_norm": 2.334221124649048,
      "learning_rate": 6.42995514915593e-06,
      "loss": 0.4863,
      "step": 762000
    },
    {
      "epoch": 12.207617553353293,
      "grad_norm": 6.4740095138549805,
      "learning_rate": 6.401365880881098e-06,
      "loss": 0.4826,
      "step": 762500
    },
    {
      "epoch": 12.215622548470245,
      "grad_norm": 8.273085594177246,
      "learning_rate": 6.372776612606267e-06,
      "loss": 0.5096,
      "step": 763000
    },
    {
      "epoch": 12.223627543587199,
      "grad_norm": 3.60676646232605,
      "learning_rate": 6.344187344331434e-06,
      "loss": 0.4957,
      "step": 763500
    },
    {
      "epoch": 12.231632538704151,
      "grad_norm": 5.643250942230225,
      "learning_rate": 6.315598076056603e-06,
      "loss": 0.5025,
      "step": 764000
    },
    {
      "epoch": 12.239637533821105,
      "grad_norm": 2.25152587890625,
      "learning_rate": 6.28700880778177e-06,
      "loss": 0.5014,
      "step": 764500
    },
    {
      "epoch": 12.247642528938057,
      "grad_norm": 8.788591384887695,
      "learning_rate": 6.258419539506938e-06,
      "loss": 0.5042,
      "step": 765000
    },
    {
      "epoch": 12.25564752405501,
      "grad_norm": 3.181335210800171,
      "learning_rate": 6.229830271232107e-06,
      "loss": 0.5226,
      "step": 765500
    },
    {
      "epoch": 12.263652519171963,
      "grad_norm": 4.77073860168457,
      "learning_rate": 6.201241002957274e-06,
      "loss": 0.5186,
      "step": 766000
    },
    {
      "epoch": 12.271657514288917,
      "grad_norm": 2.9578332901000977,
      "learning_rate": 6.172651734682442e-06,
      "loss": 0.5143,
      "step": 766500
    },
    {
      "epoch": 12.279662509405869,
      "grad_norm": 4.14086389541626,
      "learning_rate": 6.14406246640761e-06,
      "loss": 0.4985,
      "step": 767000
    },
    {
      "epoch": 12.287667504522823,
      "grad_norm": 3.246919631958008,
      "learning_rate": 6.115473198132778e-06,
      "loss": 0.4916,
      "step": 767500
    },
    {
      "epoch": 12.295672499639775,
      "grad_norm": 3.5562918186187744,
      "learning_rate": 6.086883929857946e-06,
      "loss": 0.4943,
      "step": 768000
    },
    {
      "epoch": 12.303677494756728,
      "grad_norm": 4.942566871643066,
      "learning_rate": 6.058294661583114e-06,
      "loss": 0.4989,
      "step": 768500
    },
    {
      "epoch": 12.31168248987368,
      "grad_norm": 2.9978537559509277,
      "learning_rate": 6.0297053933082815e-06,
      "loss": 0.503,
      "step": 769000
    },
    {
      "epoch": 12.319687484990634,
      "grad_norm": 2.816415309906006,
      "learning_rate": 6.00111612503345e-06,
      "loss": 0.4884,
      "step": 769500
    },
    {
      "epoch": 12.327692480107586,
      "grad_norm": 5.239892959594727,
      "learning_rate": 5.972526856758617e-06,
      "loss": 0.4994,
      "step": 770000
    },
    {
      "epoch": 12.33569747522454,
      "grad_norm": 3.875899076461792,
      "learning_rate": 5.943937588483786e-06,
      "loss": 0.5145,
      "step": 770500
    },
    {
      "epoch": 12.343702470341492,
      "grad_norm": 3.5680577754974365,
      "learning_rate": 5.915348320208954e-06,
      "loss": 0.4836,
      "step": 771000
    },
    {
      "epoch": 12.351707465458446,
      "grad_norm": 5.557962894439697,
      "learning_rate": 5.886759051934121e-06,
      "loss": 0.5002,
      "step": 771500
    },
    {
      "epoch": 12.359712460575398,
      "grad_norm": 4.212343215942383,
      "learning_rate": 5.85816978365929e-06,
      "loss": 0.4844,
      "step": 772000
    },
    {
      "epoch": 12.367717455692352,
      "grad_norm": 2.410543203353882,
      "learning_rate": 5.829580515384457e-06,
      "loss": 0.4986,
      "step": 772500
    },
    {
      "epoch": 12.375722450809304,
      "grad_norm": 2.8368823528289795,
      "learning_rate": 5.800991247109625e-06,
      "loss": 0.4905,
      "step": 773000
    },
    {
      "epoch": 12.383727445926258,
      "grad_norm": 16.605588912963867,
      "learning_rate": 5.7724019788347935e-06,
      "loss": 0.4907,
      "step": 773500
    },
    {
      "epoch": 12.39173244104321,
      "grad_norm": 5.180511474609375,
      "learning_rate": 5.743812710559961e-06,
      "loss": 0.4989,
      "step": 774000
    },
    {
      "epoch": 12.399737436160164,
      "grad_norm": 2.964442014694214,
      "learning_rate": 5.715223442285129e-06,
      "loss": 0.4893,
      "step": 774500
    },
    {
      "epoch": 12.407742431277116,
      "grad_norm": 3.8749372959136963,
      "learning_rate": 5.686634174010297e-06,
      "loss": 0.4959,
      "step": 775000
    },
    {
      "epoch": 12.41574742639407,
      "grad_norm": 2.7342443466186523,
      "learning_rate": 5.658044905735465e-06,
      "loss": 0.4997,
      "step": 775500
    },
    {
      "epoch": 12.423752421511022,
      "grad_norm": 6.1809306144714355,
      "learning_rate": 5.629455637460633e-06,
      "loss": 0.5024,
      "step": 776000
    },
    {
      "epoch": 12.431757416627976,
      "grad_norm": 7.99453592300415,
      "learning_rate": 5.6008663691858e-06,
      "loss": 0.5012,
      "step": 776500
    },
    {
      "epoch": 12.43976241174493,
      "grad_norm": 2.669306516647339,
      "learning_rate": 5.572277100910968e-06,
      "loss": 0.499,
      "step": 777000
    },
    {
      "epoch": 12.447767406861882,
      "grad_norm": 4.07491397857666,
      "learning_rate": 5.543687832636137e-06,
      "loss": 0.4812,
      "step": 777500
    },
    {
      "epoch": 12.455772401978836,
      "grad_norm": 6.057444095611572,
      "learning_rate": 5.515098564361305e-06,
      "loss": 0.4863,
      "step": 778000
    },
    {
      "epoch": 12.463777397095788,
      "grad_norm": 3.9741082191467285,
      "learning_rate": 5.4865092960864725e-06,
      "loss": 0.4903,
      "step": 778500
    },
    {
      "epoch": 12.471782392212742,
      "grad_norm": 4.1771559715271,
      "learning_rate": 5.45792002781164e-06,
      "loss": 0.4993,
      "step": 779000
    },
    {
      "epoch": 12.479787387329694,
      "grad_norm": 2.95383882522583,
      "learning_rate": 5.429330759536808e-06,
      "loss": 0.4964,
      "step": 779500
    },
    {
      "epoch": 12.487792382446647,
      "grad_norm": 3.434295892715454,
      "learning_rate": 5.400741491261977e-06,
      "loss": 0.5008,
      "step": 780000
    },
    {
      "epoch": 12.4957973775636,
      "grad_norm": 3.732835531234741,
      "learning_rate": 5.372152222987145e-06,
      "loss": 0.4836,
      "step": 780500
    },
    {
      "epoch": 12.503802372680553,
      "grad_norm": 2.545252799987793,
      "learning_rate": 5.3435629547123116e-06,
      "loss": 0.4877,
      "step": 781000
    },
    {
      "epoch": 12.511807367797505,
      "grad_norm": 3.1210455894470215,
      "learning_rate": 5.31497368643748e-06,
      "loss": 0.5189,
      "step": 781500
    },
    {
      "epoch": 12.51981236291446,
      "grad_norm": 1.3316560983657837,
      "learning_rate": 5.286384418162648e-06,
      "loss": 0.4983,
      "step": 782000
    },
    {
      "epoch": 12.527817358031411,
      "grad_norm": 4.409449100494385,
      "learning_rate": 5.257795149887816e-06,
      "loss": 0.5097,
      "step": 782500
    },
    {
      "epoch": 12.535822353148365,
      "grad_norm": 4.935122966766357,
      "learning_rate": 5.2292058816129845e-06,
      "loss": 0.4807,
      "step": 783000
    },
    {
      "epoch": 12.543827348265317,
      "grad_norm": 4.651098251342773,
      "learning_rate": 5.2006166133381515e-06,
      "loss": 0.4834,
      "step": 783500
    },
    {
      "epoch": 12.551832343382271,
      "grad_norm": 3.8669676780700684,
      "learning_rate": 5.172027345063319e-06,
      "loss": 0.4765,
      "step": 784000
    },
    {
      "epoch": 12.559837338499223,
      "grad_norm": 4.553332805633545,
      "learning_rate": 5.143438076788488e-06,
      "loss": 0.5042,
      "step": 784500
    },
    {
      "epoch": 12.567842333616177,
      "grad_norm": 4.876225471496582,
      "learning_rate": 5.114848808513656e-06,
      "loss": 0.4994,
      "step": 785000
    },
    {
      "epoch": 12.57584732873313,
      "grad_norm": 3.1707282066345215,
      "learning_rate": 5.086259540238824e-06,
      "loss": 0.4874,
      "step": 785500
    },
    {
      "epoch": 12.583852323850083,
      "grad_norm": 9.827709197998047,
      "learning_rate": 5.057670271963991e-06,
      "loss": 0.4961,
      "step": 786000
    },
    {
      "epoch": 12.591857318967035,
      "grad_norm": 4.126626968383789,
      "learning_rate": 5.029081003689159e-06,
      "loss": 0.4838,
      "step": 786500
    },
    {
      "epoch": 12.599862314083989,
      "grad_norm": 6.683156490325928,
      "learning_rate": 5.000491735414328e-06,
      "loss": 0.4923,
      "step": 787000
    },
    {
      "epoch": 12.607867309200941,
      "grad_norm": 2.2273178100585938,
      "learning_rate": 4.971902467139495e-06,
      "loss": 0.5088,
      "step": 787500
    },
    {
      "epoch": 12.615872304317895,
      "grad_norm": 6.816894054412842,
      "learning_rate": 4.943313198864663e-06,
      "loss": 0.4783,
      "step": 788000
    },
    {
      "epoch": 12.623877299434847,
      "grad_norm": 3.3452281951904297,
      "learning_rate": 4.914723930589831e-06,
      "loss": 0.5097,
      "step": 788500
    },
    {
      "epoch": 12.6318822945518,
      "grad_norm": 4.151259899139404,
      "learning_rate": 4.886134662314999e-06,
      "loss": 0.5051,
      "step": 789000
    },
    {
      "epoch": 12.639887289668753,
      "grad_norm": 2.9240286350250244,
      "learning_rate": 4.857545394040167e-06,
      "loss": 0.4883,
      "step": 789500
    },
    {
      "epoch": 12.647892284785707,
      "grad_norm": 2.7823731899261475,
      "learning_rate": 4.828956125765335e-06,
      "loss": 0.502,
      "step": 790000
    },
    {
      "epoch": 12.655897279902659,
      "grad_norm": 2.958662271499634,
      "learning_rate": 4.8003668574905026e-06,
      "loss": 0.4813,
      "step": 790500
    },
    {
      "epoch": 12.663902275019613,
      "grad_norm": 2.2671077251434326,
      "learning_rate": 4.771777589215671e-06,
      "loss": 0.5033,
      "step": 791000
    },
    {
      "epoch": 12.671907270136565,
      "grad_norm": 2.6505565643310547,
      "learning_rate": 4.743188320940839e-06,
      "loss": 0.4832,
      "step": 791500
    },
    {
      "epoch": 12.679912265253519,
      "grad_norm": 6.705207347869873,
      "learning_rate": 4.714599052666006e-06,
      "loss": 0.5052,
      "step": 792000
    },
    {
      "epoch": 12.68791726037047,
      "grad_norm": 3.7397265434265137,
      "learning_rate": 4.686009784391175e-06,
      "loss": 0.4972,
      "step": 792500
    },
    {
      "epoch": 12.695922255487424,
      "grad_norm": 3.5327951908111572,
      "learning_rate": 4.6574205161163425e-06,
      "loss": 0.4914,
      "step": 793000
    },
    {
      "epoch": 12.703927250604377,
      "grad_norm": 6.0367913246154785,
      "learning_rate": 4.62883124784151e-06,
      "loss": 0.5027,
      "step": 793500
    },
    {
      "epoch": 12.71193224572133,
      "grad_norm": 1.342130422592163,
      "learning_rate": 4.600241979566679e-06,
      "loss": 0.4867,
      "step": 794000
    },
    {
      "epoch": 12.719937240838282,
      "grad_norm": 5.889947414398193,
      "learning_rate": 4.571652711291846e-06,
      "loss": 0.4935,
      "step": 794500
    },
    {
      "epoch": 12.727942235955236,
      "grad_norm": 4.471872806549072,
      "learning_rate": 4.543063443017015e-06,
      "loss": 0.4975,
      "step": 795000
    },
    {
      "epoch": 12.735947231072188,
      "grad_norm": 5.23482608795166,
      "learning_rate": 4.514474174742182e-06,
      "loss": 0.5012,
      "step": 795500
    },
    {
      "epoch": 12.743952226189142,
      "grad_norm": 5.566445827484131,
      "learning_rate": 4.48588490646735e-06,
      "loss": 0.5014,
      "step": 796000
    },
    {
      "epoch": 12.751957221306094,
      "grad_norm": 1.3449883460998535,
      "learning_rate": 4.457295638192518e-06,
      "loss": 0.4909,
      "step": 796500
    },
    {
      "epoch": 12.759962216423048,
      "grad_norm": 4.23496150970459,
      "learning_rate": 4.428706369917686e-06,
      "loss": 0.4956,
      "step": 797000
    },
    {
      "epoch": 12.76796721154,
      "grad_norm": 2.9971823692321777,
      "learning_rate": 4.400117101642854e-06,
      "loss": 0.4933,
      "step": 797500
    },
    {
      "epoch": 12.775972206656954,
      "grad_norm": 2.645487070083618,
      "learning_rate": 4.371527833368022e-06,
      "loss": 0.4839,
      "step": 798000
    },
    {
      "epoch": 12.783977201773906,
      "grad_norm": 5.421579837799072,
      "learning_rate": 4.342938565093189e-06,
      "loss": 0.487,
      "step": 798500
    },
    {
      "epoch": 12.79198219689086,
      "grad_norm": 3.566415548324585,
      "learning_rate": 4.314349296818358e-06,
      "loss": 0.4914,
      "step": 799000
    },
    {
      "epoch": 12.799987192007812,
      "grad_norm": 2.0729153156280518,
      "learning_rate": 4.285760028543526e-06,
      "loss": 0.4908,
      "step": 799500
    },
    {
      "epoch": 12.807992187124766,
      "grad_norm": 11.286977767944336,
      "learning_rate": 4.2571707602686936e-06,
      "loss": 0.4804,
      "step": 800000
    },
    {
      "epoch": 12.815997182241718,
      "grad_norm": 2.9973301887512207,
      "learning_rate": 4.228581491993862e-06,
      "loss": 0.4937,
      "step": 800500
    },
    {
      "epoch": 12.824002177358672,
      "grad_norm": 3.281705141067505,
      "learning_rate": 4.199992223719029e-06,
      "loss": 0.4736,
      "step": 801000
    },
    {
      "epoch": 12.832007172475624,
      "grad_norm": 4.695652484893799,
      "learning_rate": 4.171402955444197e-06,
      "loss": 0.4955,
      "step": 801500
    },
    {
      "epoch": 12.840012167592578,
      "grad_norm": 10.333148002624512,
      "learning_rate": 4.142813687169366e-06,
      "loss": 0.4848,
      "step": 802000
    },
    {
      "epoch": 12.848017162709532,
      "grad_norm": 3.6028733253479004,
      "learning_rate": 4.1142244188945335e-06,
      "loss": 0.4923,
      "step": 802500
    },
    {
      "epoch": 12.856022157826484,
      "grad_norm": 6.071239948272705,
      "learning_rate": 4.085635150619701e-06,
      "loss": 0.4886,
      "step": 803000
    },
    {
      "epoch": 12.864027152943436,
      "grad_norm": 3.8797051906585693,
      "learning_rate": 4.057045882344869e-06,
      "loss": 0.4999,
      "step": 803500
    },
    {
      "epoch": 12.87203214806039,
      "grad_norm": 4.059276103973389,
      "learning_rate": 4.028456614070037e-06,
      "loss": 0.4952,
      "step": 804000
    },
    {
      "epoch": 12.880037143177343,
      "grad_norm": 2.4033586978912354,
      "learning_rate": 3.999867345795206e-06,
      "loss": 0.5122,
      "step": 804500
    },
    {
      "epoch": 12.888042138294296,
      "grad_norm": 3.4428200721740723,
      "learning_rate": 3.9712780775203726e-06,
      "loss": 0.4875,
      "step": 805000
    },
    {
      "epoch": 12.89604713341125,
      "grad_norm": 3.113152027130127,
      "learning_rate": 3.94268880924554e-06,
      "loss": 0.4939,
      "step": 805500
    },
    {
      "epoch": 12.904052128528201,
      "grad_norm": 3.14469575881958,
      "learning_rate": 3.914099540970709e-06,
      "loss": 0.4912,
      "step": 806000
    },
    {
      "epoch": 12.912057123645155,
      "grad_norm": 3.988973379135132,
      "learning_rate": 3.885510272695877e-06,
      "loss": 0.4976,
      "step": 806500
    },
    {
      "epoch": 12.920062118762107,
      "grad_norm": 4.943741321563721,
      "learning_rate": 3.856921004421045e-06,
      "loss": 0.4935,
      "step": 807000
    },
    {
      "epoch": 12.928067113879061,
      "grad_norm": 4.053717613220215,
      "learning_rate": 3.8283317361462125e-06,
      "loss": 0.5054,
      "step": 807500
    },
    {
      "epoch": 12.936072108996013,
      "grad_norm": 2.9307796955108643,
      "learning_rate": 3.7997424678713803e-06,
      "loss": 0.4862,
      "step": 808000
    },
    {
      "epoch": 12.944077104112967,
      "grad_norm": 5.447963237762451,
      "learning_rate": 3.7711531995965485e-06,
      "loss": 0.4809,
      "step": 808500
    },
    {
      "epoch": 12.95208209922992,
      "grad_norm": 3.827711343765259,
      "learning_rate": 3.7425639313217168e-06,
      "loss": 0.482,
      "step": 809000
    },
    {
      "epoch": 12.960087094346873,
      "grad_norm": 5.937821388244629,
      "learning_rate": 3.713974663046884e-06,
      "loss": 0.4857,
      "step": 809500
    },
    {
      "epoch": 12.968092089463825,
      "grad_norm": 4.630011558532715,
      "learning_rate": 3.685385394772052e-06,
      "loss": 0.475,
      "step": 810000
    },
    {
      "epoch": 12.976097084580779,
      "grad_norm": 4.6418843269348145,
      "learning_rate": 3.65679612649722e-06,
      "loss": 0.5089,
      "step": 810500
    },
    {
      "epoch": 12.984102079697731,
      "grad_norm": 2.946106433868408,
      "learning_rate": 3.6282068582223884e-06,
      "loss": 0.5099,
      "step": 811000
    },
    {
      "epoch": 12.992107074814685,
      "grad_norm": 1.8937315940856934,
      "learning_rate": 3.5996175899475563e-06,
      "loss": 0.482,
      "step": 811500
    },
    {
      "epoch": 13.000112069931637,
      "grad_norm": 8.21384048461914,
      "learning_rate": 3.5710283216727236e-06,
      "loss": 0.5069,
      "step": 812000
    },
    {
      "epoch": 13.00811706504859,
      "grad_norm": 4.2646708488464355,
      "learning_rate": 3.542439053397892e-06,
      "loss": 0.4662,
      "step": 812500
    },
    {
      "epoch": 13.016122060165543,
      "grad_norm": 2.647871255874634,
      "learning_rate": 3.51384978512306e-06,
      "loss": 0.4814,
      "step": 813000
    },
    {
      "epoch": 13.024127055282497,
      "grad_norm": 1.360458254814148,
      "learning_rate": 3.485260516848228e-06,
      "loss": 0.4675,
      "step": 813500
    },
    {
      "epoch": 13.032132050399449,
      "grad_norm": 4.096100330352783,
      "learning_rate": 3.4566712485733953e-06,
      "loss": 0.4858,
      "step": 814000
    },
    {
      "epoch": 13.040137045516403,
      "grad_norm": 3.4160118103027344,
      "learning_rate": 3.4280819802985636e-06,
      "loss": 0.4707,
      "step": 814500
    },
    {
      "epoch": 13.048142040633355,
      "grad_norm": 5.449665546417236,
      "learning_rate": 3.399492712023732e-06,
      "loss": 0.4785,
      "step": 815000
    },
    {
      "epoch": 13.056147035750309,
      "grad_norm": 7.639679908752441,
      "learning_rate": 3.3709034437488996e-06,
      "loss": 0.5028,
      "step": 815500
    },
    {
      "epoch": 13.06415203086726,
      "grad_norm": 5.235229969024658,
      "learning_rate": 3.342314175474067e-06,
      "loss": 0.4767,
      "step": 816000
    },
    {
      "epoch": 13.072157025984215,
      "grad_norm": 21.077198028564453,
      "learning_rate": 3.3137249071992352e-06,
      "loss": 0.4911,
      "step": 816500
    },
    {
      "epoch": 13.080162021101167,
      "grad_norm": 2.0332887172698975,
      "learning_rate": 3.285135638924403e-06,
      "loss": 0.4774,
      "step": 817000
    },
    {
      "epoch": 13.08816701621812,
      "grad_norm": 2.504934310913086,
      "learning_rate": 3.2565463706495713e-06,
      "loss": 0.4898,
      "step": 817500
    },
    {
      "epoch": 13.096172011335073,
      "grad_norm": 3.3741111755371094,
      "learning_rate": 3.2279571023747395e-06,
      "loss": 0.4909,
      "step": 818000
    },
    {
      "epoch": 13.104177006452026,
      "grad_norm": 3.9555306434631348,
      "learning_rate": 3.199367834099907e-06,
      "loss": 0.4765,
      "step": 818500
    },
    {
      "epoch": 13.112182001568979,
      "grad_norm": 2.1088061332702637,
      "learning_rate": 3.1707785658250747e-06,
      "loss": 0.4883,
      "step": 819000
    },
    {
      "epoch": 13.120186996685932,
      "grad_norm": 3.9936981201171875,
      "learning_rate": 3.142189297550243e-06,
      "loss": 0.4706,
      "step": 819500
    },
    {
      "epoch": 13.128191991802884,
      "grad_norm": 4.885108947753906,
      "learning_rate": 3.1136000292754108e-06,
      "loss": 0.4862,
      "step": 820000
    },
    {
      "epoch": 13.136196986919838,
      "grad_norm": 7.195546627044678,
      "learning_rate": 3.085010761000579e-06,
      "loss": 0.4528,
      "step": 820500
    },
    {
      "epoch": 13.14420198203679,
      "grad_norm": 4.907655715942383,
      "learning_rate": 3.0564214927257464e-06,
      "loss": 0.4693,
      "step": 821000
    },
    {
      "epoch": 13.152206977153744,
      "grad_norm": 4.871784687042236,
      "learning_rate": 3.0278322244509147e-06,
      "loss": 0.5026,
      "step": 821500
    },
    {
      "epoch": 13.160211972270696,
      "grad_norm": 6.813912391662598,
      "learning_rate": 2.9992429561760825e-06,
      "loss": 0.4685,
      "step": 822000
    },
    {
      "epoch": 13.16821696738765,
      "grad_norm": 4.239894390106201,
      "learning_rate": 2.9706536879012507e-06,
      "loss": 0.4744,
      "step": 822500
    },
    {
      "epoch": 13.176221962504602,
      "grad_norm": 3.2981443405151367,
      "learning_rate": 2.9420644196264185e-06,
      "loss": 0.4763,
      "step": 823000
    },
    {
      "epoch": 13.184226957621556,
      "grad_norm": 4.340000629425049,
      "learning_rate": 2.9134751513515863e-06,
      "loss": 0.4807,
      "step": 823500
    },
    {
      "epoch": 13.192231952738508,
      "grad_norm": 8.651801109313965,
      "learning_rate": 2.8848858830767546e-06,
      "loss": 0.4556,
      "step": 824000
    },
    {
      "epoch": 13.200236947855462,
      "grad_norm": 2.7075448036193848,
      "learning_rate": 2.8562966148019224e-06,
      "loss": 0.4787,
      "step": 824500
    },
    {
      "epoch": 13.208241942972414,
      "grad_norm": 4.086001873016357,
      "learning_rate": 2.82770734652709e-06,
      "loss": 0.4894,
      "step": 825000
    },
    {
      "epoch": 13.216246938089368,
      "grad_norm": 4.643930435180664,
      "learning_rate": 2.799118078252258e-06,
      "loss": 0.487,
      "step": 825500
    },
    {
      "epoch": 13.22425193320632,
      "grad_norm": 3.0239195823669434,
      "learning_rate": 2.7705288099774262e-06,
      "loss": 0.4761,
      "step": 826000
    },
    {
      "epoch": 13.232256928323274,
      "grad_norm": 2.969630241394043,
      "learning_rate": 2.741939541702594e-06,
      "loss": 0.4724,
      "step": 826500
    },
    {
      "epoch": 13.240261923440226,
      "grad_norm": 6.132841110229492,
      "learning_rate": 2.713350273427762e-06,
      "loss": 0.463,
      "step": 827000
    },
    {
      "epoch": 13.24826691855718,
      "grad_norm": 4.248220920562744,
      "learning_rate": 2.6847610051529297e-06,
      "loss": 0.4565,
      "step": 827500
    },
    {
      "epoch": 13.256271913674132,
      "grad_norm": 9.4950590133667,
      "learning_rate": 2.656171736878098e-06,
      "loss": 0.4698,
      "step": 828000
    },
    {
      "epoch": 13.264276908791086,
      "grad_norm": 6.789925575256348,
      "learning_rate": 2.6275824686032657e-06,
      "loss": 0.4704,
      "step": 828500
    },
    {
      "epoch": 13.272281903908038,
      "grad_norm": 6.251099586486816,
      "learning_rate": 2.5989932003284336e-06,
      "loss": 0.4828,
      "step": 829000
    },
    {
      "epoch": 13.280286899024992,
      "grad_norm": 2.6357898712158203,
      "learning_rate": 2.570403932053602e-06,
      "loss": 0.4884,
      "step": 829500
    },
    {
      "epoch": 13.288291894141945,
      "grad_norm": 3.8712549209594727,
      "learning_rate": 2.5418146637787696e-06,
      "loss": 0.4646,
      "step": 830000
    },
    {
      "epoch": 13.296296889258898,
      "grad_norm": 7.499981880187988,
      "learning_rate": 2.5132253955039374e-06,
      "loss": 0.4894,
      "step": 830500
    },
    {
      "epoch": 13.304301884375851,
      "grad_norm": 6.54636812210083,
      "learning_rate": 2.4846361272291052e-06,
      "loss": 0.4583,
      "step": 831000
    },
    {
      "epoch": 13.312306879492803,
      "grad_norm": 5.951048374176025,
      "learning_rate": 2.4560468589542735e-06,
      "loss": 0.4816,
      "step": 831500
    },
    {
      "epoch": 13.320311874609757,
      "grad_norm": 7.141109943389893,
      "learning_rate": 2.4274575906794413e-06,
      "loss": 0.478,
      "step": 832000
    },
    {
      "epoch": 13.32831686972671,
      "grad_norm": 4.7392191886901855,
      "learning_rate": 2.398868322404609e-06,
      "loss": 0.4805,
      "step": 832500
    },
    {
      "epoch": 13.336321864843663,
      "grad_norm": 6.788608551025391,
      "learning_rate": 2.370279054129777e-06,
      "loss": 0.4839,
      "step": 833000
    },
    {
      "epoch": 13.344326859960615,
      "grad_norm": 3.938490390777588,
      "learning_rate": 2.341689785854945e-06,
      "loss": 0.4817,
      "step": 833500
    },
    {
      "epoch": 13.35233185507757,
      "grad_norm": 2.838498830795288,
      "learning_rate": 2.313100517580113e-06,
      "loss": 0.4784,
      "step": 834000
    },
    {
      "epoch": 13.360336850194521,
      "grad_norm": 4.988091945648193,
      "learning_rate": 2.2845112493052808e-06,
      "loss": 0.4777,
      "step": 834500
    },
    {
      "epoch": 13.368341845311475,
      "grad_norm": 9.036595344543457,
      "learning_rate": 2.255921981030449e-06,
      "loss": 0.4757,
      "step": 835000
    },
    {
      "epoch": 13.376346840428427,
      "grad_norm": 1.9822568893432617,
      "learning_rate": 2.227332712755617e-06,
      "loss": 0.4632,
      "step": 835500
    },
    {
      "epoch": 13.384351835545381,
      "grad_norm": 10.910148620605469,
      "learning_rate": 2.1987434444807846e-06,
      "loss": 0.4797,
      "step": 836000
    },
    {
      "epoch": 13.392356830662333,
      "grad_norm": 8.14113712310791,
      "learning_rate": 2.1701541762059525e-06,
      "loss": 0.473,
      "step": 836500
    },
    {
      "epoch": 13.400361825779287,
      "grad_norm": 4.444918155670166,
      "learning_rate": 2.1415649079311207e-06,
      "loss": 0.473,
      "step": 837000
    },
    {
      "epoch": 13.408366820896239,
      "grad_norm": 5.981504440307617,
      "learning_rate": 2.1129756396562885e-06,
      "loss": 0.4694,
      "step": 837500
    },
    {
      "epoch": 13.416371816013193,
      "grad_norm": 4.044373989105225,
      "learning_rate": 2.0843863713814563e-06,
      "loss": 0.4664,
      "step": 838000
    },
    {
      "epoch": 13.424376811130145,
      "grad_norm": 7.819124221801758,
      "learning_rate": 2.055797103106624e-06,
      "loss": 0.4981,
      "step": 838500
    },
    {
      "epoch": 13.432381806247099,
      "grad_norm": 2.49102520942688,
      "learning_rate": 2.0272078348317924e-06,
      "loss": 0.4727,
      "step": 839000
    },
    {
      "epoch": 13.44038680136405,
      "grad_norm": 4.896381378173828,
      "learning_rate": 1.99861856655696e-06,
      "loss": 0.4634,
      "step": 839500
    },
    {
      "epoch": 13.448391796481005,
      "grad_norm": 5.374212741851807,
      "learning_rate": 1.970029298282128e-06,
      "loss": 0.4784,
      "step": 840000
    },
    {
      "epoch": 13.456396791597957,
      "grad_norm": 5.052213668823242,
      "learning_rate": 1.9414400300072962e-06,
      "loss": 0.4797,
      "step": 840500
    },
    {
      "epoch": 13.46440178671491,
      "grad_norm": 5.281627178192139,
      "learning_rate": 1.912850761732464e-06,
      "loss": 0.4653,
      "step": 841000
    },
    {
      "epoch": 13.472406781831863,
      "grad_norm": 4.409907341003418,
      "learning_rate": 1.884261493457632e-06,
      "loss": 0.4837,
      "step": 841500
    },
    {
      "epoch": 13.480411776948817,
      "grad_norm": 2.720383882522583,
      "learning_rate": 1.8556722251827999e-06,
      "loss": 0.4934,
      "step": 842000
    },
    {
      "epoch": 13.488416772065769,
      "grad_norm": 2.6165215969085693,
      "learning_rate": 1.827082956907968e-06,
      "loss": 0.4745,
      "step": 842500
    },
    {
      "epoch": 13.496421767182722,
      "grad_norm": 3.155848741531372,
      "learning_rate": 1.7984936886331357e-06,
      "loss": 0.4959,
      "step": 843000
    },
    {
      "epoch": 13.504426762299675,
      "grad_norm": 4.844764232635498,
      "learning_rate": 1.7699044203583038e-06,
      "loss": 0.4705,
      "step": 843500
    },
    {
      "epoch": 13.512431757416628,
      "grad_norm": 3.9780116081237793,
      "learning_rate": 1.7413151520834716e-06,
      "loss": 0.4887,
      "step": 844000
    },
    {
      "epoch": 13.52043675253358,
      "grad_norm": 2.3725244998931885,
      "learning_rate": 1.7127258838086396e-06,
      "loss": 0.463,
      "step": 844500
    },
    {
      "epoch": 13.528441747650534,
      "grad_norm": 3.3388452529907227,
      "learning_rate": 1.6841366155338074e-06,
      "loss": 0.484,
      "step": 845000
    },
    {
      "epoch": 13.536446742767486,
      "grad_norm": 5.526893615722656,
      "learning_rate": 1.6555473472589754e-06,
      "loss": 0.4765,
      "step": 845500
    },
    {
      "epoch": 13.54445173788444,
      "grad_norm": 9.484604835510254,
      "learning_rate": 1.6269580789841435e-06,
      "loss": 0.4719,
      "step": 846000
    },
    {
      "epoch": 13.552456733001392,
      "grad_norm": 3.8710641860961914,
      "learning_rate": 1.5983688107093113e-06,
      "loss": 0.4543,
      "step": 846500
    },
    {
      "epoch": 13.560461728118346,
      "grad_norm": 4.539215564727783,
      "learning_rate": 1.5697795424344793e-06,
      "loss": 0.4836,
      "step": 847000
    },
    {
      "epoch": 13.568466723235298,
      "grad_norm": 5.085073947906494,
      "learning_rate": 1.5411902741596471e-06,
      "loss": 0.4859,
      "step": 847500
    },
    {
      "epoch": 13.576471718352252,
      "grad_norm": 10.809453964233398,
      "learning_rate": 1.512601005884815e-06,
      "loss": 0.4729,
      "step": 848000
    },
    {
      "epoch": 13.584476713469204,
      "grad_norm": 6.418046951293945,
      "learning_rate": 1.4840117376099832e-06,
      "loss": 0.4694,
      "step": 848500
    },
    {
      "epoch": 13.592481708586158,
      "grad_norm": 6.031883716583252,
      "learning_rate": 1.455422469335151e-06,
      "loss": 0.4835,
      "step": 849000
    },
    {
      "epoch": 13.60048670370311,
      "grad_norm": 6.145044803619385,
      "learning_rate": 1.426833201060319e-06,
      "loss": 0.4704,
      "step": 849500
    },
    {
      "epoch": 13.608491698820064,
      "grad_norm": 2.719926357269287,
      "learning_rate": 1.3982439327854868e-06,
      "loss": 0.4872,
      "step": 850000
    },
    {
      "epoch": 13.616496693937016,
      "grad_norm": 6.296878814697266,
      "learning_rate": 1.3696546645106546e-06,
      "loss": 0.4848,
      "step": 850500
    },
    {
      "epoch": 13.62450168905397,
      "grad_norm": 2.58768630027771,
      "learning_rate": 1.3410653962358227e-06,
      "loss": 0.4886,
      "step": 851000
    },
    {
      "epoch": 13.632506684170922,
      "grad_norm": 3.3098840713500977,
      "learning_rate": 1.3124761279609905e-06,
      "loss": 0.4832,
      "step": 851500
    },
    {
      "epoch": 13.640511679287876,
      "grad_norm": 13.127944946289062,
      "learning_rate": 1.2838868596861585e-06,
      "loss": 0.4779,
      "step": 852000
    },
    {
      "epoch": 13.648516674404828,
      "grad_norm": 4.826221942901611,
      "learning_rate": 1.2552975914113263e-06,
      "loss": 0.4789,
      "step": 852500
    },
    {
      "epoch": 13.656521669521782,
      "grad_norm": 5.677764415740967,
      "learning_rate": 1.2267083231364943e-06,
      "loss": 0.4622,
      "step": 853000
    },
    {
      "epoch": 13.664526664638734,
      "grad_norm": 6.540976047515869,
      "learning_rate": 1.1981190548616622e-06,
      "loss": 0.474,
      "step": 853500
    },
    {
      "epoch": 13.672531659755688,
      "grad_norm": 2.021641969680786,
      "learning_rate": 1.1695297865868302e-06,
      "loss": 0.4973,
      "step": 854000
    },
    {
      "epoch": 13.68053665487264,
      "grad_norm": 3.145022392272949,
      "learning_rate": 1.1409405183119982e-06,
      "loss": 0.4977,
      "step": 854500
    },
    {
      "epoch": 13.688541649989594,
      "grad_norm": 4.5575032234191895,
      "learning_rate": 1.1123512500371662e-06,
      "loss": 0.482,
      "step": 855000
    },
    {
      "epoch": 13.696546645106547,
      "grad_norm": 5.467521667480469,
      "learning_rate": 1.083761981762334e-06,
      "loss": 0.4747,
      "step": 855500
    },
    {
      "epoch": 13.7045516402235,
      "grad_norm": 6.535976409912109,
      "learning_rate": 1.055172713487502e-06,
      "loss": 0.4782,
      "step": 856000
    },
    {
      "epoch": 13.712556635340452,
      "grad_norm": 4.153306007385254,
      "learning_rate": 1.0265834452126699e-06,
      "loss": 0.4617,
      "step": 856500
    },
    {
      "epoch": 13.720561630457405,
      "grad_norm": 6.434711456298828,
      "learning_rate": 9.97994176937838e-07,
      "loss": 0.4907,
      "step": 857000
    },
    {
      "epoch": 13.72856662557436,
      "grad_norm": 4.094590187072754,
      "learning_rate": 9.694049086630057e-07,
      "loss": 0.4864,
      "step": 857500
    },
    {
      "epoch": 13.736571620691311,
      "grad_norm": 6.502530097961426,
      "learning_rate": 9.408156403881736e-07,
      "loss": 0.4771,
      "step": 858000
    },
    {
      "epoch": 13.744576615808265,
      "grad_norm": 6.504085540771484,
      "learning_rate": 9.122263721133416e-07,
      "loss": 0.4616,
      "step": 858500
    },
    {
      "epoch": 13.752581610925217,
      "grad_norm": 7.201173305511475,
      "learning_rate": 8.836371038385095e-07,
      "loss": 0.4716,
      "step": 859000
    },
    {
      "epoch": 13.760586606042171,
      "grad_norm": 4.671670913696289,
      "learning_rate": 8.550478355636774e-07,
      "loss": 0.4594,
      "step": 859500
    },
    {
      "epoch": 13.768591601159123,
      "grad_norm": 3.424128293991089,
      "learning_rate": 8.264585672888455e-07,
      "loss": 0.4798,
      "step": 860000
    },
    {
      "epoch": 13.776596596276077,
      "grad_norm": 7.916264533996582,
      "learning_rate": 7.978692990140135e-07,
      "loss": 0.4709,
      "step": 860500
    },
    {
      "epoch": 13.784601591393029,
      "grad_norm": 4.043277740478516,
      "learning_rate": 7.692800307391813e-07,
      "loss": 0.4635,
      "step": 861000
    },
    {
      "epoch": 13.792606586509983,
      "grad_norm": 6.903904914855957,
      "learning_rate": 7.406907624643493e-07,
      "loss": 0.4862,
      "step": 861500
    },
    {
      "epoch": 13.800611581626935,
      "grad_norm": 4.017940998077393,
      "learning_rate": 7.121014941895172e-07,
      "loss": 0.4836,
      "step": 862000
    },
    {
      "epoch": 13.808616576743889,
      "grad_norm": 3.4128127098083496,
      "learning_rate": 6.835122259146851e-07,
      "loss": 0.4882,
      "step": 862500
    },
    {
      "epoch": 13.816621571860841,
      "grad_norm": 2.9634146690368652,
      "learning_rate": 6.54922957639853e-07,
      "loss": 0.4799,
      "step": 863000
    },
    {
      "epoch": 13.824626566977795,
      "grad_norm": 4.7439985275268555,
      "learning_rate": 6.26333689365021e-07,
      "loss": 0.4631,
      "step": 863500
    },
    {
      "epoch": 13.832631562094747,
      "grad_norm": 2.707301616668701,
      "learning_rate": 5.977444210901889e-07,
      "loss": 0.4716,
      "step": 864000
    },
    {
      "epoch": 13.8406365572117,
      "grad_norm": 2.9916329383850098,
      "learning_rate": 5.691551528153568e-07,
      "loss": 0.4719,
      "step": 864500
    },
    {
      "epoch": 13.848641552328653,
      "grad_norm": 4.170703887939453,
      "learning_rate": 5.405658845405247e-07,
      "loss": 0.4581,
      "step": 865000
    },
    {
      "epoch": 13.856646547445607,
      "grad_norm": 3.396083116531372,
      "learning_rate": 5.119766162656927e-07,
      "loss": 0.4906,
      "step": 865500
    },
    {
      "epoch": 13.864651542562559,
      "grad_norm": 6.377048969268799,
      "learning_rate": 4.833873479908606e-07,
      "loss": 0.4942,
      "step": 866000
    },
    {
      "epoch": 13.872656537679513,
      "grad_norm": 5.883573532104492,
      "learning_rate": 4.547980797160285e-07,
      "loss": 0.4714,
      "step": 866500
    },
    {
      "epoch": 13.880661532796465,
      "grad_norm": 1.9553189277648926,
      "learning_rate": 4.262088114411964e-07,
      "loss": 0.4604,
      "step": 867000
    },
    {
      "epoch": 13.888666527913418,
      "grad_norm": 6.122403144836426,
      "learning_rate": 3.9761954316636444e-07,
      "loss": 0.4914,
      "step": 867500
    },
    {
      "epoch": 13.89667152303037,
      "grad_norm": 11.009552955627441,
      "learning_rate": 3.6903027489153236e-07,
      "loss": 0.4665,
      "step": 868000
    },
    {
      "epoch": 13.904676518147324,
      "grad_norm": 2.507995128631592,
      "learning_rate": 3.404410066167003e-07,
      "loss": 0.471,
      "step": 868500
    },
    {
      "epoch": 13.912681513264276,
      "grad_norm": 4.352710723876953,
      "learning_rate": 3.118517383418682e-07,
      "loss": 0.4741,
      "step": 869000
    },
    {
      "epoch": 13.92068650838123,
      "grad_norm": 5.3373236656188965,
      "learning_rate": 2.8326247006703617e-07,
      "loss": 0.4796,
      "step": 869500
    },
    {
      "epoch": 13.928691503498182,
      "grad_norm": 4.790227890014648,
      "learning_rate": 2.5467320179220404e-07,
      "loss": 0.4901,
      "step": 870000
    },
    {
      "epoch": 13.936696498615136,
      "grad_norm": 3.576099395751953,
      "learning_rate": 2.2608393351737198e-07,
      "loss": 0.4952,
      "step": 870500
    },
    {
      "epoch": 13.944701493732088,
      "grad_norm": 4.678445816040039,
      "learning_rate": 1.9749466524253993e-07,
      "loss": 0.4751,
      "step": 871000
    },
    {
      "epoch": 13.952706488849042,
      "grad_norm": 4.292295455932617,
      "learning_rate": 1.6890539696770785e-07,
      "loss": 0.4758,
      "step": 871500
    },
    {
      "epoch": 13.960711483965994,
      "grad_norm": 1.6446963548660278,
      "learning_rate": 1.403161286928758e-07,
      "loss": 0.4806,
      "step": 872000
    },
    {
      "epoch": 13.968716479082948,
      "grad_norm": 18.05316734313965,
      "learning_rate": 1.1172686041804373e-07,
      "loss": 0.489,
      "step": 872500
    },
    {
      "epoch": 13.9767214741999,
      "grad_norm": 4.355331897735596,
      "learning_rate": 8.313759214321165e-08,
      "loss": 0.4641,
      "step": 873000
    },
    {
      "epoch": 13.984726469316854,
      "grad_norm": 4.650041580200195,
      "learning_rate": 5.454832386837959e-08,
      "loss": 0.4703,
      "step": 873500
    },
    {
      "epoch": 13.992731464433806,
      "grad_norm": 3.6189932823181152,
      "learning_rate": 2.5959055593547517e-08,
      "loss": 0.4887,
      "step": 874000
    },
    {
      "epoch": 14.001633123589029,
      "grad_norm": 6.557488441467285,
      "learning_rate": 6.244896488784284e-06,
      "loss": 0.4976,
      "step": 874500
    },
    {
      "epoch": 14.009638631378388,
      "grad_norm": 5.2182159423828125,
      "learning_rate": 6.219879276942537e-06,
      "loss": 0.5104,
      "step": 875000
    },
    {
      "epoch": 14.017644139167748,
      "grad_norm": 5.545884132385254,
      "learning_rate": 6.19486206510079e-06,
      "loss": 0.4879,
      "step": 875500
    },
    {
      "epoch": 14.025649646957106,
      "grad_norm": 6.339271068572998,
      "learning_rate": 6.169844853259042e-06,
      "loss": 0.4945,
      "step": 876000
    },
    {
      "epoch": 14.033655154746466,
      "grad_norm": 4.152933120727539,
      "learning_rate": 6.144827641417295e-06,
      "loss": 0.496,
      "step": 876500
    },
    {
      "epoch": 14.041660662535824,
      "grad_norm": 4.56641960144043,
      "learning_rate": 6.119810429575549e-06,
      "loss": 0.4839,
      "step": 877000
    },
    {
      "epoch": 14.049666170325184,
      "grad_norm": 4.434755802154541,
      "learning_rate": 6.0947932177338015e-06,
      "loss": 0.4847,
      "step": 877500
    },
    {
      "epoch": 14.057671678114543,
      "grad_norm": 3.9267218112945557,
      "learning_rate": 6.0697760058920544e-06,
      "loss": 0.4818,
      "step": 878000
    },
    {
      "epoch": 14.065677185903901,
      "grad_norm": 5.080101490020752,
      "learning_rate": 6.044758794050307e-06,
      "loss": 0.4875,
      "step": 878500
    },
    {
      "epoch": 14.07368269369326,
      "grad_norm": 7.18170166015625,
      "learning_rate": 6.01974158220856e-06,
      "loss": 0.4958,
      "step": 879000
    },
    {
      "epoch": 14.08168820148262,
      "grad_norm": 6.71671199798584,
      "learning_rate": 5.994724370366812e-06,
      "loss": 0.5055,
      "step": 879500
    },
    {
      "epoch": 14.089693709271979,
      "grad_norm": 2.3767013549804688,
      "learning_rate": 5.969707158525065e-06,
      "loss": 0.4852,
      "step": 880000
    },
    {
      "epoch": 14.097699217061338,
      "grad_norm": 0.9370321035385132,
      "learning_rate": 5.944689946683318e-06,
      "loss": 0.5077,
      "step": 880500
    },
    {
      "epoch": 14.105704724850698,
      "grad_norm": 4.294179439544678,
      "learning_rate": 5.919672734841571e-06,
      "loss": 0.4868,
      "step": 881000
    },
    {
      "epoch": 14.113710232640056,
      "grad_norm": 2.9366958141326904,
      "learning_rate": 5.894655522999824e-06,
      "loss": 0.49,
      "step": 881500
    },
    {
      "epoch": 14.121715740429416,
      "grad_norm": 4.7542548179626465,
      "learning_rate": 5.869638311158077e-06,
      "loss": 0.4853,
      "step": 882000
    },
    {
      "epoch": 14.129721248218775,
      "grad_norm": 4.77508020401001,
      "learning_rate": 5.84462109931633e-06,
      "loss": 0.4918,
      "step": 882500
    },
    {
      "epoch": 14.137726756008133,
      "grad_norm": 3.3975956439971924,
      "learning_rate": 5.819603887474583e-06,
      "loss": 0.4682,
      "step": 883000
    },
    {
      "epoch": 14.145732263797493,
      "grad_norm": 4.570609092712402,
      "learning_rate": 5.794586675632836e-06,
      "loss": 0.4996,
      "step": 883500
    },
    {
      "epoch": 14.15373777158685,
      "grad_norm": 2.5588910579681396,
      "learning_rate": 5.769569463791089e-06,
      "loss": 0.475,
      "step": 884000
    },
    {
      "epoch": 14.16174327937621,
      "grad_norm": 4.567698955535889,
      "learning_rate": 5.7445522519493415e-06,
      "loss": 0.4854,
      "step": 884500
    },
    {
      "epoch": 14.16974878716557,
      "grad_norm": 3.8603310585021973,
      "learning_rate": 5.7195350401075945e-06,
      "loss": 0.4687,
      "step": 885000
    },
    {
      "epoch": 14.177754294954928,
      "grad_norm": 3.3691229820251465,
      "learning_rate": 5.694517828265847e-06,
      "loss": 0.47,
      "step": 885500
    },
    {
      "epoch": 14.185759802744288,
      "grad_norm": 2.651768922805786,
      "learning_rate": 5.6695006164240994e-06,
      "loss": 0.4732,
      "step": 886000
    },
    {
      "epoch": 14.193765310533648,
      "grad_norm": 5.345330238342285,
      "learning_rate": 5.644483404582352e-06,
      "loss": 0.4966,
      "step": 886500
    },
    {
      "epoch": 14.201770818323006,
      "grad_norm": 2.291607618331909,
      "learning_rate": 5.619466192740606e-06,
      "loss": 0.4887,
      "step": 887000
    },
    {
      "epoch": 14.209776326112365,
      "grad_norm": 3.2397854328155518,
      "learning_rate": 5.594448980898859e-06,
      "loss": 0.5074,
      "step": 887500
    },
    {
      "epoch": 14.217781833901725,
      "grad_norm": 3.2426846027374268,
      "learning_rate": 5.569431769057112e-06,
      "loss": 0.4786,
      "step": 888000
    },
    {
      "epoch": 14.225787341691083,
      "grad_norm": 2.227935791015625,
      "learning_rate": 5.544414557215365e-06,
      "loss": 0.4851,
      "step": 888500
    },
    {
      "epoch": 14.233792849480443,
      "grad_norm": 2.181809663772583,
      "learning_rate": 5.519397345373617e-06,
      "loss": 0.4688,
      "step": 889000
    },
    {
      "epoch": 14.241798357269802,
      "grad_norm": 3.4667530059814453,
      "learning_rate": 5.49438013353187e-06,
      "loss": 0.4787,
      "step": 889500
    },
    {
      "epoch": 14.24980386505916,
      "grad_norm": 1.6988625526428223,
      "learning_rate": 5.469362921690123e-06,
      "loss": 0.5124,
      "step": 890000
    },
    {
      "epoch": 14.25780937284852,
      "grad_norm": 6.5541276931762695,
      "learning_rate": 5.444345709848376e-06,
      "loss": 0.4841,
      "step": 890500
    },
    {
      "epoch": 14.26581488063788,
      "grad_norm": 5.429276943206787,
      "learning_rate": 5.419328498006629e-06,
      "loss": 0.506,
      "step": 891000
    },
    {
      "epoch": 14.273820388427238,
      "grad_norm": 5.287841320037842,
      "learning_rate": 5.3943112861648816e-06,
      "loss": 0.485,
      "step": 891500
    },
    {
      "epoch": 14.281825896216597,
      "grad_norm": 2.5124423503875732,
      "learning_rate": 5.3692940743231345e-06,
      "loss": 0.4774,
      "step": 892000
    },
    {
      "epoch": 14.289831404005955,
      "grad_norm": 2.1675219535827637,
      "learning_rate": 5.344276862481387e-06,
      "loss": 0.4975,
      "step": 892500
    },
    {
      "epoch": 14.297836911795315,
      "grad_norm": 4.385028839111328,
      "learning_rate": 5.31925965063964e-06,
      "loss": 0.4934,
      "step": 893000
    },
    {
      "epoch": 14.305842419584675,
      "grad_norm": 3.6735994815826416,
      "learning_rate": 5.294242438797893e-06,
      "loss": 0.4926,
      "step": 893500
    },
    {
      "epoch": 14.313847927374033,
      "grad_norm": 9.286093711853027,
      "learning_rate": 5.269225226956146e-06,
      "loss": 0.49,
      "step": 894000
    },
    {
      "epoch": 14.321853435163392,
      "grad_norm": 4.74794864654541,
      "learning_rate": 5.244208015114399e-06,
      "loss": 0.4954,
      "step": 894500
    },
    {
      "epoch": 14.329858942952752,
      "grad_norm": 3.7026174068450928,
      "learning_rate": 5.219190803272652e-06,
      "loss": 0.4939,
      "step": 895000
    },
    {
      "epoch": 14.33786445074211,
      "grad_norm": 4.784877300262451,
      "learning_rate": 5.194173591430905e-06,
      "loss": 0.4757,
      "step": 895500
    },
    {
      "epoch": 14.34586995853147,
      "grad_norm": 2.1173105239868164,
      "learning_rate": 5.169156379589157e-06,
      "loss": 0.4863,
      "step": 896000
    },
    {
      "epoch": 14.35387546632083,
      "grad_norm": 5.551151752471924,
      "learning_rate": 5.14413916774741e-06,
      "loss": 0.4851,
      "step": 896500
    },
    {
      "epoch": 14.361880974110187,
      "grad_norm": 2.1007986068725586,
      "learning_rate": 5.119121955905664e-06,
      "loss": 0.4901,
      "step": 897000
    },
    {
      "epoch": 14.369886481899547,
      "grad_norm": 6.593371391296387,
      "learning_rate": 5.094104744063917e-06,
      "loss": 0.4842,
      "step": 897500
    },
    {
      "epoch": 14.377891989688907,
      "grad_norm": 6.09916877746582,
      "learning_rate": 5.0690875322221695e-06,
      "loss": 0.4762,
      "step": 898000
    },
    {
      "epoch": 14.385897497478265,
      "grad_norm": 3.0346672534942627,
      "learning_rate": 5.0440703203804224e-06,
      "loss": 0.4954,
      "step": 898500
    },
    {
      "epoch": 14.393903005267624,
      "grad_norm": 4.478115558624268,
      "learning_rate": 5.0190531085386745e-06,
      "loss": 0.5075,
      "step": 899000
    },
    {
      "epoch": 14.401908513056982,
      "grad_norm": 8.433676719665527,
      "learning_rate": 4.9940358966969274e-06,
      "loss": 0.477,
      "step": 899500
    },
    {
      "epoch": 14.409914020846342,
      "grad_norm": 2.552554130554199,
      "learning_rate": 4.96901868485518e-06,
      "loss": 0.4877,
      "step": 900000
    },
    {
      "epoch": 14.417919528635702,
      "grad_norm": 4.811963081359863,
      "learning_rate": 4.944001473013433e-06,
      "loss": 0.4814,
      "step": 900500
    },
    {
      "epoch": 14.42592503642506,
      "grad_norm": 3.8514132499694824,
      "learning_rate": 4.918984261171686e-06,
      "loss": 0.4879,
      "step": 901000
    },
    {
      "epoch": 14.43393054421442,
      "grad_norm": 5.114526748657227,
      "learning_rate": 4.893967049329939e-06,
      "loss": 0.5018,
      "step": 901500
    },
    {
      "epoch": 14.44193605200378,
      "grad_norm": 7.457131385803223,
      "learning_rate": 4.868949837488192e-06,
      "loss": 0.4829,
      "step": 902000
    },
    {
      "epoch": 14.449941559793137,
      "grad_norm": 5.034055709838867,
      "learning_rate": 4.843932625646445e-06,
      "loss": 0.481,
      "step": 902500
    },
    {
      "epoch": 14.457947067582497,
      "grad_norm": 4.742847442626953,
      "learning_rate": 4.818915413804698e-06,
      "loss": 0.4982,
      "step": 903000
    },
    {
      "epoch": 14.465952575371857,
      "grad_norm": 3.9873812198638916,
      "learning_rate": 4.793898201962951e-06,
      "loss": 0.4816,
      "step": 903500
    },
    {
      "epoch": 14.473958083161214,
      "grad_norm": 5.150600433349609,
      "learning_rate": 4.768880990121204e-06,
      "loss": 0.4945,
      "step": 904000
    },
    {
      "epoch": 14.481963590950574,
      "grad_norm": 5.989622116088867,
      "learning_rate": 4.743863778279457e-06,
      "loss": 0.4947,
      "step": 904500
    },
    {
      "epoch": 14.489969098739934,
      "grad_norm": 5.468503952026367,
      "learning_rate": 4.7188465664377095e-06,
      "loss": 0.4837,
      "step": 905000
    },
    {
      "epoch": 14.497974606529292,
      "grad_norm": 4.8000640869140625,
      "learning_rate": 4.693829354595962e-06,
      "loss": 0.4779,
      "step": 905500
    },
    {
      "epoch": 14.505980114318652,
      "grad_norm": 2.2242138385772705,
      "learning_rate": 4.6688121427542145e-06,
      "loss": 0.4753,
      "step": 906000
    },
    {
      "epoch": 14.51398562210801,
      "grad_norm": 4.889643669128418,
      "learning_rate": 4.6437949309124675e-06,
      "loss": 0.4928,
      "step": 906500
    },
    {
      "epoch": 14.52199112989737,
      "grad_norm": 3.3935859203338623,
      "learning_rate": 4.618777719070721e-06,
      "loss": 0.4917,
      "step": 907000
    },
    {
      "epoch": 14.529996637686729,
      "grad_norm": 3.6917531490325928,
      "learning_rate": 4.593760507228974e-06,
      "loss": 0.4887,
      "step": 907500
    },
    {
      "epoch": 14.538002145476087,
      "grad_norm": 4.4678826332092285,
      "learning_rate": 4.568743295387227e-06,
      "loss": 0.4988,
      "step": 908000
    },
    {
      "epoch": 14.546007653265447,
      "grad_norm": 7.7170634269714355,
      "learning_rate": 4.54372608354548e-06,
      "loss": 0.4839,
      "step": 908500
    },
    {
      "epoch": 14.554013161054806,
      "grad_norm": 2.929189682006836,
      "learning_rate": 4.518708871703732e-06,
      "loss": 0.4668,
      "step": 909000
    },
    {
      "epoch": 14.562018668844164,
      "grad_norm": 6.289102077484131,
      "learning_rate": 4.493691659861985e-06,
      "loss": 0.4849,
      "step": 909500
    },
    {
      "epoch": 14.570024176633524,
      "grad_norm": 5.227800369262695,
      "learning_rate": 4.468674448020238e-06,
      "loss": 0.5043,
      "step": 910000
    },
    {
      "epoch": 14.578029684422884,
      "grad_norm": 4.78224515914917,
      "learning_rate": 4.443657236178491e-06,
      "loss": 0.4874,
      "step": 910500
    },
    {
      "epoch": 14.586035192212242,
      "grad_norm": 5.565854072570801,
      "learning_rate": 4.418640024336744e-06,
      "loss": 0.4825,
      "step": 911000
    },
    {
      "epoch": 14.594040700001601,
      "grad_norm": 2.3084218502044678,
      "learning_rate": 4.393622812494997e-06,
      "loss": 0.4857,
      "step": 911500
    },
    {
      "epoch": 14.602046207790961,
      "grad_norm": 5.424861907958984,
      "learning_rate": 4.3686056006532496e-06,
      "loss": 0.4979,
      "step": 912000
    },
    {
      "epoch": 14.610051715580319,
      "grad_norm": 4.128798484802246,
      "learning_rate": 4.3435883888115025e-06,
      "loss": 0.5047,
      "step": 912500
    },
    {
      "epoch": 14.618057223369679,
      "grad_norm": 5.176788330078125,
      "learning_rate": 4.318571176969755e-06,
      "loss": 0.4925,
      "step": 913000
    },
    {
      "epoch": 14.626062731159038,
      "grad_norm": 2.909708023071289,
      "learning_rate": 4.293553965128008e-06,
      "loss": 0.4761,
      "step": 913500
    },
    {
      "epoch": 14.634068238948396,
      "grad_norm": 3.5808935165405273,
      "learning_rate": 4.268536753286261e-06,
      "loss": 0.4852,
      "step": 914000
    },
    {
      "epoch": 14.642073746737756,
      "grad_norm": 6.598717212677002,
      "learning_rate": 4.243519541444514e-06,
      "loss": 0.4822,
      "step": 914500
    },
    {
      "epoch": 14.650079254527114,
      "grad_norm": 2.397521495819092,
      "learning_rate": 4.218502329602767e-06,
      "loss": 0.4714,
      "step": 915000
    },
    {
      "epoch": 14.658084762316474,
      "grad_norm": 3.159400701522827,
      "learning_rate": 4.193485117761019e-06,
      "loss": 0.484,
      "step": 915500
    },
    {
      "epoch": 14.666090270105833,
      "grad_norm": 6.70546293258667,
      "learning_rate": 4.168467905919272e-06,
      "loss": 0.4835,
      "step": 916000
    },
    {
      "epoch": 14.674095777895191,
      "grad_norm": 4.957679271697998,
      "learning_rate": 4.143450694077526e-06,
      "loss": 0.4886,
      "step": 916500
    },
    {
      "epoch": 14.682101285684551,
      "grad_norm": 7.469851970672607,
      "learning_rate": 4.118433482235779e-06,
      "loss": 0.4856,
      "step": 917000
    },
    {
      "epoch": 14.69010679347391,
      "grad_norm": 3.9375979900360107,
      "learning_rate": 4.093416270394032e-06,
      "loss": 0.4872,
      "step": 917500
    },
    {
      "epoch": 14.698112301263269,
      "grad_norm": 4.023369789123535,
      "learning_rate": 4.068399058552285e-06,
      "loss": 0.4922,
      "step": 918000
    },
    {
      "epoch": 14.706117809052628,
      "grad_norm": 2.701173782348633,
      "learning_rate": 4.0433818467105375e-06,
      "loss": 0.502,
      "step": 918500
    },
    {
      "epoch": 14.714123316841988,
      "grad_norm": 2.2140331268310547,
      "learning_rate": 4.01836463486879e-06,
      "loss": 0.4721,
      "step": 919000
    },
    {
      "epoch": 14.722128824631346,
      "grad_norm": 2.9274256229400635,
      "learning_rate": 3.9933474230270425e-06,
      "loss": 0.494,
      "step": 919500
    },
    {
      "epoch": 14.730134332420706,
      "grad_norm": 2.306500196456909,
      "learning_rate": 3.9683302111852954e-06,
      "loss": 0.5164,
      "step": 920000
    },
    {
      "epoch": 14.738139840210064,
      "grad_norm": 6.955080032348633,
      "learning_rate": 3.943312999343548e-06,
      "loss": 0.4726,
      "step": 920500
    },
    {
      "epoch": 14.746145347999423,
      "grad_norm": 3.8533928394317627,
      "learning_rate": 3.918295787501801e-06,
      "loss": 0.4966,
      "step": 921000
    },
    {
      "epoch": 14.754150855788783,
      "grad_norm": 3.8940656185150146,
      "learning_rate": 3.893278575660054e-06,
      "loss": 0.4798,
      "step": 921500
    },
    {
      "epoch": 14.762156363578141,
      "grad_norm": 2.123814821243286,
      "learning_rate": 3.868261363818307e-06,
      "loss": 0.4923,
      "step": 922000
    },
    {
      "epoch": 14.7701618713675,
      "grad_norm": 5.7522687911987305,
      "learning_rate": 3.84324415197656e-06,
      "loss": 0.4817,
      "step": 922500
    },
    {
      "epoch": 14.77816737915686,
      "grad_norm": 3.2154622077941895,
      "learning_rate": 3.818226940134813e-06,
      "loss": 0.4817,
      "step": 923000
    },
    {
      "epoch": 14.786172886946218,
      "grad_norm": 4.8317975997924805,
      "learning_rate": 3.793209728293066e-06,
      "loss": 0.4887,
      "step": 923500
    },
    {
      "epoch": 14.794178394735578,
      "grad_norm": 5.3537726402282715,
      "learning_rate": 3.7681925164513188e-06,
      "loss": 0.4873,
      "step": 924000
    },
    {
      "epoch": 14.802183902524938,
      "grad_norm": 5.1974592208862305,
      "learning_rate": 3.7431753046095717e-06,
      "loss": 0.4967,
      "step": 924500
    },
    {
      "epoch": 14.810189410314296,
      "grad_norm": 4.553103446960449,
      "learning_rate": 3.7181580927678246e-06,
      "loss": 0.4905,
      "step": 925000
    },
    {
      "epoch": 14.818194918103655,
      "grad_norm": 2.0754947662353516,
      "learning_rate": 3.693140880926077e-06,
      "loss": 0.4899,
      "step": 925500
    },
    {
      "epoch": 14.826200425893015,
      "grad_norm": 4.64585542678833,
      "learning_rate": 3.66812366908433e-06,
      "loss": 0.4879,
      "step": 926000
    },
    {
      "epoch": 14.834205933682373,
      "grad_norm": 3.895488739013672,
      "learning_rate": 3.643106457242583e-06,
      "loss": 0.5015,
      "step": 926500
    },
    {
      "epoch": 14.842211441471733,
      "grad_norm": 2.8385186195373535,
      "learning_rate": 3.618089245400836e-06,
      "loss": 0.4931,
      "step": 927000
    },
    {
      "epoch": 14.850216949261092,
      "grad_norm": 4.607309818267822,
      "learning_rate": 3.593072033559089e-06,
      "loss": 0.4822,
      "step": 927500
    },
    {
      "epoch": 14.85822245705045,
      "grad_norm": 4.936906814575195,
      "learning_rate": 3.5680548217173417e-06,
      "loss": 0.4916,
      "step": 928000
    },
    {
      "epoch": 14.86622796483981,
      "grad_norm": 3.251115083694458,
      "learning_rate": 3.543037609875595e-06,
      "loss": 0.4747,
      "step": 928500
    },
    {
      "epoch": 14.874233472629168,
      "grad_norm": 1.8942159414291382,
      "learning_rate": 3.518020398033847e-06,
      "loss": 0.4923,
      "step": 929000
    },
    {
      "epoch": 14.882238980418528,
      "grad_norm": 6.795762062072754,
      "learning_rate": 3.4930031861921e-06,
      "loss": 0.4778,
      "step": 929500
    },
    {
      "epoch": 14.890244488207887,
      "grad_norm": 4.291184425354004,
      "learning_rate": 3.467985974350353e-06,
      "loss": 0.4812,
      "step": 930000
    },
    {
      "epoch": 14.898249995997245,
      "grad_norm": 3.6458699703216553,
      "learning_rate": 3.4429687625086063e-06,
      "loss": 0.4966,
      "step": 930500
    },
    {
      "epoch": 14.906255503786605,
      "grad_norm": 3.946807622909546,
      "learning_rate": 3.4179515506668592e-06,
      "loss": 0.4902,
      "step": 931000
    },
    {
      "epoch": 14.914261011575965,
      "grad_norm": 4.158203125,
      "learning_rate": 3.392934338825112e-06,
      "loss": 0.4783,
      "step": 931500
    },
    {
      "epoch": 14.922266519365323,
      "grad_norm": 3.455291986465454,
      "learning_rate": 3.3679171269833642e-06,
      "loss": 0.4802,
      "step": 932000
    },
    {
      "epoch": 14.930272027154682,
      "grad_norm": 9.832682609558105,
      "learning_rate": 3.3428999151416176e-06,
      "loss": 0.4902,
      "step": 932500
    },
    {
      "epoch": 14.938277534944042,
      "grad_norm": 2.6560001373291016,
      "learning_rate": 3.3178827032998705e-06,
      "loss": 0.4702,
      "step": 933000
    },
    {
      "epoch": 14.9462830427334,
      "grad_norm": 6.088718891143799,
      "learning_rate": 3.2928654914581234e-06,
      "loss": 0.5002,
      "step": 933500
    },
    {
      "epoch": 14.95428855052276,
      "grad_norm": 8.429947853088379,
      "learning_rate": 3.2678482796163763e-06,
      "loss": 0.4857,
      "step": 934000
    },
    {
      "epoch": 14.96229405831212,
      "grad_norm": 4.548020839691162,
      "learning_rate": 3.2428310677746292e-06,
      "loss": 0.4803,
      "step": 934500
    },
    {
      "epoch": 14.970299566101477,
      "grad_norm": 5.085371017456055,
      "learning_rate": 3.217813855932882e-06,
      "loss": 0.4862,
      "step": 935000
    },
    {
      "epoch": 14.978305073890837,
      "grad_norm": 6.425253391265869,
      "learning_rate": 3.1927966440911347e-06,
      "loss": 0.4891,
      "step": 935500
    },
    {
      "epoch": 14.986310581680197,
      "grad_norm": 2.2543458938598633,
      "learning_rate": 3.1677794322493876e-06,
      "loss": 0.484,
      "step": 936000
    },
    {
      "epoch": 14.994316089469555,
      "grad_norm": 4.290701389312744,
      "learning_rate": 3.1427622204076405e-06,
      "loss": 0.4843,
      "step": 936500
    },
    {
      "epoch": 15.002321597258915,
      "grad_norm": 1.199513554573059,
      "learning_rate": 3.1177450085658934e-06,
      "loss": 0.4964,
      "step": 937000
    },
    {
      "epoch": 15.010327105048273,
      "grad_norm": 4.983152389526367,
      "learning_rate": 3.0927277967241463e-06,
      "loss": 0.4794,
      "step": 937500
    },
    {
      "epoch": 15.018332612837632,
      "grad_norm": 3.4366049766540527,
      "learning_rate": 3.0677105848823993e-06,
      "loss": 0.4889,
      "step": 938000
    },
    {
      "epoch": 15.026338120626992,
      "grad_norm": 2.7136878967285156,
      "learning_rate": 3.042693373040652e-06,
      "loss": 0.4426,
      "step": 938500
    },
    {
      "epoch": 15.03434362841635,
      "grad_norm": 3.542433023452759,
      "learning_rate": 3.017676161198905e-06,
      "loss": 0.4581,
      "step": 939000
    },
    {
      "epoch": 15.04234913620571,
      "grad_norm": 6.9367218017578125,
      "learning_rate": 2.9926589493571576e-06,
      "loss": 0.4615,
      "step": 939500
    },
    {
      "epoch": 15.05035464399507,
      "grad_norm": 4.181769847869873,
      "learning_rate": 2.9676417375154105e-06,
      "loss": 0.4697,
      "step": 940000
    },
    {
      "epoch": 15.058360151784427,
      "grad_norm": 2.8122828006744385,
      "learning_rate": 2.942624525673664e-06,
      "loss": 0.4628,
      "step": 940500
    },
    {
      "epoch": 15.066365659573787,
      "grad_norm": 7.6954803466796875,
      "learning_rate": 2.9176073138319164e-06,
      "loss": 0.4622,
      "step": 941000
    },
    {
      "epoch": 15.074371167363147,
      "grad_norm": 2.366791009902954,
      "learning_rate": 2.8925901019901693e-06,
      "loss": 0.4788,
      "step": 941500
    },
    {
      "epoch": 15.082376675152505,
      "grad_norm": 4.853147506713867,
      "learning_rate": 2.867572890148422e-06,
      "loss": 0.4728,
      "step": 942000
    },
    {
      "epoch": 15.090382182941864,
      "grad_norm": 2.6348073482513428,
      "learning_rate": 2.842555678306675e-06,
      "loss": 0.471,
      "step": 942500
    },
    {
      "epoch": 15.098387690731222,
      "grad_norm": 4.579074382781982,
      "learning_rate": 2.817538466464928e-06,
      "loss": 0.4766,
      "step": 943000
    },
    {
      "epoch": 15.106393198520582,
      "grad_norm": 4.011550426483154,
      "learning_rate": 2.792521254623181e-06,
      "loss": 0.4884,
      "step": 943500
    },
    {
      "epoch": 15.114398706309942,
      "grad_norm": 5.400666236877441,
      "learning_rate": 2.767504042781434e-06,
      "loss": 0.4841,
      "step": 944000
    },
    {
      "epoch": 15.1224042140993,
      "grad_norm": 9.727272033691406,
      "learning_rate": 2.7424868309396864e-06,
      "loss": 0.4771,
      "step": 944500
    },
    {
      "epoch": 15.13040972188866,
      "grad_norm": 7.336095809936523,
      "learning_rate": 2.7174696190979397e-06,
      "loss": 0.4671,
      "step": 945000
    },
    {
      "epoch": 15.138415229678019,
      "grad_norm": 2.59273099899292,
      "learning_rate": 2.6924524072561926e-06,
      "loss": 0.4718,
      "step": 945500
    },
    {
      "epoch": 15.146420737467377,
      "grad_norm": 5.944637298583984,
      "learning_rate": 2.667435195414445e-06,
      "loss": 0.4753,
      "step": 946000
    },
    {
      "epoch": 15.154426245256737,
      "grad_norm": 5.674914360046387,
      "learning_rate": 2.642417983572698e-06,
      "loss": 0.4869,
      "step": 946500
    },
    {
      "epoch": 15.162431753046096,
      "grad_norm": 4.496495723724365,
      "learning_rate": 2.617400771730951e-06,
      "loss": 0.4744,
      "step": 947000
    },
    {
      "epoch": 15.170437260835454,
      "grad_norm": 2.9867048263549805,
      "learning_rate": 2.592383559889204e-06,
      "loss": 0.477,
      "step": 947500
    },
    {
      "epoch": 15.178442768624814,
      "grad_norm": 2.6401333808898926,
      "learning_rate": 2.567366348047457e-06,
      "loss": 0.4621,
      "step": 948000
    },
    {
      "epoch": 15.186448276414174,
      "grad_norm": 4.1794586181640625,
      "learning_rate": 2.5423491362057097e-06,
      "loss": 0.4815,
      "step": 948500
    },
    {
      "epoch": 15.194453784203532,
      "grad_norm": 5.43332052230835,
      "learning_rate": 2.5173319243639626e-06,
      "loss": 0.4577,
      "step": 949000
    },
    {
      "epoch": 15.202459291992891,
      "grad_norm": 4.122920513153076,
      "learning_rate": 2.492314712522215e-06,
      "loss": 0.4783,
      "step": 949500
    },
    {
      "epoch": 15.210464799782251,
      "grad_norm": 2.8691084384918213,
      "learning_rate": 2.4672975006804685e-06,
      "loss": 0.4666,
      "step": 950000
    },
    {
      "epoch": 15.218470307571609,
      "grad_norm": 5.039830684661865,
      "learning_rate": 2.4422802888387214e-06,
      "loss": 0.4706,
      "step": 950500
    },
    {
      "epoch": 15.226475815360969,
      "grad_norm": 3.635134220123291,
      "learning_rate": 2.417263076996974e-06,
      "loss": 0.4789,
      "step": 951000
    },
    {
      "epoch": 15.234481323150327,
      "grad_norm": 15.387380599975586,
      "learning_rate": 2.392245865155227e-06,
      "loss": 0.4618,
      "step": 951500
    },
    {
      "epoch": 15.242486830939686,
      "grad_norm": 6.385750770568848,
      "learning_rate": 2.3672286533134797e-06,
      "loss": 0.4696,
      "step": 952000
    },
    {
      "epoch": 15.250492338729046,
      "grad_norm": 7.6569132804870605,
      "learning_rate": 2.3422114414717327e-06,
      "loss": 0.4583,
      "step": 952500
    },
    {
      "epoch": 15.258497846518404,
      "grad_norm": 4.083571910858154,
      "learning_rate": 2.3171942296299856e-06,
      "loss": 0.4655,
      "step": 953000
    },
    {
      "epoch": 15.266503354307764,
      "grad_norm": 9.765698432922363,
      "learning_rate": 2.2921770177882385e-06,
      "loss": 0.4705,
      "step": 953500
    },
    {
      "epoch": 15.274508862097123,
      "grad_norm": 3.6966004371643066,
      "learning_rate": 2.2671598059464914e-06,
      "loss": 0.4774,
      "step": 954000
    },
    {
      "epoch": 15.282514369886481,
      "grad_norm": 5.196597576141357,
      "learning_rate": 2.242142594104744e-06,
      "loss": 0.4734,
      "step": 954500
    },
    {
      "epoch": 15.290519877675841,
      "grad_norm": 5.534942626953125,
      "learning_rate": 2.2171253822629973e-06,
      "loss": 0.4771,
      "step": 955000
    },
    {
      "epoch": 15.2985253854652,
      "grad_norm": 6.788665771484375,
      "learning_rate": 2.19210817042125e-06,
      "loss": 0.4691,
      "step": 955500
    },
    {
      "epoch": 15.306530893254559,
      "grad_norm": 3.763272523880005,
      "learning_rate": 2.1670909585795027e-06,
      "loss": 0.4791,
      "step": 956000
    },
    {
      "epoch": 15.314536401043918,
      "grad_norm": 4.258082389831543,
      "learning_rate": 2.1420737467377556e-06,
      "loss": 0.4633,
      "step": 956500
    },
    {
      "epoch": 15.322541908833276,
      "grad_norm": 9.78219223022461,
      "learning_rate": 2.1170565348960085e-06,
      "loss": 0.4901,
      "step": 957000
    },
    {
      "epoch": 15.330547416622636,
      "grad_norm": 3.4021096229553223,
      "learning_rate": 2.0920393230542614e-06,
      "loss": 0.4772,
      "step": 957500
    },
    {
      "epoch": 15.338552924411996,
      "grad_norm": 5.799580097198486,
      "learning_rate": 2.0670221112125143e-06,
      "loss": 0.4825,
      "step": 958000
    },
    {
      "epoch": 15.346558432201354,
      "grad_norm": 6.026081562042236,
      "learning_rate": 2.0420048993707673e-06,
      "loss": 0.4666,
      "step": 958500
    },
    {
      "epoch": 15.354563939990713,
      "grad_norm": 5.689210414886475,
      "learning_rate": 2.01698768752902e-06,
      "loss": 0.4617,
      "step": 959000
    },
    {
      "epoch": 15.362569447780073,
      "grad_norm": 1.996969223022461,
      "learning_rate": 1.9919704756872727e-06,
      "loss": 0.4777,
      "step": 959500
    },
    {
      "epoch": 15.370574955569431,
      "grad_norm": 5.068509578704834,
      "learning_rate": 1.966953263845526e-06,
      "loss": 0.4662,
      "step": 960000
    },
    {
      "epoch": 15.37858046335879,
      "grad_norm": 4.670975208282471,
      "learning_rate": 1.941936052003779e-06,
      "loss": 0.4772,
      "step": 960500
    },
    {
      "epoch": 15.38658597114815,
      "grad_norm": 5.728074073791504,
      "learning_rate": 1.9169188401620314e-06,
      "loss": 0.479,
      "step": 961000
    },
    {
      "epoch": 15.394591478937508,
      "grad_norm": 6.503870487213135,
      "learning_rate": 1.8919016283202844e-06,
      "loss": 0.4669,
      "step": 961500
    },
    {
      "epoch": 15.402596986726868,
      "grad_norm": 3.8193085193634033,
      "learning_rate": 1.8668844164785375e-06,
      "loss": 0.4727,
      "step": 962000
    },
    {
      "epoch": 15.410602494516228,
      "grad_norm": 3.2925548553466797,
      "learning_rate": 1.84186720463679e-06,
      "loss": 0.4667,
      "step": 962500
    },
    {
      "epoch": 15.418608002305586,
      "grad_norm": 4.211548328399658,
      "learning_rate": 1.8168499927950431e-06,
      "loss": 0.4585,
      "step": 963000
    },
    {
      "epoch": 15.426613510094946,
      "grad_norm": 2.3337337970733643,
      "learning_rate": 1.791832780953296e-06,
      "loss": 0.4653,
      "step": 963500
    },
    {
      "epoch": 15.434619017884305,
      "grad_norm": 3.306063413619995,
      "learning_rate": 1.766815569111549e-06,
      "loss": 0.4689,
      "step": 964000
    },
    {
      "epoch": 15.442624525673663,
      "grad_norm": 3.9295687675476074,
      "learning_rate": 1.7417983572698017e-06,
      "loss": 0.4946,
      "step": 964500
    },
    {
      "epoch": 15.450630033463023,
      "grad_norm": 3.2988319396972656,
      "learning_rate": 1.7167811454280546e-06,
      "loss": 0.4735,
      "step": 965000
    },
    {
      "epoch": 15.45863554125238,
      "grad_norm": 4.605602264404297,
      "learning_rate": 1.6917639335863075e-06,
      "loss": 0.4558,
      "step": 965500
    },
    {
      "epoch": 15.46664104904174,
      "grad_norm": 11.229503631591797,
      "learning_rate": 1.6667467217445602e-06,
      "loss": 0.4564,
      "step": 966000
    },
    {
      "epoch": 15.4746465568311,
      "grad_norm": 13.219212532043457,
      "learning_rate": 1.6417295099028131e-06,
      "loss": 0.4765,
      "step": 966500
    },
    {
      "epoch": 15.482652064620458,
      "grad_norm": 3.1825075149536133,
      "learning_rate": 1.6167122980610663e-06,
      "loss": 0.4634,
      "step": 967000
    },
    {
      "epoch": 15.490657572409818,
      "grad_norm": 6.901919364929199,
      "learning_rate": 1.5916950862193188e-06,
      "loss": 0.4626,
      "step": 967500
    },
    {
      "epoch": 15.498663080199178,
      "grad_norm": 5.75392484664917,
      "learning_rate": 1.5666778743775719e-06,
      "loss": 0.4772,
      "step": 968000
    },
    {
      "epoch": 15.506668587988536,
      "grad_norm": 4.439280986785889,
      "learning_rate": 1.5416606625358248e-06,
      "loss": 0.4754,
      "step": 968500
    },
    {
      "epoch": 15.514674095777895,
      "grad_norm": 3.112225294113159,
      "learning_rate": 1.5166434506940775e-06,
      "loss": 0.4805,
      "step": 969000
    },
    {
      "epoch": 15.522679603567255,
      "grad_norm": 3.92714524269104,
      "learning_rate": 1.4916262388523304e-06,
      "loss": 0.4811,
      "step": 969500
    },
    {
      "epoch": 15.530685111356613,
      "grad_norm": 3.3060784339904785,
      "learning_rate": 1.4666090270105834e-06,
      "loss": 0.4745,
      "step": 970000
    },
    {
      "epoch": 15.538690619145973,
      "grad_norm": 2.5399792194366455,
      "learning_rate": 1.4415918151688363e-06,
      "loss": 0.4796,
      "step": 970500
    },
    {
      "epoch": 15.546696126935332,
      "grad_norm": 10.595091819763184,
      "learning_rate": 1.4165746033270892e-06,
      "loss": 0.4743,
      "step": 971000
    },
    {
      "epoch": 15.55470163472469,
      "grad_norm": 9.073383331298828,
      "learning_rate": 1.391557391485342e-06,
      "loss": 0.4511,
      "step": 971500
    },
    {
      "epoch": 15.56270714251405,
      "grad_norm": 5.173275470733643,
      "learning_rate": 1.3665401796435948e-06,
      "loss": 0.4682,
      "step": 972000
    },
    {
      "epoch": 15.57071265030341,
      "grad_norm": 2.058908224105835,
      "learning_rate": 1.3415229678018477e-06,
      "loss": 0.4703,
      "step": 972500
    },
    {
      "epoch": 15.578718158092768,
      "grad_norm": 3.4149222373962402,
      "learning_rate": 1.3165057559601007e-06,
      "loss": 0.4608,
      "step": 973000
    },
    {
      "epoch": 15.586723665882127,
      "grad_norm": 4.143109321594238,
      "learning_rate": 1.2914885441183536e-06,
      "loss": 0.4794,
      "step": 973500
    },
    {
      "epoch": 15.594729173671485,
      "grad_norm": 2.4470391273498535,
      "learning_rate": 1.2664713322766063e-06,
      "loss": 0.4871,
      "step": 974000
    },
    {
      "epoch": 15.602734681460845,
      "grad_norm": 2.6164770126342773,
      "learning_rate": 1.2414541204348592e-06,
      "loss": 0.4591,
      "step": 974500
    },
    {
      "epoch": 15.610740189250205,
      "grad_norm": 4.587658405303955,
      "learning_rate": 1.2164369085931121e-06,
      "loss": 0.4604,
      "step": 975000
    },
    {
      "epoch": 15.618745697039563,
      "grad_norm": 6.425861835479736,
      "learning_rate": 1.191419696751365e-06,
      "loss": 0.4774,
      "step": 975500
    },
    {
      "epoch": 15.626751204828922,
      "grad_norm": 1.9393123388290405,
      "learning_rate": 1.1664024849096177e-06,
      "loss": 0.4847,
      "step": 976000
    },
    {
      "epoch": 15.634756712618282,
      "grad_norm": 2.293111562728882,
      "learning_rate": 1.1413852730678707e-06,
      "loss": 0.4659,
      "step": 976500
    },
    {
      "epoch": 15.64276222040764,
      "grad_norm": 6.576907634735107,
      "learning_rate": 1.1163680612261236e-06,
      "loss": 0.4778,
      "step": 977000
    },
    {
      "epoch": 15.650767728197,
      "grad_norm": 5.270922660827637,
      "learning_rate": 1.0913508493843765e-06,
      "loss": 0.4714,
      "step": 977500
    },
    {
      "epoch": 15.65877323598636,
      "grad_norm": 3.4276347160339355,
      "learning_rate": 1.0663336375426294e-06,
      "loss": 0.4691,
      "step": 978000
    },
    {
      "epoch": 15.666778743775717,
      "grad_norm": 9.778956413269043,
      "learning_rate": 1.0413164257008821e-06,
      "loss": 0.4671,
      "step": 978500
    },
    {
      "epoch": 15.674784251565077,
      "grad_norm": 3.4061334133148193,
      "learning_rate": 1.016299213859135e-06,
      "loss": 0.4656,
      "step": 979000
    },
    {
      "epoch": 15.682789759354435,
      "grad_norm": 5.84296989440918,
      "learning_rate": 9.91282002017388e-07,
      "loss": 0.4774,
      "step": 979500
    },
    {
      "epoch": 15.690795267143795,
      "grad_norm": 18.14995002746582,
      "learning_rate": 9.662647901756409e-07,
      "loss": 0.4584,
      "step": 980000
    },
    {
      "epoch": 15.698800774933154,
      "grad_norm": 2.6703319549560547,
      "learning_rate": 9.412475783338937e-07,
      "loss": 0.4684,
      "step": 980500
    },
    {
      "epoch": 15.706806282722512,
      "grad_norm": 2.9578683376312256,
      "learning_rate": 9.162303664921465e-07,
      "loss": 0.4618,
      "step": 981000
    },
    {
      "epoch": 15.714811790511872,
      "grad_norm": 2.856483221054077,
      "learning_rate": 8.912131546503995e-07,
      "loss": 0.4815,
      "step": 981500
    },
    {
      "epoch": 15.722817298301232,
      "grad_norm": 8.02392578125,
      "learning_rate": 8.661959428086524e-07,
      "loss": 0.4659,
      "step": 982000
    },
    {
      "epoch": 15.73082280609059,
      "grad_norm": 1.0176723003387451,
      "learning_rate": 8.411787309669054e-07,
      "loss": 0.4729,
      "step": 982500
    },
    {
      "epoch": 15.73882831387995,
      "grad_norm": 4.965301513671875,
      "learning_rate": 8.161615191251582e-07,
      "loss": 0.4814,
      "step": 983000
    },
    {
      "epoch": 15.74683382166931,
      "grad_norm": 6.9442009925842285,
      "learning_rate": 7.911443072834109e-07,
      "loss": 0.4551,
      "step": 983500
    },
    {
      "epoch": 15.754839329458667,
      "grad_norm": 3.3702876567840576,
      "learning_rate": 7.661270954416639e-07,
      "loss": 0.4695,
      "step": 984000
    },
    {
      "epoch": 15.762844837248027,
      "grad_norm": 1.9732717275619507,
      "learning_rate": 7.411098835999167e-07,
      "loss": 0.4717,
      "step": 984500
    },
    {
      "epoch": 15.770850345037386,
      "grad_norm": 2.3195579051971436,
      "learning_rate": 7.160926717581697e-07,
      "loss": 0.4743,
      "step": 985000
    },
    {
      "epoch": 15.778855852826744,
      "grad_norm": 5.058508396148682,
      "learning_rate": 6.910754599164226e-07,
      "loss": 0.4763,
      "step": 985500
    },
    {
      "epoch": 15.786861360616104,
      "grad_norm": 4.193575859069824,
      "learning_rate": 6.660582480746754e-07,
      "loss": 0.4747,
      "step": 986000
    },
    {
      "epoch": 15.794866868405464,
      "grad_norm": 8.042194366455078,
      "learning_rate": 6.410410362329283e-07,
      "loss": 0.4608,
      "step": 986500
    },
    {
      "epoch": 15.802872376194822,
      "grad_norm": 4.464004039764404,
      "learning_rate": 6.160238243911811e-07,
      "loss": 0.4641,
      "step": 987000
    },
    {
      "epoch": 15.810877883984181,
      "grad_norm": 3.7822041511535645,
      "learning_rate": 5.91006612549434e-07,
      "loss": 0.4714,
      "step": 987500
    },
    {
      "epoch": 15.81888339177354,
      "grad_norm": 2.88623309135437,
      "learning_rate": 5.65989400707687e-07,
      "loss": 0.4747,
      "step": 988000
    },
    {
      "epoch": 15.8268888995629,
      "grad_norm": 4.40857458114624,
      "learning_rate": 5.409721888659398e-07,
      "loss": 0.4879,
      "step": 988500
    },
    {
      "epoch": 15.834894407352259,
      "grad_norm": 5.445592403411865,
      "learning_rate": 5.159549770241926e-07,
      "loss": 0.4625,
      "step": 989000
    },
    {
      "epoch": 15.842899915141617,
      "grad_norm": 5.780066967010498,
      "learning_rate": 4.909377651824455e-07,
      "loss": 0.4659,
      "step": 989500
    },
    {
      "epoch": 15.850905422930976,
      "grad_norm": 4.859716415405273,
      "learning_rate": 4.6592055334069844e-07,
      "loss": 0.469,
      "step": 990000
    },
    {
      "epoch": 15.858910930720336,
      "grad_norm": 4.285500526428223,
      "learning_rate": 4.409033414989513e-07,
      "loss": 0.4619,
      "step": 990500
    },
    {
      "epoch": 15.866916438509694,
      "grad_norm": 4.535587787628174,
      "learning_rate": 4.158861296572042e-07,
      "loss": 0.4678,
      "step": 991000
    },
    {
      "epoch": 15.874921946299054,
      "grad_norm": 3.8489558696746826,
      "learning_rate": 3.9086891781545704e-07,
      "loss": 0.4869,
      "step": 991500
    },
    {
      "epoch": 15.882927454088414,
      "grad_norm": 3.9636969566345215,
      "learning_rate": 3.658517059737099e-07,
      "loss": 0.4723,
      "step": 992000
    },
    {
      "epoch": 15.890932961877771,
      "grad_norm": 4.193367004394531,
      "learning_rate": 3.408344941319628e-07,
      "loss": 0.4849,
      "step": 992500
    },
    {
      "epoch": 15.898938469667131,
      "grad_norm": 3.654834747314453,
      "learning_rate": 3.158172822902157e-07,
      "loss": 0.4783,
      "step": 993000
    },
    {
      "epoch": 15.90694397745649,
      "grad_norm": 3.2229857444763184,
      "learning_rate": 2.9080007044846855e-07,
      "loss": 0.4781,
      "step": 993500
    },
    {
      "epoch": 15.914949485245849,
      "grad_norm": 4.470325946807861,
      "learning_rate": 2.657828586067215e-07,
      "loss": 0.478,
      "step": 994000
    },
    {
      "epoch": 15.922954993035209,
      "grad_norm": 13.357008934020996,
      "learning_rate": 2.407656467649743e-07,
      "loss": 0.4645,
      "step": 994500
    },
    {
      "epoch": 15.930960500824566,
      "grad_norm": 14.189038276672363,
      "learning_rate": 2.157484349232272e-07,
      "loss": 0.4801,
      "step": 995000
    },
    {
      "epoch": 15.938966008613926,
      "grad_norm": 4.876831531524658,
      "learning_rate": 1.9073122308148007e-07,
      "loss": 0.4758,
      "step": 995500
    },
    {
      "epoch": 15.946971516403286,
      "grad_norm": 2.641456127166748,
      "learning_rate": 1.6571401123973294e-07,
      "loss": 0.477,
      "step": 996000
    },
    {
      "epoch": 15.954977024192644,
      "grad_norm": 4.912203311920166,
      "learning_rate": 1.406967993979858e-07,
      "loss": 0.4577,
      "step": 996500
    },
    {
      "epoch": 15.962982531982004,
      "grad_norm": 8.308849334716797,
      "learning_rate": 1.1567958755623869e-07,
      "loss": 0.4574,
      "step": 997000
    },
    {
      "epoch": 15.970988039771363,
      "grad_norm": 6.512892246246338,
      "learning_rate": 9.066237571449158e-08,
      "loss": 0.4505,
      "step": 997500
    },
    {
      "epoch": 15.978993547560721,
      "grad_norm": 6.704353332519531,
      "learning_rate": 6.564516387274445e-08,
      "loss": 0.4758,
      "step": 998000
    },
    {
      "epoch": 15.986999055350081,
      "grad_norm": 5.751596450805664,
      "learning_rate": 4.062795203099733e-08,
      "loss": 0.4808,
      "step": 998500
    },
    {
      "epoch": 15.99500456313944,
      "grad_norm": 4.437801361083984,
      "learning_rate": 1.5610740189250205e-08,
      "loss": 0.4778,
      "step": 999000
    },
    {
      "epoch": 16.0030100709288,
      "grad_norm": 3.320068836212158,
      "learning_rate": 5.547194247420003e-06,
      "loss": 0.4997,
      "step": 999500
    },
    {
      "epoch": 16.01101557871816,
      "grad_norm": 3.3597958087921143,
      "learning_rate": 5.524956725782895e-06,
      "loss": 0.4907,
      "step": 1000000
    },
    {
      "epoch": 16.019021086507518,
      "grad_norm": 2.5223512649536133,
      "learning_rate": 5.502719204145786e-06,
      "loss": 0.4977,
      "step": 1000500
    },
    {
      "epoch": 16.027026594296878,
      "grad_norm": 4.685521602630615,
      "learning_rate": 5.480481682508677e-06,
      "loss": 0.4858,
      "step": 1001000
    },
    {
      "epoch": 16.035032102086234,
      "grad_norm": 2.3826425075531006,
      "learning_rate": 5.458244160871569e-06,
      "loss": 0.5017,
      "step": 1001500
    },
    {
      "epoch": 16.043037609875594,
      "grad_norm": 1.7845991849899292,
      "learning_rate": 5.436006639234461e-06,
      "loss": 0.4952,
      "step": 1002000
    },
    {
      "epoch": 16.051043117664953,
      "grad_norm": 4.193188190460205,
      "learning_rate": 5.413769117597352e-06,
      "loss": 0.4849,
      "step": 1002500
    },
    {
      "epoch": 16.059048625454313,
      "grad_norm": 2.8239097595214844,
      "learning_rate": 5.3915315959602435e-06,
      "loss": 0.4788,
      "step": 1003000
    },
    {
      "epoch": 16.067054133243673,
      "grad_norm": 3.9033710956573486,
      "learning_rate": 5.3692940743231345e-06,
      "loss": 0.4943,
      "step": 1003500
    },
    {
      "epoch": 16.075059641033032,
      "grad_norm": 4.129645347595215,
      "learning_rate": 5.3470565526860255e-06,
      "loss": 0.4995,
      "step": 1004000
    },
    {
      "epoch": 16.08306514882239,
      "grad_norm": 8.012463569641113,
      "learning_rate": 5.324819031048917e-06,
      "loss": 0.4824,
      "step": 1004500
    },
    {
      "epoch": 16.09107065661175,
      "grad_norm": 2.5393896102905273,
      "learning_rate": 5.302581509411808e-06,
      "loss": 0.5048,
      "step": 1005000
    },
    {
      "epoch": 16.099076164401108,
      "grad_norm": 3.5724642276763916,
      "learning_rate": 5.2803439877747e-06,
      "loss": 0.4881,
      "step": 1005500
    },
    {
      "epoch": 16.107081672190468,
      "grad_norm": 5.871524333953857,
      "learning_rate": 5.258106466137592e-06,
      "loss": 0.5013,
      "step": 1006000
    },
    {
      "epoch": 16.115087179979827,
      "grad_norm": 7.143259048461914,
      "learning_rate": 5.235868944500483e-06,
      "loss": 0.4914,
      "step": 1006500
    },
    {
      "epoch": 16.123092687769184,
      "grad_norm": 4.432036876678467,
      "learning_rate": 5.213631422863375e-06,
      "loss": 0.5028,
      "step": 1007000
    },
    {
      "epoch": 16.131098195558543,
      "grad_norm": 4.675901412963867,
      "learning_rate": 5.191393901226267e-06,
      "loss": 0.5008,
      "step": 1007500
    },
    {
      "epoch": 16.139103703347903,
      "grad_norm": 7.273356914520264,
      "learning_rate": 5.169156379589157e-06,
      "loss": 0.4934,
      "step": 1008000
    },
    {
      "epoch": 16.147109211137263,
      "grad_norm": 6.4344964027404785,
      "learning_rate": 5.146918857952049e-06,
      "loss": 0.4896,
      "step": 1008500
    },
    {
      "epoch": 16.155114718926622,
      "grad_norm": 4.01615047454834,
      "learning_rate": 5.124681336314941e-06,
      "loss": 0.4699,
      "step": 1009000
    },
    {
      "epoch": 16.163120226715982,
      "grad_norm": 4.257608890533447,
      "learning_rate": 5.102443814677832e-06,
      "loss": 0.4872,
      "step": 1009500
    },
    {
      "epoch": 16.17112573450534,
      "grad_norm": 5.8604817390441895,
      "learning_rate": 5.080206293040724e-06,
      "loss": 0.5093,
      "step": 1010000
    },
    {
      "epoch": 16.179131242294698,
      "grad_norm": 2.7904653549194336,
      "learning_rate": 5.0579687714036154e-06,
      "loss": 0.4922,
      "step": 1010500
    },
    {
      "epoch": 16.187136750084058,
      "grad_norm": 11.552020072937012,
      "learning_rate": 5.0357312497665065e-06,
      "loss": 0.4857,
      "step": 1011000
    },
    {
      "epoch": 16.195142257873417,
      "grad_norm": 3.6425905227661133,
      "learning_rate": 5.0134937281293975e-06,
      "loss": 0.5033,
      "step": 1011500
    },
    {
      "epoch": 16.203147765662777,
      "grad_norm": 2.6695234775543213,
      "learning_rate": 4.991256206492289e-06,
      "loss": 0.4995,
      "step": 1012000
    },
    {
      "epoch": 16.211153273452133,
      "grad_norm": 4.380670070648193,
      "learning_rate": 4.96901868485518e-06,
      "loss": 0.4846,
      "step": 1012500
    },
    {
      "epoch": 16.219158781241493,
      "grad_norm": 4.541496276855469,
      "learning_rate": 4.946781163218072e-06,
      "loss": 0.4947,
      "step": 1013000
    },
    {
      "epoch": 16.227164289030853,
      "grad_norm": 3.8630454540252686,
      "learning_rate": 4.924543641580963e-06,
      "loss": 0.4872,
      "step": 1013500
    },
    {
      "epoch": 16.235169796820212,
      "grad_norm": 3.7287585735321045,
      "learning_rate": 4.902306119943855e-06,
      "loss": 0.4993,
      "step": 1014000
    },
    {
      "epoch": 16.243175304609572,
      "grad_norm": 2.1855852603912354,
      "learning_rate": 4.880068598306747e-06,
      "loss": 0.502,
      "step": 1014500
    },
    {
      "epoch": 16.251180812398932,
      "grad_norm": 4.8124470710754395,
      "learning_rate": 4.857831076669637e-06,
      "loss": 0.4924,
      "step": 1015000
    },
    {
      "epoch": 16.259186320188288,
      "grad_norm": 4.140233039855957,
      "learning_rate": 4.835593555032529e-06,
      "loss": 0.4876,
      "step": 1015500
    },
    {
      "epoch": 16.267191827977648,
      "grad_norm": 6.230991840362549,
      "learning_rate": 4.813356033395421e-06,
      "loss": 0.5,
      "step": 1016000
    },
    {
      "epoch": 16.275197335767007,
      "grad_norm": 7.618268966674805,
      "learning_rate": 4.791118511758312e-06,
      "loss": 0.4979,
      "step": 1016500
    },
    {
      "epoch": 16.283202843556367,
      "grad_norm": 3.917677164077759,
      "learning_rate": 4.768880990121204e-06,
      "loss": 0.4847,
      "step": 1017000
    },
    {
      "epoch": 16.291208351345727,
      "grad_norm": 4.517875671386719,
      "learning_rate": 4.7466434684840956e-06,
      "loss": 0.4756,
      "step": 1017500
    },
    {
      "epoch": 16.299213859135087,
      "grad_norm": 3.0597102642059326,
      "learning_rate": 4.7244059468469866e-06,
      "loss": 0.5002,
      "step": 1018000
    },
    {
      "epoch": 16.307219366924443,
      "grad_norm": 4.526713848114014,
      "learning_rate": 4.7021684252098784e-06,
      "loss": 0.4892,
      "step": 1018500
    },
    {
      "epoch": 16.315224874713802,
      "grad_norm": 3.196087121963501,
      "learning_rate": 4.6799309035727694e-06,
      "loss": 0.4615,
      "step": 1019000
    },
    {
      "epoch": 16.323230382503162,
      "grad_norm": 4.769159317016602,
      "learning_rate": 4.6576933819356605e-06,
      "loss": 0.4949,
      "step": 1019500
    },
    {
      "epoch": 16.331235890292522,
      "grad_norm": 3.1491003036499023,
      "learning_rate": 4.635455860298552e-06,
      "loss": 0.4862,
      "step": 1020000
    },
    {
      "epoch": 16.33924139808188,
      "grad_norm": 3.2873377799987793,
      "learning_rate": 4.613218338661443e-06,
      "loss": 0.4972,
      "step": 1020500
    },
    {
      "epoch": 16.347246905871238,
      "grad_norm": 3.1412196159362793,
      "learning_rate": 4.590980817024335e-06,
      "loss": 0.4853,
      "step": 1021000
    },
    {
      "epoch": 16.355252413660597,
      "grad_norm": 2.1586968898773193,
      "learning_rate": 4.568743295387227e-06,
      "loss": 0.4842,
      "step": 1021500
    },
    {
      "epoch": 16.363257921449957,
      "grad_norm": 3.060781717300415,
      "learning_rate": 4.546505773750118e-06,
      "loss": 0.5042,
      "step": 1022000
    },
    {
      "epoch": 16.371263429239317,
      "grad_norm": 10.17690372467041,
      "learning_rate": 4.524268252113009e-06,
      "loss": 0.5114,
      "step": 1022500
    },
    {
      "epoch": 16.379268937028677,
      "grad_norm": 4.566922187805176,
      "learning_rate": 4.502030730475901e-06,
      "loss": 0.4892,
      "step": 1023000
    },
    {
      "epoch": 16.387274444818036,
      "grad_norm": 6.314723491668701,
      "learning_rate": 4.479793208838792e-06,
      "loss": 0.4939,
      "step": 1023500
    },
    {
      "epoch": 16.395279952607392,
      "grad_norm": 3.4784555435180664,
      "learning_rate": 4.457555687201684e-06,
      "loss": 0.4822,
      "step": 1024000
    },
    {
      "epoch": 16.403285460396752,
      "grad_norm": 6.998312950134277,
      "learning_rate": 4.435318165564576e-06,
      "loss": 0.496,
      "step": 1024500
    },
    {
      "epoch": 16.411290968186112,
      "grad_norm": 3.4252848625183105,
      "learning_rate": 4.413080643927467e-06,
      "loss": 0.4871,
      "step": 1025000
    },
    {
      "epoch": 16.41929647597547,
      "grad_norm": 6.095332145690918,
      "learning_rate": 4.3908431222903586e-06,
      "loss": 0.4815,
      "step": 1025500
    },
    {
      "epoch": 16.42730198376483,
      "grad_norm": 2.8394761085510254,
      "learning_rate": 4.3686056006532496e-06,
      "loss": 0.4956,
      "step": 1026000
    },
    {
      "epoch": 16.43530749155419,
      "grad_norm": 2.8418352603912354,
      "learning_rate": 4.346368079016141e-06,
      "loss": 0.4992,
      "step": 1026500
    },
    {
      "epoch": 16.443312999343547,
      "grad_norm": 4.407130718231201,
      "learning_rate": 4.3241305573790324e-06,
      "loss": 0.4887,
      "step": 1027000
    },
    {
      "epoch": 16.451318507132907,
      "grad_norm": 1.8019049167633057,
      "learning_rate": 4.301893035741924e-06,
      "loss": 0.4912,
      "step": 1027500
    },
    {
      "epoch": 16.459324014922267,
      "grad_norm": 8.818528175354004,
      "learning_rate": 4.279655514104815e-06,
      "loss": 0.4734,
      "step": 1028000
    },
    {
      "epoch": 16.467329522711626,
      "grad_norm": 4.299025058746338,
      "learning_rate": 4.257417992467707e-06,
      "loss": 0.495,
      "step": 1028500
    },
    {
      "epoch": 16.475335030500986,
      "grad_norm": 3.9051029682159424,
      "learning_rate": 4.235180470830598e-06,
      "loss": 0.4823,
      "step": 1029000
    },
    {
      "epoch": 16.483340538290342,
      "grad_norm": 3.679758310317993,
      "learning_rate": 4.21294294919349e-06,
      "loss": 0.4786,
      "step": 1029500
    },
    {
      "epoch": 16.491346046079702,
      "grad_norm": 3.0780327320098877,
      "learning_rate": 4.190705427556381e-06,
      "loss": 0.4926,
      "step": 1030000
    },
    {
      "epoch": 16.49935155386906,
      "grad_norm": 4.135069370269775,
      "learning_rate": 4.168467905919272e-06,
      "loss": 0.4807,
      "step": 1030500
    },
    {
      "epoch": 16.50735706165842,
      "grad_norm": 4.281737804412842,
      "learning_rate": 4.146230384282164e-06,
      "loss": 0.476,
      "step": 1031000
    },
    {
      "epoch": 16.51536256944778,
      "grad_norm": 7.540194034576416,
      "learning_rate": 4.123992862645056e-06,
      "loss": 0.4966,
      "step": 1031500
    },
    {
      "epoch": 16.52336807723714,
      "grad_norm": 1.3562052249908447,
      "learning_rate": 4.101755341007947e-06,
      "loss": 0.4905,
      "step": 1032000
    },
    {
      "epoch": 16.531373585026497,
      "grad_norm": 4.118293762207031,
      "learning_rate": 4.079517819370839e-06,
      "loss": 0.4929,
      "step": 1032500
    },
    {
      "epoch": 16.539379092815857,
      "grad_norm": 3.9289443492889404,
      "learning_rate": 4.0572802977337305e-06,
      "loss": 0.4968,
      "step": 1033000
    },
    {
      "epoch": 16.547384600605216,
      "grad_norm": 3.566805601119995,
      "learning_rate": 4.0350427760966215e-06,
      "loss": 0.4869,
      "step": 1033500
    },
    {
      "epoch": 16.555390108394576,
      "grad_norm": 1.995937705039978,
      "learning_rate": 4.0128052544595126e-06,
      "loss": 0.4759,
      "step": 1034000
    },
    {
      "epoch": 16.563395616183936,
      "grad_norm": 5.11262845993042,
      "learning_rate": 3.990567732822404e-06,
      "loss": 0.5006,
      "step": 1034500
    },
    {
      "epoch": 16.571401123973295,
      "grad_norm": 5.200411796569824,
      "learning_rate": 3.9683302111852954e-06,
      "loss": 0.4878,
      "step": 1035000
    },
    {
      "epoch": 16.57940663176265,
      "grad_norm": 4.549868106842041,
      "learning_rate": 3.946092689548187e-06,
      "loss": 0.4936,
      "step": 1035500
    },
    {
      "epoch": 16.58741213955201,
      "grad_norm": 5.6479926109313965,
      "learning_rate": 3.923855167911078e-06,
      "loss": 0.4854,
      "step": 1036000
    },
    {
      "epoch": 16.59541764734137,
      "grad_norm": 6.808571815490723,
      "learning_rate": 3.90161764627397e-06,
      "loss": 0.4719,
      "step": 1036500
    },
    {
      "epoch": 16.60342315513073,
      "grad_norm": 3.1923272609710693,
      "learning_rate": 3.879380124636862e-06,
      "loss": 0.476,
      "step": 1037000
    },
    {
      "epoch": 16.61142866292009,
      "grad_norm": 3.343132257461548,
      "learning_rate": 3.857142602999752e-06,
      "loss": 0.4784,
      "step": 1037500
    },
    {
      "epoch": 16.619434170709447,
      "grad_norm": 7.5450663566589355,
      "learning_rate": 3.834905081362644e-06,
      "loss": 0.4984,
      "step": 1038000
    },
    {
      "epoch": 16.627439678498806,
      "grad_norm": 4.570603847503662,
      "learning_rate": 3.8126675597255355e-06,
      "loss": 0.4795,
      "step": 1038500
    },
    {
      "epoch": 16.635445186288166,
      "grad_norm": 2.932119131088257,
      "learning_rate": 3.7904300380884273e-06,
      "loss": 0.4989,
      "step": 1039000
    },
    {
      "epoch": 16.643450694077526,
      "grad_norm": 2.962671995162964,
      "learning_rate": 3.7681925164513188e-06,
      "loss": 0.4925,
      "step": 1039500
    },
    {
      "epoch": 16.651456201866885,
      "grad_norm": 6.591310977935791,
      "learning_rate": 3.7459549948142102e-06,
      "loss": 0.4771,
      "step": 1040000
    },
    {
      "epoch": 16.659461709656245,
      "grad_norm": 5.415672779083252,
      "learning_rate": 3.7237174731771017e-06,
      "loss": 0.4945,
      "step": 1040500
    },
    {
      "epoch": 16.6674672174456,
      "grad_norm": 3.4777793884277344,
      "learning_rate": 3.7014799515399927e-06,
      "loss": 0.4893,
      "step": 1041000
    },
    {
      "epoch": 16.67547272523496,
      "grad_norm": 4.996813774108887,
      "learning_rate": 3.679242429902884e-06,
      "loss": 0.4811,
      "step": 1041500
    },
    {
      "epoch": 16.68347823302432,
      "grad_norm": 2.665595531463623,
      "learning_rate": 3.657004908265776e-06,
      "loss": 0.4895,
      "step": 1042000
    },
    {
      "epoch": 16.69148374081368,
      "grad_norm": 4.598361015319824,
      "learning_rate": 3.6347673866286674e-06,
      "loss": 0.488,
      "step": 1042500
    },
    {
      "epoch": 16.69948924860304,
      "grad_norm": 6.2233710289001465,
      "learning_rate": 3.612529864991559e-06,
      "loss": 0.4807,
      "step": 1043000
    },
    {
      "epoch": 16.707494756392396,
      "grad_norm": 2.9398999214172363,
      "learning_rate": 3.5902923433544503e-06,
      "loss": 0.5095,
      "step": 1043500
    },
    {
      "epoch": 16.715500264181756,
      "grad_norm": 1.7160375118255615,
      "learning_rate": 3.5680548217173417e-06,
      "loss": 0.4989,
      "step": 1044000
    },
    {
      "epoch": 16.723505771971116,
      "grad_norm": 4.945777416229248,
      "learning_rate": 3.5458173000802336e-06,
      "loss": 0.4869,
      "step": 1044500
    },
    {
      "epoch": 16.731511279760475,
      "grad_norm": 5.8653883934021,
      "learning_rate": 3.523579778443124e-06,
      "loss": 0.4911,
      "step": 1045000
    },
    {
      "epoch": 16.739516787549835,
      "grad_norm": 6.450119495391846,
      "learning_rate": 3.501342256806016e-06,
      "loss": 0.4704,
      "step": 1045500
    },
    {
      "epoch": 16.747522295339195,
      "grad_norm": 5.410782814025879,
      "learning_rate": 3.4791047351689075e-06,
      "loss": 0.4794,
      "step": 1046000
    },
    {
      "epoch": 16.75552780312855,
      "grad_norm": 3.938450336456299,
      "learning_rate": 3.456867213531799e-06,
      "loss": 0.4936,
      "step": 1046500
    },
    {
      "epoch": 16.76353331091791,
      "grad_norm": 3.2795467376708984,
      "learning_rate": 3.4346296918946903e-06,
      "loss": 0.4799,
      "step": 1047000
    },
    {
      "epoch": 16.77153881870727,
      "grad_norm": 6.184102535247803,
      "learning_rate": 3.412392170257582e-06,
      "loss": 0.4902,
      "step": 1047500
    },
    {
      "epoch": 16.77954432649663,
      "grad_norm": 3.711850166320801,
      "learning_rate": 3.3901546486204736e-06,
      "loss": 0.4783,
      "step": 1048000
    },
    {
      "epoch": 16.78754983428599,
      "grad_norm": 4.8438801765441895,
      "learning_rate": 3.3679171269833642e-06,
      "loss": 0.4839,
      "step": 1048500
    },
    {
      "epoch": 16.795555342075346,
      "grad_norm": 2.835169792175293,
      "learning_rate": 3.345679605346256e-06,
      "loss": 0.4699,
      "step": 1049000
    },
    {
      "epoch": 16.803560849864706,
      "grad_norm": 5.692667007446289,
      "learning_rate": 3.3234420837091475e-06,
      "loss": 0.4884,
      "step": 1049500
    },
    {
      "epoch": 16.811566357654065,
      "grad_norm": 2.854924201965332,
      "learning_rate": 3.301204562072039e-06,
      "loss": 0.4798,
      "step": 1050000
    },
    {
      "epoch": 16.819571865443425,
      "grad_norm": 6.449284553527832,
      "learning_rate": 3.2789670404349304e-06,
      "loss": 0.5063,
      "step": 1050500
    },
    {
      "epoch": 16.827577373232785,
      "grad_norm": 2.8463685512542725,
      "learning_rate": 3.2567295187978223e-06,
      "loss": 0.4812,
      "step": 1051000
    },
    {
      "epoch": 16.835582881022145,
      "grad_norm": 4.008872032165527,
      "learning_rate": 3.2344919971607137e-06,
      "loss": 0.4983,
      "step": 1051500
    },
    {
      "epoch": 16.8435883888115,
      "grad_norm": 2.792407274246216,
      "learning_rate": 3.212254475523605e-06,
      "loss": 0.4661,
      "step": 1052000
    },
    {
      "epoch": 16.85159389660086,
      "grad_norm": 4.086069107055664,
      "learning_rate": 3.190016953886496e-06,
      "loss": 0.5018,
      "step": 1052500
    },
    {
      "epoch": 16.85959940439022,
      "grad_norm": 5.187023639678955,
      "learning_rate": 3.1677794322493876e-06,
      "loss": 0.5044,
      "step": 1053000
    },
    {
      "epoch": 16.86760491217958,
      "grad_norm": 3.019991159439087,
      "learning_rate": 3.145541910612279e-06,
      "loss": 0.4963,
      "step": 1053500
    },
    {
      "epoch": 16.87561041996894,
      "grad_norm": 11.5669527053833,
      "learning_rate": 3.1233043889751705e-06,
      "loss": 0.489,
      "step": 1054000
    },
    {
      "epoch": 16.8836159277583,
      "grad_norm": 4.278346538543701,
      "learning_rate": 3.1010668673380623e-06,
      "loss": 0.4765,
      "step": 1054500
    },
    {
      "epoch": 16.891621435547655,
      "grad_norm": 6.455617427825928,
      "learning_rate": 3.0788293457009533e-06,
      "loss": 0.4899,
      "step": 1055000
    },
    {
      "epoch": 16.899626943337015,
      "grad_norm": 8.34457778930664,
      "learning_rate": 3.0565918240638448e-06,
      "loss": 0.482,
      "step": 1055500
    },
    {
      "epoch": 16.907632451126375,
      "grad_norm": 3.667945146560669,
      "learning_rate": 3.0343543024267366e-06,
      "loss": 0.5,
      "step": 1056000
    },
    {
      "epoch": 16.915637958915735,
      "grad_norm": 6.054594039916992,
      "learning_rate": 3.012116780789628e-06,
      "loss": 0.499,
      "step": 1056500
    },
    {
      "epoch": 16.923643466705094,
      "grad_norm": 6.340884208679199,
      "learning_rate": 2.989879259152519e-06,
      "loss": 0.4701,
      "step": 1057000
    },
    {
      "epoch": 16.93164897449445,
      "grad_norm": 4.584266662597656,
      "learning_rate": 2.9676417375154105e-06,
      "loss": 0.4815,
      "step": 1057500
    },
    {
      "epoch": 16.93965448228381,
      "grad_norm": 6.95589542388916,
      "learning_rate": 2.9454042158783024e-06,
      "loss": 0.4939,
      "step": 1058000
    },
    {
      "epoch": 16.94765999007317,
      "grad_norm": 4.7313032150268555,
      "learning_rate": 2.923166694241194e-06,
      "loss": 0.4947,
      "step": 1058500
    },
    {
      "epoch": 16.95566549786253,
      "grad_norm": 5.603786468505859,
      "learning_rate": 2.900929172604085e-06,
      "loss": 0.4935,
      "step": 1059000
    },
    {
      "epoch": 16.96367100565189,
      "grad_norm": 2.8887462615966797,
      "learning_rate": 2.8786916509669767e-06,
      "loss": 0.5032,
      "step": 1059500
    },
    {
      "epoch": 16.97167651344125,
      "grad_norm": 3.922787666320801,
      "learning_rate": 2.856454129329868e-06,
      "loss": 0.4796,
      "step": 1060000
    },
    {
      "epoch": 16.979682021230605,
      "grad_norm": 2.778318405151367,
      "learning_rate": 2.834216607692759e-06,
      "loss": 0.4817,
      "step": 1060500
    },
    {
      "epoch": 16.987687529019965,
      "grad_norm": 8.688809394836426,
      "learning_rate": 2.811979086055651e-06,
      "loss": 0.49,
      "step": 1061000
    },
    {
      "epoch": 16.995693036809325,
      "grad_norm": 2.1855015754699707,
      "learning_rate": 2.7897415644185424e-06,
      "loss": 0.4748,
      "step": 1061500
    },
    {
      "epoch": 17.003698544598684,
      "grad_norm": 2.5378165245056152,
      "learning_rate": 2.767504042781434e-06,
      "loss": 0.4888,
      "step": 1062000
    },
    {
      "epoch": 17.011704052388044,
      "grad_norm": 1.9112628698349,
      "learning_rate": 2.745266521144325e-06,
      "loss": 0.469,
      "step": 1062500
    },
    {
      "epoch": 17.019709560177404,
      "grad_norm": 4.953358173370361,
      "learning_rate": 2.7230289995072167e-06,
      "loss": 0.4646,
      "step": 1063000
    },
    {
      "epoch": 17.02771506796676,
      "grad_norm": 7.504002094268799,
      "learning_rate": 2.700791477870108e-06,
      "loss": 0.4785,
      "step": 1063500
    },
    {
      "epoch": 17.03572057575612,
      "grad_norm": 2.586487293243408,
      "learning_rate": 2.6785539562329996e-06,
      "loss": 0.48,
      "step": 1064000
    },
    {
      "epoch": 17.04372608354548,
      "grad_norm": 4.117012023925781,
      "learning_rate": 2.656316434595891e-06,
      "loss": 0.477,
      "step": 1064500
    },
    {
      "epoch": 17.05173159133484,
      "grad_norm": 2.545771837234497,
      "learning_rate": 2.6340789129587825e-06,
      "loss": 0.4853,
      "step": 1065000
    },
    {
      "epoch": 17.0597370991242,
      "grad_norm": 2.5988898277282715,
      "learning_rate": 2.611841391321674e-06,
      "loss": 0.4759,
      "step": 1065500
    },
    {
      "epoch": 17.067742606913555,
      "grad_norm": 4.172975540161133,
      "learning_rate": 2.5896038696845654e-06,
      "loss": 0.4797,
      "step": 1066000
    },
    {
      "epoch": 17.075748114702915,
      "grad_norm": 6.829773902893066,
      "learning_rate": 2.567366348047457e-06,
      "loss": 0.4898,
      "step": 1066500
    },
    {
      "epoch": 17.083753622492274,
      "grad_norm": 3.1716620922088623,
      "learning_rate": 2.5451288264103482e-06,
      "loss": 0.4692,
      "step": 1067000
    },
    {
      "epoch": 17.091759130281634,
      "grad_norm": 6.014225482940674,
      "learning_rate": 2.5228913047732397e-06,
      "loss": 0.4805,
      "step": 1067500
    },
    {
      "epoch": 17.099764638070994,
      "grad_norm": 5.569185256958008,
      "learning_rate": 2.500653783136131e-06,
      "loss": 0.4811,
      "step": 1068000
    },
    {
      "epoch": 17.107770145860353,
      "grad_norm": 11.598359107971191,
      "learning_rate": 2.4784162614990225e-06,
      "loss": 0.4741,
      "step": 1068500
    },
    {
      "epoch": 17.11577565364971,
      "grad_norm": 10.046777725219727,
      "learning_rate": 2.456178739861914e-06,
      "loss": 0.4674,
      "step": 1069000
    },
    {
      "epoch": 17.12378116143907,
      "grad_norm": 8.960810661315918,
      "learning_rate": 2.4339412182248054e-06,
      "loss": 0.4695,
      "step": 1069500
    },
    {
      "epoch": 17.13178666922843,
      "grad_norm": 7.672238349914551,
      "learning_rate": 2.411703696587697e-06,
      "loss": 0.4803,
      "step": 1070000
    },
    {
      "epoch": 17.13979217701779,
      "grad_norm": 3.7966647148132324,
      "learning_rate": 2.3894661749505883e-06,
      "loss": 0.4658,
      "step": 1070500
    },
    {
      "epoch": 17.14779768480715,
      "grad_norm": 5.155425548553467,
      "learning_rate": 2.3672286533134797e-06,
      "loss": 0.4761,
      "step": 1071000
    },
    {
      "epoch": 17.155803192596505,
      "grad_norm": 2.6474695205688477,
      "learning_rate": 2.3449911316763716e-06,
      "loss": 0.4641,
      "step": 1071500
    },
    {
      "epoch": 17.163808700385864,
      "grad_norm": 6.041805267333984,
      "learning_rate": 2.3227536100392626e-06,
      "loss": 0.4565,
      "step": 1072000
    },
    {
      "epoch": 17.171814208175224,
      "grad_norm": 3.8130807876586914,
      "learning_rate": 2.300516088402154e-06,
      "loss": 0.4946,
      "step": 1072500
    },
    {
      "epoch": 17.179819715964584,
      "grad_norm": 2.394489049911499,
      "learning_rate": 2.2782785667650455e-06,
      "loss": 0.4913,
      "step": 1073000
    },
    {
      "epoch": 17.187825223753944,
      "grad_norm": 2.631420373916626,
      "learning_rate": 2.256041045127937e-06,
      "loss": 0.4695,
      "step": 1073500
    },
    {
      "epoch": 17.195830731543303,
      "grad_norm": 8.450589179992676,
      "learning_rate": 2.2338035234908284e-06,
      "loss": 0.4772,
      "step": 1074000
    },
    {
      "epoch": 17.20383623933266,
      "grad_norm": 1.5755752325057983,
      "learning_rate": 2.21156600185372e-06,
      "loss": 0.4815,
      "step": 1074500
    },
    {
      "epoch": 17.21184174712202,
      "grad_norm": 3.3621928691864014,
      "learning_rate": 2.1893284802166117e-06,
      "loss": 0.4645,
      "step": 1075000
    },
    {
      "epoch": 17.21984725491138,
      "grad_norm": 8.1481294631958,
      "learning_rate": 2.1670909585795027e-06,
      "loss": 0.4693,
      "step": 1075500
    },
    {
      "epoch": 17.22785276270074,
      "grad_norm": 6.808199882507324,
      "learning_rate": 2.144853436942394e-06,
      "loss": 0.4575,
      "step": 1076000
    },
    {
      "epoch": 17.235858270490098,
      "grad_norm": 4.483650207519531,
      "learning_rate": 2.122615915305286e-06,
      "loss": 0.4756,
      "step": 1076500
    },
    {
      "epoch": 17.243863778279458,
      "grad_norm": 3.543853282928467,
      "learning_rate": 2.1003783936681774e-06,
      "loss": 0.4762,
      "step": 1077000
    },
    {
      "epoch": 17.251869286068814,
      "grad_norm": 2.9102375507354736,
      "learning_rate": 2.0781408720310684e-06,
      "loss": 0.4613,
      "step": 1077500
    },
    {
      "epoch": 17.259874793858174,
      "grad_norm": 5.152004718780518,
      "learning_rate": 2.05590335039396e-06,
      "loss": 0.4802,
      "step": 1078000
    },
    {
      "epoch": 17.267880301647534,
      "grad_norm": 2.351494550704956,
      "learning_rate": 2.0336658287568517e-06,
      "loss": 0.4809,
      "step": 1078500
    },
    {
      "epoch": 17.275885809436893,
      "grad_norm": 6.287476539611816,
      "learning_rate": 2.0114283071197427e-06,
      "loss": 0.4829,
      "step": 1079000
    },
    {
      "epoch": 17.283891317226253,
      "grad_norm": 5.727603435516357,
      "learning_rate": 1.989190785482634e-06,
      "loss": 0.455,
      "step": 1079500
    },
    {
      "epoch": 17.29189682501561,
      "grad_norm": 2.5893239974975586,
      "learning_rate": 1.966953263845526e-06,
      "loss": 0.4756,
      "step": 1080000
    },
    {
      "epoch": 17.29990233280497,
      "grad_norm": 3.0875186920166016,
      "learning_rate": 1.9447157422084175e-06,
      "loss": 0.4574,
      "step": 1080500
    },
    {
      "epoch": 17.30790784059433,
      "grad_norm": 2.968477964401245,
      "learning_rate": 1.9224782205713085e-06,
      "loss": 0.4689,
      "step": 1081000
    },
    {
      "epoch": 17.31591334838369,
      "grad_norm": 1.5762590169906616,
      "learning_rate": 1.9002406989342001e-06,
      "loss": 0.4691,
      "step": 1081500
    },
    {
      "epoch": 17.323918856173048,
      "grad_norm": 1.0662868022918701,
      "learning_rate": 1.8780031772970916e-06,
      "loss": 0.4729,
      "step": 1082000
    },
    {
      "epoch": 17.331924363962408,
      "grad_norm": 2.7883665561676025,
      "learning_rate": 1.8557656556599832e-06,
      "loss": 0.4882,
      "step": 1082500
    },
    {
      "epoch": 17.339929871751764,
      "grad_norm": 3.897179126739502,
      "learning_rate": 1.8335281340228744e-06,
      "loss": 0.4664,
      "step": 1083000
    },
    {
      "epoch": 17.347935379541124,
      "grad_norm": 3.7050023078918457,
      "learning_rate": 1.8112906123857659e-06,
      "loss": 0.4666,
      "step": 1083500
    },
    {
      "epoch": 17.355940887330483,
      "grad_norm": 5.30200719833374,
      "learning_rate": 1.7890530907486575e-06,
      "loss": 0.474,
      "step": 1084000
    },
    {
      "epoch": 17.363946395119843,
      "grad_norm": 5.489684104919434,
      "learning_rate": 1.766815569111549e-06,
      "loss": 0.4757,
      "step": 1084500
    },
    {
      "epoch": 17.371951902909203,
      "grad_norm": 3.866670608520508,
      "learning_rate": 1.7445780474744402e-06,
      "loss": 0.4663,
      "step": 1085000
    },
    {
      "epoch": 17.37995741069856,
      "grad_norm": 6.237705707550049,
      "learning_rate": 1.7223405258373318e-06,
      "loss": 0.4754,
      "step": 1085500
    },
    {
      "epoch": 17.38796291848792,
      "grad_norm": 2.606412410736084,
      "learning_rate": 1.7001030042002233e-06,
      "loss": 0.4694,
      "step": 1086000
    },
    {
      "epoch": 17.39596842627728,
      "grad_norm": 3.8146286010742188,
      "learning_rate": 1.6778654825631145e-06,
      "loss": 0.4589,
      "step": 1086500
    },
    {
      "epoch": 17.403973934066638,
      "grad_norm": 4.005591869354248,
      "learning_rate": 1.655627960926006e-06,
      "loss": 0.466,
      "step": 1087000
    },
    {
      "epoch": 17.411979441855998,
      "grad_norm": 2.106107473373413,
      "learning_rate": 1.6333904392888976e-06,
      "loss": 0.4811,
      "step": 1087500
    },
    {
      "epoch": 17.419984949645357,
      "grad_norm": 4.1516218185424805,
      "learning_rate": 1.611152917651789e-06,
      "loss": 0.4604,
      "step": 1088000
    },
    {
      "epoch": 17.427990457434714,
      "grad_norm": 6.943382740020752,
      "learning_rate": 1.5889153960146802e-06,
      "loss": 0.4852,
      "step": 1088500
    },
    {
      "epoch": 17.435995965224073,
      "grad_norm": 2.7392005920410156,
      "learning_rate": 1.5666778743775719e-06,
      "loss": 0.4674,
      "step": 1089000
    },
    {
      "epoch": 17.444001473013433,
      "grad_norm": 12.76200008392334,
      "learning_rate": 1.5444403527404633e-06,
      "loss": 0.4748,
      "step": 1089500
    },
    {
      "epoch": 17.452006980802793,
      "grad_norm": 4.714579105377197,
      "learning_rate": 1.5222028311033548e-06,
      "loss": 0.4758,
      "step": 1090000
    },
    {
      "epoch": 17.460012488592152,
      "grad_norm": 3.6814463138580322,
      "learning_rate": 1.4999653094662462e-06,
      "loss": 0.4719,
      "step": 1090500
    },
    {
      "epoch": 17.468017996381512,
      "grad_norm": 4.441279411315918,
      "learning_rate": 1.4777277878291376e-06,
      "loss": 0.4792,
      "step": 1091000
    },
    {
      "epoch": 17.47602350417087,
      "grad_norm": 2.6648309230804443,
      "learning_rate": 1.455490266192029e-06,
      "loss": 0.4767,
      "step": 1091500
    },
    {
      "epoch": 17.484029011960228,
      "grad_norm": 5.080042362213135,
      "learning_rate": 1.4332527445549205e-06,
      "loss": 0.4868,
      "step": 1092000
    },
    {
      "epoch": 17.492034519749588,
      "grad_norm": 4.419218063354492,
      "learning_rate": 1.411015222917812e-06,
      "loss": 0.4626,
      "step": 1092500
    },
    {
      "epoch": 17.500040027538947,
      "grad_norm": 2.406048536300659,
      "learning_rate": 1.3887777012807034e-06,
      "loss": 0.4874,
      "step": 1093000
    },
    {
      "epoch": 17.508045535328307,
      "grad_norm": 4.721379280090332,
      "learning_rate": 1.3665401796435948e-06,
      "loss": 0.4567,
      "step": 1093500
    },
    {
      "epoch": 17.516051043117663,
      "grad_norm": 5.225808143615723,
      "learning_rate": 1.3443026580064863e-06,
      "loss": 0.4733,
      "step": 1094000
    },
    {
      "epoch": 17.524056550907023,
      "grad_norm": 3.6231112480163574,
      "learning_rate": 1.3220651363693777e-06,
      "loss": 0.4381,
      "step": 1094500
    },
    {
      "epoch": 17.532062058696383,
      "grad_norm": 5.586578845977783,
      "learning_rate": 1.2998276147322693e-06,
      "loss": 0.4684,
      "step": 1095000
    },
    {
      "epoch": 17.540067566485742,
      "grad_norm": 9.29577922821045,
      "learning_rate": 1.2775900930951606e-06,
      "loss": 0.4768,
      "step": 1095500
    },
    {
      "epoch": 17.548073074275102,
      "grad_norm": 2.235805034637451,
      "learning_rate": 1.2553525714580522e-06,
      "loss": 0.478,
      "step": 1096000
    },
    {
      "epoch": 17.556078582064462,
      "grad_norm": 7.519118785858154,
      "learning_rate": 1.2331150498209434e-06,
      "loss": 0.4891,
      "step": 1096500
    },
    {
      "epoch": 17.564084089853818,
      "grad_norm": 3.9088447093963623,
      "learning_rate": 1.210877528183835e-06,
      "loss": 0.4806,
      "step": 1097000
    },
    {
      "epoch": 17.572089597643178,
      "grad_norm": 4.006049156188965,
      "learning_rate": 1.1886400065467265e-06,
      "loss": 0.4713,
      "step": 1097500
    },
    {
      "epoch": 17.580095105432537,
      "grad_norm": 4.295517921447754,
      "learning_rate": 1.1664024849096177e-06,
      "loss": 0.4684,
      "step": 1098000
    },
    {
      "epoch": 17.588100613221897,
      "grad_norm": 4.977668285369873,
      "learning_rate": 1.1441649632725094e-06,
      "loss": 0.4781,
      "step": 1098500
    },
    {
      "epoch": 17.596106121011257,
      "grad_norm": 4.406919956207275,
      "learning_rate": 1.1219274416354006e-06,
      "loss": 0.4853,
      "step": 1099000
    },
    {
      "epoch": 17.604111628800617,
      "grad_norm": 3.8351857662200928,
      "learning_rate": 1.0996899199982923e-06,
      "loss": 0.4894,
      "step": 1099500
    },
    {
      "epoch": 17.612117136589973,
      "grad_norm": 8.851205825805664,
      "learning_rate": 1.0774523983611835e-06,
      "loss": 0.4623,
      "step": 1100000
    },
    {
      "epoch": 17.620122644379332,
      "grad_norm": 5.859399318695068,
      "learning_rate": 1.0552148767240751e-06,
      "loss": 0.4941,
      "step": 1100500
    },
    {
      "epoch": 17.628128152168692,
      "grad_norm": 4.04964017868042,
      "learning_rate": 1.0329773550869666e-06,
      "loss": 0.4739,
      "step": 1101000
    },
    {
      "epoch": 17.636133659958052,
      "grad_norm": 3.636183261871338,
      "learning_rate": 1.010739833449858e-06,
      "loss": 0.4697,
      "step": 1101500
    },
    {
      "epoch": 17.64413916774741,
      "grad_norm": 3.4776365756988525,
      "learning_rate": 9.885023118127495e-07,
      "loss": 0.4872,
      "step": 1102000
    },
    {
      "epoch": 17.652144675536768,
      "grad_norm": 4.05740213394165,
      "learning_rate": 9.662647901756409e-07,
      "loss": 0.4823,
      "step": 1102500
    },
    {
      "epoch": 17.660150183326127,
      "grad_norm": 4.02537727355957,
      "learning_rate": 9.440272685385323e-07,
      "loss": 0.4533,
      "step": 1103000
    },
    {
      "epoch": 17.668155691115487,
      "grad_norm": 5.8192009925842285,
      "learning_rate": 9.217897469014239e-07,
      "loss": 0.4901,
      "step": 1103500
    },
    {
      "epoch": 17.676161198904847,
      "grad_norm": 2.576287031173706,
      "learning_rate": 8.995522252643152e-07,
      "loss": 0.4892,
      "step": 1104000
    },
    {
      "epoch": 17.684166706694207,
      "grad_norm": 3.171064853668213,
      "learning_rate": 8.773147036272065e-07,
      "loss": 0.4712,
      "step": 1104500
    },
    {
      "epoch": 17.692172214483566,
      "grad_norm": 4.71618127822876,
      "learning_rate": 8.550771819900981e-07,
      "loss": 0.4627,
      "step": 1105000
    },
    {
      "epoch": 17.700177722272922,
      "grad_norm": 17.784618377685547,
      "learning_rate": 8.328396603529895e-07,
      "loss": 0.4772,
      "step": 1105500
    },
    {
      "epoch": 17.708183230062282,
      "grad_norm": 1.5642120838165283,
      "learning_rate": 8.106021387158811e-07,
      "loss": 0.4958,
      "step": 1106000
    },
    {
      "epoch": 17.716188737851642,
      "grad_norm": 2.959164619445801,
      "learning_rate": 7.883646170787724e-07,
      "loss": 0.4831,
      "step": 1106500
    },
    {
      "epoch": 17.724194245641,
      "grad_norm": 9.001289367675781,
      "learning_rate": 7.661270954416639e-07,
      "loss": 0.4739,
      "step": 1107000
    },
    {
      "epoch": 17.73219975343036,
      "grad_norm": 6.052950859069824,
      "learning_rate": 7.438895738045554e-07,
      "loss": 0.4807,
      "step": 1107500
    },
    {
      "epoch": 17.74020526121972,
      "grad_norm": 4.315983772277832,
      "learning_rate": 7.216520521674468e-07,
      "loss": 0.4676,
      "step": 1108000
    },
    {
      "epoch": 17.748210769009077,
      "grad_norm": 3.7493340969085693,
      "learning_rate": 6.994145305303382e-07,
      "loss": 0.4837,
      "step": 1108500
    },
    {
      "epoch": 17.756216276798437,
      "grad_norm": 3.2906992435455322,
      "learning_rate": 6.771770088932297e-07,
      "loss": 0.4717,
      "step": 1109000
    },
    {
      "epoch": 17.764221784587797,
      "grad_norm": 2.7194695472717285,
      "learning_rate": 6.549394872561211e-07,
      "loss": 0.4786,
      "step": 1109500
    },
    {
      "epoch": 17.772227292377156,
      "grad_norm": 2.922560930252075,
      "learning_rate": 6.327019656190126e-07,
      "loss": 0.4866,
      "step": 1110000
    },
    {
      "epoch": 17.780232800166516,
      "grad_norm": 2.08493709564209,
      "learning_rate": 6.10464443981904e-07,
      "loss": 0.468,
      "step": 1110500
    },
    {
      "epoch": 17.788238307955872,
      "grad_norm": 6.392858505249023,
      "learning_rate": 5.882269223447954e-07,
      "loss": 0.4834,
      "step": 1111000
    },
    {
      "epoch": 17.796243815745232,
      "grad_norm": 5.745046615600586,
      "learning_rate": 5.65989400707687e-07,
      "loss": 0.4573,
      "step": 1111500
    },
    {
      "epoch": 17.80424932353459,
      "grad_norm": 2.2464184761047363,
      "learning_rate": 5.437518790705784e-07,
      "loss": 0.4694,
      "step": 1112000
    },
    {
      "epoch": 17.81225483132395,
      "grad_norm": 4.921762466430664,
      "learning_rate": 5.215143574334698e-07,
      "loss": 0.4512,
      "step": 1112500
    },
    {
      "epoch": 17.82026033911331,
      "grad_norm": 3.645191192626953,
      "learning_rate": 4.992768357963613e-07,
      "loss": 0.474,
      "step": 1113000
    },
    {
      "epoch": 17.82826584690267,
      "grad_norm": 4.278903961181641,
      "learning_rate": 4.770393141592527e-07,
      "loss": 0.4806,
      "step": 1113500
    },
    {
      "epoch": 17.836271354692027,
      "grad_norm": 3.8131752014160156,
      "learning_rate": 4.548017925221441e-07,
      "loss": 0.4696,
      "step": 1114000
    },
    {
      "epoch": 17.844276862481387,
      "grad_norm": 4.631896495819092,
      "learning_rate": 4.3256427088503554e-07,
      "loss": 0.4772,
      "step": 1114500
    },
    {
      "epoch": 17.852282370270746,
      "grad_norm": 2.4399619102478027,
      "learning_rate": 4.1032674924792703e-07,
      "loss": 0.462,
      "step": 1115000
    },
    {
      "epoch": 17.860287878060106,
      "grad_norm": 4.763491630554199,
      "learning_rate": 3.880892276108185e-07,
      "loss": 0.4898,
      "step": 1115500
    },
    {
      "epoch": 17.868293385849466,
      "grad_norm": 5.235447406768799,
      "learning_rate": 3.658517059737099e-07,
      "loss": 0.4719,
      "step": 1116000
    },
    {
      "epoch": 17.876298893638822,
      "grad_norm": 3.5353541374206543,
      "learning_rate": 3.436141843366014e-07,
      "loss": 0.4628,
      "step": 1116500
    },
    {
      "epoch": 17.88430440142818,
      "grad_norm": 7.0610833168029785,
      "learning_rate": 3.2137666269949283e-07,
      "loss": 0.4655,
      "step": 1117000
    },
    {
      "epoch": 17.89230990921754,
      "grad_norm": 3.4666523933410645,
      "learning_rate": 2.9913914106238427e-07,
      "loss": 0.4744,
      "step": 1117500
    },
    {
      "epoch": 17.9003154170069,
      "grad_norm": 4.214587211608887,
      "learning_rate": 2.769016194252757e-07,
      "loss": 0.4721,
      "step": 1118000
    },
    {
      "epoch": 17.90832092479626,
      "grad_norm": 5.83065938949585,
      "learning_rate": 2.546640977881672e-07,
      "loss": 0.4916,
      "step": 1118500
    },
    {
      "epoch": 17.91632643258562,
      "grad_norm": 3.6731925010681152,
      "learning_rate": 2.3242657615105858e-07,
      "loss": 0.4641,
      "step": 1119000
    },
    {
      "epoch": 17.924331940374977,
      "grad_norm": 2.5840108394622803,
      "learning_rate": 2.1018905451395004e-07,
      "loss": 0.4659,
      "step": 1119500
    },
    {
      "epoch": 17.932337448164336,
      "grad_norm": 3.795551061630249,
      "learning_rate": 1.879515328768415e-07,
      "loss": 0.4695,
      "step": 1120000
    },
    {
      "epoch": 17.940342955953696,
      "grad_norm": 4.62161111831665,
      "learning_rate": 1.6571401123973294e-07,
      "loss": 0.4784,
      "step": 1120500
    },
    {
      "epoch": 17.948348463743056,
      "grad_norm": 2.8283498287200928,
      "learning_rate": 1.434764896026244e-07,
      "loss": 0.4842,
      "step": 1121000
    },
    {
      "epoch": 17.956353971532415,
      "grad_norm": 4.1455078125,
      "learning_rate": 1.2123896796551584e-07,
      "loss": 0.4743,
      "step": 1121500
    },
    {
      "epoch": 17.96435947932177,
      "grad_norm": 8.017561912536621,
      "learning_rate": 9.900144632840728e-08,
      "loss": 0.4655,
      "step": 1122000
    },
    {
      "epoch": 17.97236498711113,
      "grad_norm": 6.243171691894531,
      "learning_rate": 7.676392469129873e-08,
      "loss": 0.4682,
      "step": 1122500
    },
    {
      "epoch": 17.98037049490049,
      "grad_norm": 5.816123008728027,
      "learning_rate": 5.452640305419018e-08,
      "loss": 0.466,
      "step": 1123000
    },
    {
      "epoch": 17.98837600268985,
      "grad_norm": 4.090620040893555,
      "learning_rate": 3.228888141708162e-08,
      "loss": 0.4694,
      "step": 1123500
    },
    {
      "epoch": 17.99638151047921,
      "grad_norm": 3.3039982318878174,
      "learning_rate": 1.0051359779973066e-08,
      "loss": 0.4842,
      "step": 1124000
    },
    {
      "epoch": 18.002945791040954,
      "grad_norm": 4.474339962005615,
      "learning_rate": 4.992635522397618e-06,
      "loss": 0.504,
      "step": 1124500
    },
    {
      "epoch": 18.010950658000063,
      "grad_norm": 8.516033172607422,
      "learning_rate": 4.97262335499984e-06,
      "loss": 0.4825,
      "step": 1125000
    },
    {
      "epoch": 18.018955524959175,
      "grad_norm": 6.543155193328857,
      "learning_rate": 4.952611187602062e-06,
      "loss": 0.4873,
      "step": 1125500
    },
    {
      "epoch": 18.026960391918287,
      "grad_norm": 4.995852470397949,
      "learning_rate": 4.932599020204284e-06,
      "loss": 0.5061,
      "step": 1126000
    },
    {
      "epoch": 18.0349652588774,
      "grad_norm": 4.642284393310547,
      "learning_rate": 4.912586852806507e-06,
      "loss": 0.4751,
      "step": 1126500
    },
    {
      "epoch": 18.042970125836508,
      "grad_norm": 2.6128604412078857,
      "learning_rate": 4.892574685408729e-06,
      "loss": 0.5052,
      "step": 1127000
    },
    {
      "epoch": 18.05097499279562,
      "grad_norm": 4.476958751678467,
      "learning_rate": 4.872562518010951e-06,
      "loss": 0.4937,
      "step": 1127500
    },
    {
      "epoch": 18.058979859754732,
      "grad_norm": 11.200394630432129,
      "learning_rate": 4.8525503506131735e-06,
      "loss": 0.4846,
      "step": 1128000
    },
    {
      "epoch": 18.06698472671384,
      "grad_norm": 5.9925994873046875,
      "learning_rate": 4.832538183215395e-06,
      "loss": 0.4893,
      "step": 1128500
    },
    {
      "epoch": 18.074989593672953,
      "grad_norm": 3.596742868423462,
      "learning_rate": 4.812526015817617e-06,
      "loss": 0.4988,
      "step": 1129000
    },
    {
      "epoch": 18.082994460632065,
      "grad_norm": 4.958249568939209,
      "learning_rate": 4.79251384841984e-06,
      "loss": 0.4738,
      "step": 1129500
    },
    {
      "epoch": 18.090999327591174,
      "grad_norm": 5.602156639099121,
      "learning_rate": 4.772501681022062e-06,
      "loss": 0.4934,
      "step": 1130000
    },
    {
      "epoch": 18.099004194550286,
      "grad_norm": 4.212394714355469,
      "learning_rate": 4.752489513624284e-06,
      "loss": 0.4923,
      "step": 1130500
    },
    {
      "epoch": 18.107009061509398,
      "grad_norm": 3.5643327236175537,
      "learning_rate": 4.7324773462265066e-06,
      "loss": 0.4911,
      "step": 1131000
    },
    {
      "epoch": 18.11501392846851,
      "grad_norm": 2.7900586128234863,
      "learning_rate": 4.7124651788287284e-06,
      "loss": 0.4846,
      "step": 1131500
    },
    {
      "epoch": 18.12301879542762,
      "grad_norm": 3.342073678970337,
      "learning_rate": 4.69245301143095e-06,
      "loss": 0.504,
      "step": 1132000
    },
    {
      "epoch": 18.13102366238673,
      "grad_norm": 6.608224391937256,
      "learning_rate": 4.672440844033173e-06,
      "loss": 0.4706,
      "step": 1132500
    },
    {
      "epoch": 18.139028529345843,
      "grad_norm": 5.465963363647461,
      "learning_rate": 4.652428676635395e-06,
      "loss": 0.4929,
      "step": 1133000
    },
    {
      "epoch": 18.147033396304952,
      "grad_norm": 4.925381660461426,
      "learning_rate": 4.632416509237617e-06,
      "loss": 0.4912,
      "step": 1133500
    },
    {
      "epoch": 18.155038263264064,
      "grad_norm": 8.146567344665527,
      "learning_rate": 4.612404341839839e-06,
      "loss": 0.4759,
      "step": 1134000
    },
    {
      "epoch": 18.163043130223176,
      "grad_norm": 6.677505016326904,
      "learning_rate": 4.592392174442061e-06,
      "loss": 0.4934,
      "step": 1134500
    },
    {
      "epoch": 18.17104799718229,
      "grad_norm": 5.377140045166016,
      "learning_rate": 4.5723800070442825e-06,
      "loss": 0.496,
      "step": 1135000
    },
    {
      "epoch": 18.179052864141397,
      "grad_norm": 2.600062131881714,
      "learning_rate": 4.552367839646505e-06,
      "loss": 0.4924,
      "step": 1135500
    },
    {
      "epoch": 18.18705773110051,
      "grad_norm": 4.703793048858643,
      "learning_rate": 4.532355672248727e-06,
      "loss": 0.4923,
      "step": 1136000
    },
    {
      "epoch": 18.19506259805962,
      "grad_norm": 2.8353140354156494,
      "learning_rate": 4.512343504850949e-06,
      "loss": 0.4906,
      "step": 1136500
    },
    {
      "epoch": 18.20306746501873,
      "grad_norm": 4.365902423858643,
      "learning_rate": 4.492331337453172e-06,
      "loss": 0.502,
      "step": 1137000
    },
    {
      "epoch": 18.211072331977842,
      "grad_norm": 5.35777473449707,
      "learning_rate": 4.472319170055394e-06,
      "loss": 0.4794,
      "step": 1137500
    },
    {
      "epoch": 18.219077198936954,
      "grad_norm": 4.797760963439941,
      "learning_rate": 4.4523070026576155e-06,
      "loss": 0.4871,
      "step": 1138000
    },
    {
      "epoch": 18.227082065896063,
      "grad_norm": 9.097350120544434,
      "learning_rate": 4.432294835259838e-06,
      "loss": 0.4923,
      "step": 1138500
    },
    {
      "epoch": 18.235086932855175,
      "grad_norm": 3.9990391731262207,
      "learning_rate": 4.41228266786206e-06,
      "loss": 0.482,
      "step": 1139000
    },
    {
      "epoch": 18.243091799814287,
      "grad_norm": 3.409621000289917,
      "learning_rate": 4.392270500464282e-06,
      "loss": 0.482,
      "step": 1139500
    },
    {
      "epoch": 18.2510966667734,
      "grad_norm": 2.842830181121826,
      "learning_rate": 4.372258333066505e-06,
      "loss": 0.4838,
      "step": 1140000
    },
    {
      "epoch": 18.259101533732508,
      "grad_norm": 7.091646194458008,
      "learning_rate": 4.352246165668727e-06,
      "loss": 0.478,
      "step": 1140500
    },
    {
      "epoch": 18.26710640069162,
      "grad_norm": 7.636477470397949,
      "learning_rate": 4.3322339982709486e-06,
      "loss": 0.4874,
      "step": 1141000
    },
    {
      "epoch": 18.275111267650733,
      "grad_norm": 7.309669494628906,
      "learning_rate": 4.312221830873171e-06,
      "loss": 0.4673,
      "step": 1141500
    },
    {
      "epoch": 18.28311613460984,
      "grad_norm": 7.755620956420898,
      "learning_rate": 4.292209663475393e-06,
      "loss": 0.4928,
      "step": 1142000
    },
    {
      "epoch": 18.291121001568953,
      "grad_norm": 2.72440767288208,
      "learning_rate": 4.272197496077615e-06,
      "loss": 0.4856,
      "step": 1142500
    },
    {
      "epoch": 18.299125868528066,
      "grad_norm": 3.856067419052124,
      "learning_rate": 4.252185328679838e-06,
      "loss": 0.4882,
      "step": 1143000
    },
    {
      "epoch": 18.307130735487178,
      "grad_norm": 2.5792887210845947,
      "learning_rate": 4.23217316128206e-06,
      "loss": 0.49,
      "step": 1143500
    },
    {
      "epoch": 18.315135602446286,
      "grad_norm": 2.889202833175659,
      "learning_rate": 4.212160993884282e-06,
      "loss": 0.4896,
      "step": 1144000
    },
    {
      "epoch": 18.3231404694054,
      "grad_norm": 3.374478816986084,
      "learning_rate": 4.192148826486504e-06,
      "loss": 0.4798,
      "step": 1144500
    },
    {
      "epoch": 18.33114533636451,
      "grad_norm": 3.6660332679748535,
      "learning_rate": 4.172136659088726e-06,
      "loss": 0.4754,
      "step": 1145000
    },
    {
      "epoch": 18.33915020332362,
      "grad_norm": 5.857443809509277,
      "learning_rate": 4.152124491690948e-06,
      "loss": 0.4848,
      "step": 1145500
    },
    {
      "epoch": 18.34715507028273,
      "grad_norm": 2.2304461002349854,
      "learning_rate": 4.132112324293171e-06,
      "loss": 0.4856,
      "step": 1146000
    },
    {
      "epoch": 18.355159937241844,
      "grad_norm": 4.985145092010498,
      "learning_rate": 4.112100156895393e-06,
      "loss": 0.4895,
      "step": 1146500
    },
    {
      "epoch": 18.363164804200956,
      "grad_norm": 2.0326454639434814,
      "learning_rate": 4.092087989497615e-06,
      "loss": 0.4842,
      "step": 1147000
    },
    {
      "epoch": 18.371169671160064,
      "grad_norm": 4.189046859741211,
      "learning_rate": 4.072075822099837e-06,
      "loss": 0.4841,
      "step": 1147500
    },
    {
      "epoch": 18.379174538119177,
      "grad_norm": 4.312851905822754,
      "learning_rate": 4.052063654702059e-06,
      "loss": 0.4971,
      "step": 1148000
    },
    {
      "epoch": 18.38717940507829,
      "grad_norm": 2.6502063274383545,
      "learning_rate": 4.032051487304281e-06,
      "loss": 0.4748,
      "step": 1148500
    },
    {
      "epoch": 18.395184272037397,
      "grad_norm": 3.508376121520996,
      "learning_rate": 4.012039319906504e-06,
      "loss": 0.4799,
      "step": 1149000
    },
    {
      "epoch": 18.40318913899651,
      "grad_norm": 3.7835512161254883,
      "learning_rate": 3.992027152508726e-06,
      "loss": 0.4998,
      "step": 1149500
    },
    {
      "epoch": 18.411194005955622,
      "grad_norm": 3.1637842655181885,
      "learning_rate": 3.972014985110948e-06,
      "loss": 0.4849,
      "step": 1150000
    },
    {
      "epoch": 18.41919887291473,
      "grad_norm": 2.1120548248291016,
      "learning_rate": 3.9520028177131695e-06,
      "loss": 0.4956,
      "step": 1150500
    },
    {
      "epoch": 18.427203739873843,
      "grad_norm": 3.964332342147827,
      "learning_rate": 3.931990650315391e-06,
      "loss": 0.4738,
      "step": 1151000
    },
    {
      "epoch": 18.435208606832955,
      "grad_norm": 10.134326934814453,
      "learning_rate": 3.911978482917614e-06,
      "loss": 0.4902,
      "step": 1151500
    },
    {
      "epoch": 18.443213473792067,
      "grad_norm": 2.4887290000915527,
      "learning_rate": 3.891966315519836e-06,
      "loss": 0.4934,
      "step": 1152000
    },
    {
      "epoch": 18.451218340751176,
      "grad_norm": 4.491779804229736,
      "learning_rate": 3.871954148122058e-06,
      "loss": 0.4922,
      "step": 1152500
    },
    {
      "epoch": 18.459223207710288,
      "grad_norm": 11.837944984436035,
      "learning_rate": 3.851941980724281e-06,
      "loss": 0.4788,
      "step": 1153000
    },
    {
      "epoch": 18.4672280746694,
      "grad_norm": 1.7387620210647583,
      "learning_rate": 3.8319298133265026e-06,
      "loss": 0.4914,
      "step": 1153500
    },
    {
      "epoch": 18.47523294162851,
      "grad_norm": 5.321355819702148,
      "learning_rate": 3.811917645928725e-06,
      "loss": 0.4654,
      "step": 1154000
    },
    {
      "epoch": 18.48323780858762,
      "grad_norm": 5.96406364440918,
      "learning_rate": 3.791905478530947e-06,
      "loss": 0.4817,
      "step": 1154500
    },
    {
      "epoch": 18.491242675546733,
      "grad_norm": 5.9456071853637695,
      "learning_rate": 3.771893311133169e-06,
      "loss": 0.487,
      "step": 1155000
    },
    {
      "epoch": 18.499247542505845,
      "grad_norm": 5.672395706176758,
      "learning_rate": 3.751881143735391e-06,
      "loss": 0.4777,
      "step": 1155500
    },
    {
      "epoch": 18.507252409464954,
      "grad_norm": 3.654193162918091,
      "learning_rate": 3.7318689763376137e-06,
      "loss": 0.476,
      "step": 1156000
    },
    {
      "epoch": 18.515257276424066,
      "grad_norm": 10.51172161102295,
      "learning_rate": 3.7118568089398356e-06,
      "loss": 0.4901,
      "step": 1156500
    },
    {
      "epoch": 18.523262143383178,
      "grad_norm": 4.521280288696289,
      "learning_rate": 3.6918446415420575e-06,
      "loss": 0.486,
      "step": 1157000
    },
    {
      "epoch": 18.531267010342287,
      "grad_norm": 2.106830596923828,
      "learning_rate": 3.67183247414428e-06,
      "loss": 0.4778,
      "step": 1157500
    },
    {
      "epoch": 18.5392718773014,
      "grad_norm": 2.6092071533203125,
      "learning_rate": 3.651820306746502e-06,
      "loss": 0.4927,
      "step": 1158000
    },
    {
      "epoch": 18.54727674426051,
      "grad_norm": 8.068127632141113,
      "learning_rate": 3.631808139348724e-06,
      "loss": 0.4648,
      "step": 1158500
    },
    {
      "epoch": 18.55528161121962,
      "grad_norm": 2.008131742477417,
      "learning_rate": 3.6117959719509467e-06,
      "loss": 0.4741,
      "step": 1159000
    },
    {
      "epoch": 18.563286478178732,
      "grad_norm": 4.979071140289307,
      "learning_rate": 3.5917838045531686e-06,
      "loss": 0.4902,
      "step": 1159500
    },
    {
      "epoch": 18.571291345137844,
      "grad_norm": 8.559883117675781,
      "learning_rate": 3.5717716371553905e-06,
      "loss": 0.4765,
      "step": 1160000
    },
    {
      "epoch": 18.579296212096956,
      "grad_norm": 5.060667514801025,
      "learning_rate": 3.551759469757613e-06,
      "loss": 0.4919,
      "step": 1160500
    },
    {
      "epoch": 18.587301079056065,
      "grad_norm": 9.778956413269043,
      "learning_rate": 3.531747302359835e-06,
      "loss": 0.4861,
      "step": 1161000
    },
    {
      "epoch": 18.595305946015177,
      "grad_norm": 3.2945938110351562,
      "learning_rate": 3.511735134962057e-06,
      "loss": 0.4775,
      "step": 1161500
    },
    {
      "epoch": 18.60331081297429,
      "grad_norm": 5.335166931152344,
      "learning_rate": 3.4917229675642793e-06,
      "loss": 0.4754,
      "step": 1162000
    },
    {
      "epoch": 18.611315679933398,
      "grad_norm": 5.940889835357666,
      "learning_rate": 3.471710800166501e-06,
      "loss": 0.4717,
      "step": 1162500
    },
    {
      "epoch": 18.61932054689251,
      "grad_norm": 3.539409875869751,
      "learning_rate": 3.451698632768723e-06,
      "loss": 0.49,
      "step": 1163000
    },
    {
      "epoch": 18.627325413851622,
      "grad_norm": 5.515932083129883,
      "learning_rate": 3.431686465370946e-06,
      "loss": 0.4852,
      "step": 1163500
    },
    {
      "epoch": 18.635330280810734,
      "grad_norm": 3.9176433086395264,
      "learning_rate": 3.4116742979731677e-06,
      "loss": 0.4926,
      "step": 1164000
    },
    {
      "epoch": 18.643335147769843,
      "grad_norm": 5.2425994873046875,
      "learning_rate": 3.3916621305753896e-06,
      "loss": 0.4896,
      "step": 1164500
    },
    {
      "epoch": 18.651340014728955,
      "grad_norm": 5.597903728485107,
      "learning_rate": 3.3716499631776123e-06,
      "loss": 0.4892,
      "step": 1165000
    },
    {
      "epoch": 18.659344881688067,
      "grad_norm": 8.55500316619873,
      "learning_rate": 3.3516377957798342e-06,
      "loss": 0.4727,
      "step": 1165500
    },
    {
      "epoch": 18.667349748647176,
      "grad_norm": 8.972051620483398,
      "learning_rate": 3.331625628382056e-06,
      "loss": 0.4814,
      "step": 1166000
    },
    {
      "epoch": 18.675354615606288,
      "grad_norm": 3.123560905456543,
      "learning_rate": 3.311613460984279e-06,
      "loss": 0.4741,
      "step": 1166500
    },
    {
      "epoch": 18.6833594825654,
      "grad_norm": 3.5972776412963867,
      "learning_rate": 3.2916012935865007e-06,
      "loss": 0.4876,
      "step": 1167000
    },
    {
      "epoch": 18.691364349524513,
      "grad_norm": 2.35406494140625,
      "learning_rate": 3.2715891261887226e-06,
      "loss": 0.4926,
      "step": 1167500
    },
    {
      "epoch": 18.69936921648362,
      "grad_norm": 3.842172861099243,
      "learning_rate": 3.2515769587909454e-06,
      "loss": 0.4749,
      "step": 1168000
    },
    {
      "epoch": 18.707374083442733,
      "grad_norm": 2.5136115550994873,
      "learning_rate": 3.2315647913931673e-06,
      "loss": 0.4679,
      "step": 1168500
    },
    {
      "epoch": 18.715378950401846,
      "grad_norm": 2.10617995262146,
      "learning_rate": 3.211552623995389e-06,
      "loss": 0.4985,
      "step": 1169000
    },
    {
      "epoch": 18.723383817360954,
      "grad_norm": 1.6239784955978394,
      "learning_rate": 3.1915404565976115e-06,
      "loss": 0.4682,
      "step": 1169500
    },
    {
      "epoch": 18.731388684320066,
      "grad_norm": 8.054064750671387,
      "learning_rate": 3.1715282891998333e-06,
      "loss": 0.489,
      "step": 1170000
    },
    {
      "epoch": 18.73939355127918,
      "grad_norm": 3.7494990825653076,
      "learning_rate": 3.151516121802056e-06,
      "loss": 0.4936,
      "step": 1170500
    },
    {
      "epoch": 18.74739841823829,
      "grad_norm": 5.7666120529174805,
      "learning_rate": 3.131503954404278e-06,
      "loss": 0.4879,
      "step": 1171000
    },
    {
      "epoch": 18.7554032851974,
      "grad_norm": 2.73576021194458,
      "learning_rate": 3.1114917870065e-06,
      "loss": 0.4835,
      "step": 1171500
    },
    {
      "epoch": 18.76340815215651,
      "grad_norm": 4.724822044372559,
      "learning_rate": 3.091479619608722e-06,
      "loss": 0.4812,
      "step": 1172000
    },
    {
      "epoch": 18.771413019115624,
      "grad_norm": 2.2301383018493652,
      "learning_rate": 3.0714674522109445e-06,
      "loss": 0.4942,
      "step": 1172500
    },
    {
      "epoch": 18.779417886074732,
      "grad_norm": 3.9166386127471924,
      "learning_rate": 3.0514552848131664e-06,
      "loss": 0.4823,
      "step": 1173000
    },
    {
      "epoch": 18.787422753033844,
      "grad_norm": 2.76149582862854,
      "learning_rate": 3.0314431174153887e-06,
      "loss": 0.4755,
      "step": 1173500
    },
    {
      "epoch": 18.795427619992957,
      "grad_norm": 4.671806812286377,
      "learning_rate": 3.011430950017611e-06,
      "loss": 0.4839,
      "step": 1174000
    },
    {
      "epoch": 18.803432486952065,
      "grad_norm": 3.1374385356903076,
      "learning_rate": 2.9914187826198333e-06,
      "loss": 0.4904,
      "step": 1174500
    },
    {
      "epoch": 18.811437353911177,
      "grad_norm": 2.857435703277588,
      "learning_rate": 2.971406615222055e-06,
      "loss": 0.494,
      "step": 1175000
    },
    {
      "epoch": 18.81944222087029,
      "grad_norm": 5.229888916015625,
      "learning_rate": 2.9513944478242775e-06,
      "loss": 0.4919,
      "step": 1175500
    },
    {
      "epoch": 18.827447087829402,
      "grad_norm": 3.9975478649139404,
      "learning_rate": 2.9313822804264994e-06,
      "loss": 0.479,
      "step": 1176000
    },
    {
      "epoch": 18.83545195478851,
      "grad_norm": 2.5351784229278564,
      "learning_rate": 2.9113701130287217e-06,
      "loss": 0.4771,
      "step": 1176500
    },
    {
      "epoch": 18.843456821747623,
      "grad_norm": 3.3689396381378174,
      "learning_rate": 2.8913579456309436e-06,
      "loss": 0.494,
      "step": 1177000
    },
    {
      "epoch": 18.851461688706735,
      "grad_norm": 5.443850994110107,
      "learning_rate": 2.871345778233166e-06,
      "loss": 0.4772,
      "step": 1177500
    },
    {
      "epoch": 18.859466555665843,
      "grad_norm": 4.372746467590332,
      "learning_rate": 2.851333610835388e-06,
      "loss": 0.4889,
      "step": 1178000
    },
    {
      "epoch": 18.867471422624956,
      "grad_norm": 2.8797667026519775,
      "learning_rate": 2.83132144343761e-06,
      "loss": 0.4793,
      "step": 1178500
    },
    {
      "epoch": 18.875476289584068,
      "grad_norm": 1.1321204900741577,
      "learning_rate": 2.8113092760398324e-06,
      "loss": 0.4971,
      "step": 1179000
    },
    {
      "epoch": 18.88348115654318,
      "grad_norm": 11.297869682312012,
      "learning_rate": 2.7912971086420543e-06,
      "loss": 0.4771,
      "step": 1179500
    },
    {
      "epoch": 18.89148602350229,
      "grad_norm": 5.251623630523682,
      "learning_rate": 2.7712849412442766e-06,
      "loss": 0.496,
      "step": 1180000
    },
    {
      "epoch": 18.8994908904614,
      "grad_norm": 2.8646771907806396,
      "learning_rate": 2.751272773846499e-06,
      "loss": 0.4963,
      "step": 1180500
    },
    {
      "epoch": 18.907495757420513,
      "grad_norm": 7.453289985656738,
      "learning_rate": 2.731260606448721e-06,
      "loss": 0.4865,
      "step": 1181000
    },
    {
      "epoch": 18.91550062437962,
      "grad_norm": 5.714101791381836,
      "learning_rate": 2.711248439050943e-06,
      "loss": 0.4945,
      "step": 1181500
    },
    {
      "epoch": 18.923505491338734,
      "grad_norm": 3.8876733779907227,
      "learning_rate": 2.6912362716531655e-06,
      "loss": 0.4745,
      "step": 1182000
    },
    {
      "epoch": 18.931510358297846,
      "grad_norm": 5.025344371795654,
      "learning_rate": 2.6712241042553873e-06,
      "loss": 0.4858,
      "step": 1182500
    },
    {
      "epoch": 18.939515225256955,
      "grad_norm": 4.334558010101318,
      "learning_rate": 2.6512119368576097e-06,
      "loss": 0.4654,
      "step": 1183000
    },
    {
      "epoch": 18.947520092216067,
      "grad_norm": 3.6741623878479004,
      "learning_rate": 2.631199769459832e-06,
      "loss": 0.4784,
      "step": 1183500
    },
    {
      "epoch": 18.95552495917518,
      "grad_norm": 2.9873552322387695,
      "learning_rate": 2.611187602062054e-06,
      "loss": 0.4783,
      "step": 1184000
    },
    {
      "epoch": 18.96352982613429,
      "grad_norm": 3.856658935546875,
      "learning_rate": 2.5911754346642757e-06,
      "loss": 0.5004,
      "step": 1184500
    },
    {
      "epoch": 18.9715346930934,
      "grad_norm": 3.367452383041382,
      "learning_rate": 2.571163267266498e-06,
      "loss": 0.4791,
      "step": 1185000
    },
    {
      "epoch": 18.979539560052512,
      "grad_norm": 3.4614577293395996,
      "learning_rate": 2.5511510998687204e-06,
      "loss": 0.4987,
      "step": 1185500
    },
    {
      "epoch": 18.987544427011624,
      "grad_norm": 4.297019958496094,
      "learning_rate": 2.5311389324709423e-06,
      "loss": 0.4888,
      "step": 1186000
    },
    {
      "epoch": 18.995549293970733,
      "grad_norm": 1.7226383686065674,
      "learning_rate": 2.5111267650731646e-06,
      "loss": 0.4834,
      "step": 1186500
    },
    {
      "epoch": 19.003554160929845,
      "grad_norm": 3.9230690002441406,
      "learning_rate": 2.491114597675387e-06,
      "loss": 0.4862,
      "step": 1187000
    },
    {
      "epoch": 19.011559027888957,
      "grad_norm": 6.847403526306152,
      "learning_rate": 2.4711024302776088e-06,
      "loss": 0.4729,
      "step": 1187500
    },
    {
      "epoch": 19.01956389484807,
      "grad_norm": 1.7803398370742798,
      "learning_rate": 2.451090262879831e-06,
      "loss": 0.4821,
      "step": 1188000
    },
    {
      "epoch": 19.027568761807178,
      "grad_norm": 11.894259452819824,
      "learning_rate": 2.4310780954820534e-06,
      "loss": 0.4607,
      "step": 1188500
    },
    {
      "epoch": 19.03557362876629,
      "grad_norm": 5.543946743011475,
      "learning_rate": 2.4110659280842753e-06,
      "loss": 0.4742,
      "step": 1189000
    },
    {
      "epoch": 19.043578495725402,
      "grad_norm": 4.000144004821777,
      "learning_rate": 2.3910537606864976e-06,
      "loss": 0.4707,
      "step": 1189500
    },
    {
      "epoch": 19.05158336268451,
      "grad_norm": 2.9567110538482666,
      "learning_rate": 2.37104159328872e-06,
      "loss": 0.4754,
      "step": 1190000
    },
    {
      "epoch": 19.059588229643623,
      "grad_norm": 5.241669178009033,
      "learning_rate": 2.351029425890942e-06,
      "loss": 0.4763,
      "step": 1190500
    },
    {
      "epoch": 19.067593096602735,
      "grad_norm": 2.2632083892822266,
      "learning_rate": 2.331017258493164e-06,
      "loss": 0.4794,
      "step": 1191000
    },
    {
      "epoch": 19.075597963561847,
      "grad_norm": 5.354747295379639,
      "learning_rate": 2.3110050910953864e-06,
      "loss": 0.4685,
      "step": 1191500
    },
    {
      "epoch": 19.083602830520956,
      "grad_norm": 4.766550540924072,
      "learning_rate": 2.2909929236976083e-06,
      "loss": 0.4907,
      "step": 1192000
    },
    {
      "epoch": 19.091607697480068,
      "grad_norm": 8.340404510498047,
      "learning_rate": 2.27098075629983e-06,
      "loss": 0.4609,
      "step": 1192500
    },
    {
      "epoch": 19.09961256443918,
      "grad_norm": 4.171548366546631,
      "learning_rate": 2.2509685889020525e-06,
      "loss": 0.4703,
      "step": 1193000
    },
    {
      "epoch": 19.10761743139829,
      "grad_norm": 1.267313838005066,
      "learning_rate": 2.2309564215042744e-06,
      "loss": 0.4523,
      "step": 1193500
    },
    {
      "epoch": 19.1156222983574,
      "grad_norm": 7.396315097808838,
      "learning_rate": 2.2109442541064967e-06,
      "loss": 0.4705,
      "step": 1194000
    },
    {
      "epoch": 19.123627165316513,
      "grad_norm": 4.6709489822387695,
      "learning_rate": 2.190932086708719e-06,
      "loss": 0.4694,
      "step": 1194500
    },
    {
      "epoch": 19.131632032275622,
      "grad_norm": 1.9561340808868408,
      "learning_rate": 2.170919919310941e-06,
      "loss": 0.47,
      "step": 1195000
    },
    {
      "epoch": 19.139636899234734,
      "grad_norm": 4.969695091247559,
      "learning_rate": 2.1509077519131632e-06,
      "loss": 0.4704,
      "step": 1195500
    },
    {
      "epoch": 19.147641766193846,
      "grad_norm": 3.8829216957092285,
      "learning_rate": 2.1308955845153855e-06,
      "loss": 0.4747,
      "step": 1196000
    },
    {
      "epoch": 19.15564663315296,
      "grad_norm": 4.563548564910889,
      "learning_rate": 2.110883417117608e-06,
      "loss": 0.4623,
      "step": 1196500
    },
    {
      "epoch": 19.163651500112067,
      "grad_norm": 3.1310224533081055,
      "learning_rate": 2.0908712497198297e-06,
      "loss": 0.4864,
      "step": 1197000
    },
    {
      "epoch": 19.17165636707118,
      "grad_norm": 5.359046459197998,
      "learning_rate": 2.070859082322052e-06,
      "loss": 0.4916,
      "step": 1197500
    },
    {
      "epoch": 19.17966123403029,
      "grad_norm": 1.9776203632354736,
      "learning_rate": 2.0508469149242744e-06,
      "loss": 0.4701,
      "step": 1198000
    },
    {
      "epoch": 19.1876661009894,
      "grad_norm": 2.6107635498046875,
      "learning_rate": 2.0308347475264962e-06,
      "loss": 0.4652,
      "step": 1198500
    },
    {
      "epoch": 19.195670967948512,
      "grad_norm": 1.5741345882415771,
      "learning_rate": 2.0108225801287186e-06,
      "loss": 0.475,
      "step": 1199000
    },
    {
      "epoch": 19.203675834907624,
      "grad_norm": 3.5712292194366455,
      "learning_rate": 1.9908104127309404e-06,
      "loss": 0.4799,
      "step": 1199500
    },
    {
      "epoch": 19.211680701866737,
      "grad_norm": 7.61855411529541,
      "learning_rate": 1.9707982453331628e-06,
      "loss": 0.4682,
      "step": 1200000
    },
    {
      "epoch": 19.219685568825845,
      "grad_norm": 3.240063190460205,
      "learning_rate": 1.9507860779353846e-06,
      "loss": 0.4815,
      "step": 1200500
    },
    {
      "epoch": 19.227690435784957,
      "grad_norm": 5.916876792907715,
      "learning_rate": 1.930773910537607e-06,
      "loss": 0.4677,
      "step": 1201000
    },
    {
      "epoch": 19.23569530274407,
      "grad_norm": 6.626944065093994,
      "learning_rate": 1.910761743139829e-06,
      "loss": 0.4654,
      "step": 1201500
    },
    {
      "epoch": 19.243700169703178,
      "grad_norm": 5.867184162139893,
      "learning_rate": 1.8907495757420512e-06,
      "loss": 0.4732,
      "step": 1202000
    },
    {
      "epoch": 19.25170503666229,
      "grad_norm": 9.10059928894043,
      "learning_rate": 1.8707374083442735e-06,
      "loss": 0.475,
      "step": 1202500
    },
    {
      "epoch": 19.259709903621403,
      "grad_norm": 4.25872802734375,
      "learning_rate": 1.8507252409464954e-06,
      "loss": 0.4823,
      "step": 1203000
    },
    {
      "epoch": 19.26771477058051,
      "grad_norm": 8.553465843200684,
      "learning_rate": 1.8307130735487177e-06,
      "loss": 0.4636,
      "step": 1203500
    },
    {
      "epoch": 19.275719637539623,
      "grad_norm": 4.435591697692871,
      "learning_rate": 1.81070090615094e-06,
      "loss": 0.462,
      "step": 1204000
    },
    {
      "epoch": 19.283724504498736,
      "grad_norm": 3.2857413291931152,
      "learning_rate": 1.7906887387531619e-06,
      "loss": 0.4694,
      "step": 1204500
    },
    {
      "epoch": 19.291729371457848,
      "grad_norm": 11.85155200958252,
      "learning_rate": 1.7706765713553842e-06,
      "loss": 0.474,
      "step": 1205000
    },
    {
      "epoch": 19.299734238416956,
      "grad_norm": 3.9176723957061768,
      "learning_rate": 1.7506644039576063e-06,
      "loss": 0.4609,
      "step": 1205500
    },
    {
      "epoch": 19.30773910537607,
      "grad_norm": 2.2518579959869385,
      "learning_rate": 1.7306522365598284e-06,
      "loss": 0.4781,
      "step": 1206000
    },
    {
      "epoch": 19.31574397233518,
      "grad_norm": 3.373955249786377,
      "learning_rate": 1.7106400691620505e-06,
      "loss": 0.4638,
      "step": 1206500
    },
    {
      "epoch": 19.32374883929429,
      "grad_norm": 3.6428933143615723,
      "learning_rate": 1.6906279017642728e-06,
      "loss": 0.4694,
      "step": 1207000
    },
    {
      "epoch": 19.3317537062534,
      "grad_norm": 7.531801223754883,
      "learning_rate": 1.6706157343664951e-06,
      "loss": 0.4837,
      "step": 1207500
    },
    {
      "epoch": 19.339758573212514,
      "grad_norm": 2.0792975425720215,
      "learning_rate": 1.650603566968717e-06,
      "loss": 0.466,
      "step": 1208000
    },
    {
      "epoch": 19.347763440171626,
      "grad_norm": 4.554461479187012,
      "learning_rate": 1.6305913995709393e-06,
      "loss": 0.4795,
      "step": 1208500
    },
    {
      "epoch": 19.355768307130734,
      "grad_norm": 4.595674514770508,
      "learning_rate": 1.6105792321731614e-06,
      "loss": 0.4808,
      "step": 1209000
    },
    {
      "epoch": 19.363773174089847,
      "grad_norm": 3.176635503768921,
      "learning_rate": 1.5905670647753835e-06,
      "loss": 0.467,
      "step": 1209500
    },
    {
      "epoch": 19.37177804104896,
      "grad_norm": 5.546535491943359,
      "learning_rate": 1.5705548973776056e-06,
      "loss": 0.4767,
      "step": 1210000
    },
    {
      "epoch": 19.379782908008067,
      "grad_norm": 4.338701248168945,
      "learning_rate": 1.5505427299798277e-06,
      "loss": 0.4721,
      "step": 1210500
    },
    {
      "epoch": 19.38778777496718,
      "grad_norm": 3.1989033222198486,
      "learning_rate": 1.53053056258205e-06,
      "loss": 0.4717,
      "step": 1211000
    },
    {
      "epoch": 19.395792641926292,
      "grad_norm": 2.3113627433776855,
      "learning_rate": 1.5105183951842721e-06,
      "loss": 0.4607,
      "step": 1211500
    },
    {
      "epoch": 19.403797508885404,
      "grad_norm": 7.8291144371032715,
      "learning_rate": 1.4905062277864942e-06,
      "loss": 0.4713,
      "step": 1212000
    },
    {
      "epoch": 19.411802375844513,
      "grad_norm": 8.812088012695312,
      "learning_rate": 1.4704940603887165e-06,
      "loss": 0.486,
      "step": 1212500
    },
    {
      "epoch": 19.419807242803625,
      "grad_norm": 4.986352443695068,
      "learning_rate": 1.4504818929909386e-06,
      "loss": 0.4781,
      "step": 1213000
    },
    {
      "epoch": 19.427812109762737,
      "grad_norm": 5.643225193023682,
      "learning_rate": 1.4304697255931607e-06,
      "loss": 0.4609,
      "step": 1213500
    },
    {
      "epoch": 19.435816976721846,
      "grad_norm": 2.9139091968536377,
      "learning_rate": 1.4104575581953828e-06,
      "loss": 0.4706,
      "step": 1214000
    },
    {
      "epoch": 19.443821843680958,
      "grad_norm": 9.33517074584961,
      "learning_rate": 1.390445390797605e-06,
      "loss": 0.4622,
      "step": 1214500
    },
    {
      "epoch": 19.45182671064007,
      "grad_norm": 1.8409978151321411,
      "learning_rate": 1.3704332233998273e-06,
      "loss": 0.4767,
      "step": 1215000
    },
    {
      "epoch": 19.45983157759918,
      "grad_norm": 3.575728178024292,
      "learning_rate": 1.3504210560020494e-06,
      "loss": 0.4687,
      "step": 1215500
    },
    {
      "epoch": 19.46783644455829,
      "grad_norm": 8.008544921875,
      "learning_rate": 1.3304088886042715e-06,
      "loss": 0.4903,
      "step": 1216000
    },
    {
      "epoch": 19.475841311517403,
      "grad_norm": 5.650244235992432,
      "learning_rate": 1.3103967212064938e-06,
      "loss": 0.4593,
      "step": 1216500
    },
    {
      "epoch": 19.483846178476515,
      "grad_norm": 6.7262864112854,
      "learning_rate": 1.2903845538087159e-06,
      "loss": 0.4646,
      "step": 1217000
    },
    {
      "epoch": 19.491851045435624,
      "grad_norm": 9.613391876220703,
      "learning_rate": 1.2703723864109378e-06,
      "loss": 0.4703,
      "step": 1217500
    },
    {
      "epoch": 19.499855912394736,
      "grad_norm": 9.15925121307373,
      "learning_rate": 1.25036021901316e-06,
      "loss": 0.4765,
      "step": 1218000
    },
    {
      "epoch": 19.507860779353848,
      "grad_norm": 3.0818803310394287,
      "learning_rate": 1.2303480516153822e-06,
      "loss": 0.4661,
      "step": 1218500
    },
    {
      "epoch": 19.515865646312957,
      "grad_norm": 2.2133309841156006,
      "learning_rate": 1.2103358842176043e-06,
      "loss": 0.4802,
      "step": 1219000
    },
    {
      "epoch": 19.52387051327207,
      "grad_norm": 6.974055767059326,
      "learning_rate": 1.1903237168198266e-06,
      "loss": 0.487,
      "step": 1219500
    },
    {
      "epoch": 19.53187538023118,
      "grad_norm": 5.741732597351074,
      "learning_rate": 1.1703115494220487e-06,
      "loss": 0.4765,
      "step": 1220000
    },
    {
      "epoch": 19.539880247190293,
      "grad_norm": 3.0526340007781982,
      "learning_rate": 1.150299382024271e-06,
      "loss": 0.4589,
      "step": 1220500
    },
    {
      "epoch": 19.547885114149402,
      "grad_norm": 5.375230312347412,
      "learning_rate": 1.1302872146264929e-06,
      "loss": 0.4649,
      "step": 1221000
    },
    {
      "epoch": 19.555889981108514,
      "grad_norm": 4.944153308868408,
      "learning_rate": 1.110275047228715e-06,
      "loss": 0.4678,
      "step": 1221500
    },
    {
      "epoch": 19.563894848067626,
      "grad_norm": 4.154967784881592,
      "learning_rate": 1.0902628798309373e-06,
      "loss": 0.4703,
      "step": 1222000
    },
    {
      "epoch": 19.571899715026735,
      "grad_norm": 10.162178039550781,
      "learning_rate": 1.0702507124331594e-06,
      "loss": 0.4756,
      "step": 1222500
    },
    {
      "epoch": 19.579904581985847,
      "grad_norm": 5.548275470733643,
      "learning_rate": 1.0502385450353815e-06,
      "loss": 0.4768,
      "step": 1223000
    },
    {
      "epoch": 19.58790944894496,
      "grad_norm": 7.683252811431885,
      "learning_rate": 1.0302263776376038e-06,
      "loss": 0.474,
      "step": 1223500
    },
    {
      "epoch": 19.595914315904068,
      "grad_norm": 3.2809386253356934,
      "learning_rate": 1.010214210239826e-06,
      "loss": 0.4676,
      "step": 1224000
    },
    {
      "epoch": 19.60391918286318,
      "grad_norm": 2.48064923286438,
      "learning_rate": 9.90202042842048e-07,
      "loss": 0.4715,
      "step": 1224500
    },
    {
      "epoch": 19.611924049822292,
      "grad_norm": 8.56914234161377,
      "learning_rate": 9.701898754442701e-07,
      "loss": 0.4594,
      "step": 1225000
    },
    {
      "epoch": 19.619928916781404,
      "grad_norm": 7.830671310424805,
      "learning_rate": 9.501777080464923e-07,
      "loss": 0.4744,
      "step": 1225500
    },
    {
      "epoch": 19.627933783740513,
      "grad_norm": 2.9655773639678955,
      "learning_rate": 9.301655406487145e-07,
      "loss": 0.4778,
      "step": 1226000
    },
    {
      "epoch": 19.635938650699625,
      "grad_norm": 1.77266263961792,
      "learning_rate": 9.101533732509366e-07,
      "loss": 0.4665,
      "step": 1226500
    },
    {
      "epoch": 19.643943517658737,
      "grad_norm": 9.208456039428711,
      "learning_rate": 8.901412058531587e-07,
      "loss": 0.4738,
      "step": 1227000
    },
    {
      "epoch": 19.651948384617846,
      "grad_norm": 6.881874084472656,
      "learning_rate": 8.701290384553809e-07,
      "loss": 0.4665,
      "step": 1227500
    },
    {
      "epoch": 19.659953251576958,
      "grad_norm": 8.651503562927246,
      "learning_rate": 8.50116871057603e-07,
      "loss": 0.458,
      "step": 1228000
    },
    {
      "epoch": 19.66795811853607,
      "grad_norm": 15.92725944519043,
      "learning_rate": 8.301047036598251e-07,
      "loss": 0.4746,
      "step": 1228500
    },
    {
      "epoch": 19.675962985495183,
      "grad_norm": 4.704753875732422,
      "learning_rate": 8.100925362620474e-07,
      "loss": 0.4564,
      "step": 1229000
    },
    {
      "epoch": 19.68396785245429,
      "grad_norm": 4.804250717163086,
      "learning_rate": 7.900803688642695e-07,
      "loss": 0.4638,
      "step": 1229500
    },
    {
      "epoch": 19.691972719413403,
      "grad_norm": 4.514575004577637,
      "learning_rate": 7.700682014664916e-07,
      "loss": 0.4792,
      "step": 1230000
    },
    {
      "epoch": 19.699977586372516,
      "grad_norm": 7.019821643829346,
      "learning_rate": 7.500560340687138e-07,
      "loss": 0.4817,
      "step": 1230500
    },
    {
      "epoch": 19.707982453331624,
      "grad_norm": 4.297420501708984,
      "learning_rate": 7.30043866670936e-07,
      "loss": 0.4718,
      "step": 1231000
    },
    {
      "epoch": 19.715987320290736,
      "grad_norm": 3.9335789680480957,
      "learning_rate": 7.100316992731582e-07,
      "loss": 0.4714,
      "step": 1231500
    },
    {
      "epoch": 19.72399218724985,
      "grad_norm": 5.067098140716553,
      "learning_rate": 6.900195318753803e-07,
      "loss": 0.4703,
      "step": 1232000
    },
    {
      "epoch": 19.73199705420896,
      "grad_norm": 15.070858001708984,
      "learning_rate": 6.700073644776025e-07,
      "loss": 0.4813,
      "step": 1232500
    },
    {
      "epoch": 19.74000192116807,
      "grad_norm": 5.0576677322387695,
      "learning_rate": 6.499951970798246e-07,
      "loss": 0.4702,
      "step": 1233000
    },
    {
      "epoch": 19.74800678812718,
      "grad_norm": 5.213879585266113,
      "learning_rate": 6.299830296820467e-07,
      "loss": 0.4622,
      "step": 1233500
    },
    {
      "epoch": 19.756011655086294,
      "grad_norm": 5.146517276763916,
      "learning_rate": 6.099708622842689e-07,
      "loss": 0.486,
      "step": 1234000
    },
    {
      "epoch": 19.764016522045402,
      "grad_norm": 3.1687660217285156,
      "learning_rate": 5.89958694886491e-07,
      "loss": 0.4587,
      "step": 1234500
    },
    {
      "epoch": 19.772021389004514,
      "grad_norm": 5.323264122009277,
      "learning_rate": 5.699465274887132e-07,
      "loss": 0.4642,
      "step": 1235000
    },
    {
      "epoch": 19.780026255963627,
      "grad_norm": 8.635204315185547,
      "learning_rate": 5.499343600909353e-07,
      "loss": 0.4779,
      "step": 1235500
    },
    {
      "epoch": 19.78803112292274,
      "grad_norm": 5.972875118255615,
      "learning_rate": 5.299221926931575e-07,
      "loss": 0.4764,
      "step": 1236000
    },
    {
      "epoch": 19.796035989881847,
      "grad_norm": 4.452615737915039,
      "learning_rate": 5.099100252953796e-07,
      "loss": 0.4622,
      "step": 1236500
    },
    {
      "epoch": 19.80404085684096,
      "grad_norm": 3.8037192821502686,
      "learning_rate": 4.898978578976018e-07,
      "loss": 0.4659,
      "step": 1237000
    },
    {
      "epoch": 19.812045723800072,
      "grad_norm": 2.260453939437866,
      "learning_rate": 4.6988569049982394e-07,
      "loss": 0.4782,
      "step": 1237500
    },
    {
      "epoch": 19.82005059075918,
      "grad_norm": 3.6703882217407227,
      "learning_rate": 4.498735231020461e-07,
      "loss": 0.4707,
      "step": 1238000
    },
    {
      "epoch": 19.828055457718293,
      "grad_norm": 5.231165409088135,
      "learning_rate": 4.298613557042682e-07,
      "loss": 0.4731,
      "step": 1238500
    },
    {
      "epoch": 19.836060324677405,
      "grad_norm": 2.8552517890930176,
      "learning_rate": 4.0984918830649035e-07,
      "loss": 0.4786,
      "step": 1239000
    },
    {
      "epoch": 19.844065191636513,
      "grad_norm": 2.7564685344696045,
      "learning_rate": 3.8983702090871255e-07,
      "loss": 0.4543,
      "step": 1239500
    },
    {
      "epoch": 19.852070058595626,
      "grad_norm": 5.434584140777588,
      "learning_rate": 3.6982485351093466e-07,
      "loss": 0.4791,
      "step": 1240000
    },
    {
      "epoch": 19.860074925554738,
      "grad_norm": 2.414767026901245,
      "learning_rate": 3.498126861131568e-07,
      "loss": 0.47,
      "step": 1240500
    },
    {
      "epoch": 19.86807979251385,
      "grad_norm": 3.398077964782715,
      "learning_rate": 3.2980051871537896e-07,
      "loss": 0.4777,
      "step": 1241000
    },
    {
      "epoch": 19.87608465947296,
      "grad_norm": 7.7403693199157715,
      "learning_rate": 3.097883513176011e-07,
      "loss": 0.4819,
      "step": 1241500
    },
    {
      "epoch": 19.88408952643207,
      "grad_norm": 2.690823554992676,
      "learning_rate": 2.8977618391982327e-07,
      "loss": 0.4781,
      "step": 1242000
    },
    {
      "epoch": 19.892094393391183,
      "grad_norm": 2.60908579826355,
      "learning_rate": 2.697640165220454e-07,
      "loss": 0.4663,
      "step": 1242500
    },
    {
      "epoch": 19.90009926035029,
      "grad_norm": 5.206094741821289,
      "learning_rate": 2.497518491242676e-07,
      "loss": 0.4856,
      "step": 1243000
    },
    {
      "epoch": 19.908104127309404,
      "grad_norm": 1.7642673254013062,
      "learning_rate": 2.297396817264897e-07,
      "loss": 0.4608,
      "step": 1243500
    },
    {
      "epoch": 19.916108994268516,
      "grad_norm": 5.812032699584961,
      "learning_rate": 2.0972751432871188e-07,
      "loss": 0.4843,
      "step": 1244000
    },
    {
      "epoch": 19.924113861227628,
      "grad_norm": 7.2426862716674805,
      "learning_rate": 1.89715346930934e-07,
      "loss": 0.4539,
      "step": 1244500
    },
    {
      "epoch": 19.932118728186737,
      "grad_norm": 2.8939008712768555,
      "learning_rate": 1.6970317953315616e-07,
      "loss": 0.478,
      "step": 1245000
    },
    {
      "epoch": 19.94012359514585,
      "grad_norm": 7.092377662658691,
      "learning_rate": 1.4969101213537832e-07,
      "loss": 0.4817,
      "step": 1245500
    },
    {
      "epoch": 19.94812846210496,
      "grad_norm": 4.475900173187256,
      "learning_rate": 1.2967884473760047e-07,
      "loss": 0.4705,
      "step": 1246000
    },
    {
      "epoch": 19.95613332906407,
      "grad_norm": 2.132084369659424,
      "learning_rate": 1.0966667733982262e-07,
      "loss": 0.4931,
      "step": 1246500
    },
    {
      "epoch": 19.964138196023182,
      "grad_norm": 3.673750877380371,
      "learning_rate": 8.965450994204478e-08,
      "loss": 0.4672,
      "step": 1247000
    },
    {
      "epoch": 19.972143062982294,
      "grad_norm": 1.4192376136779785,
      "learning_rate": 6.964234254426692e-08,
      "loss": 0.4868,
      "step": 1247500
    },
    {
      "epoch": 19.980147929941403,
      "grad_norm": 7.042381763458252,
      "learning_rate": 4.963017514648906e-08,
      "loss": 0.4707,
      "step": 1248000
    },
    {
      "epoch": 19.988152796900515,
      "grad_norm": 3.0032248497009277,
      "learning_rate": 2.961800774871122e-08,
      "loss": 0.4698,
      "step": 1248500
    },
    {
      "epoch": 19.996157663859627,
      "grad_norm": 8.07888412475586,
      "learning_rate": 9.605840350933367e-09,
      "loss": 0.4659,
      "step": 1249000
    },
    {
      "epoch": 20.004482797265492,
      "grad_norm": 3.4989097118377686,
      "learning_rate": 4.535266369851151e-06,
      "loss": 0.4648,
      "step": 1249500
    },
    {
      "epoch": 20.012487792382448,
      "grad_norm": 5.064871788024902,
      "learning_rate": 4.517073199130803e-06,
      "loss": 0.5032,
      "step": 1250000
    },
    {
      "epoch": 20.0204927874994,
      "grad_norm": 5.855804443359375,
      "learning_rate": 4.4988800284104554e-06,
      "loss": 0.4899,
      "step": 1250500
    },
    {
      "epoch": 20.028497782616352,
      "grad_norm": 4.269130706787109,
      "learning_rate": 4.480686857690108e-06,
      "loss": 0.4754,
      "step": 1251000
    },
    {
      "epoch": 20.036502777733304,
      "grad_norm": 4.392674922943115,
      "learning_rate": 4.462493686969761e-06,
      "loss": 0.4814,
      "step": 1251500
    },
    {
      "epoch": 20.04450777285026,
      "grad_norm": 3.8916049003601074,
      "learning_rate": 4.444300516249413e-06,
      "loss": 0.4707,
      "step": 1252000
    },
    {
      "epoch": 20.05251276796721,
      "grad_norm": 8.702713012695312,
      "learning_rate": 4.426107345529065e-06,
      "loss": 0.4748,
      "step": 1252500
    },
    {
      "epoch": 20.060517763084164,
      "grad_norm": 6.409435749053955,
      "learning_rate": 4.407914174808717e-06,
      "loss": 0.4769,
      "step": 1253000
    },
    {
      "epoch": 20.068522758201116,
      "grad_norm": 5.406142234802246,
      "learning_rate": 4.389721004088369e-06,
      "loss": 0.4852,
      "step": 1253500
    },
    {
      "epoch": 20.07652775331807,
      "grad_norm": 3.52368426322937,
      "learning_rate": 4.371527833368022e-06,
      "loss": 0.4869,
      "step": 1254000
    },
    {
      "epoch": 20.084532748435024,
      "grad_norm": 7.539942264556885,
      "learning_rate": 4.3533346626476745e-06,
      "loss": 0.4834,
      "step": 1254500
    },
    {
      "epoch": 20.092537743551976,
      "grad_norm": 6.7719855308532715,
      "learning_rate": 4.335141491927326e-06,
      "loss": 0.4962,
      "step": 1255000
    },
    {
      "epoch": 20.100542738668928,
      "grad_norm": 8.190918922424316,
      "learning_rate": 4.316948321206979e-06,
      "loss": 0.4755,
      "step": 1255500
    },
    {
      "epoch": 20.108547733785883,
      "grad_norm": 1.961722731590271,
      "learning_rate": 4.298755150486631e-06,
      "loss": 0.4829,
      "step": 1256000
    },
    {
      "epoch": 20.116552728902835,
      "grad_norm": 2.411813735961914,
      "learning_rate": 4.280561979766284e-06,
      "loss": 0.4733,
      "step": 1256500
    },
    {
      "epoch": 20.124557724019787,
      "grad_norm": 4.330793380737305,
      "learning_rate": 4.262368809045935e-06,
      "loss": 0.4745,
      "step": 1257000
    },
    {
      "epoch": 20.132562719136743,
      "grad_norm": 3.9420835971832275,
      "learning_rate": 4.2441756383255875e-06,
      "loss": 0.4745,
      "step": 1257500
    },
    {
      "epoch": 20.140567714253695,
      "grad_norm": 6.099603652954102,
      "learning_rate": 4.2259824676052405e-06,
      "loss": 0.4907,
      "step": 1258000
    },
    {
      "epoch": 20.148572709370647,
      "grad_norm": 6.749240875244141,
      "learning_rate": 4.207789296884893e-06,
      "loss": 0.4734,
      "step": 1258500
    },
    {
      "epoch": 20.1565777044876,
      "grad_norm": 3.2985260486602783,
      "learning_rate": 4.189596126164546e-06,
      "loss": 0.4922,
      "step": 1259000
    },
    {
      "epoch": 20.164582699604555,
      "grad_norm": 3.65561842918396,
      "learning_rate": 4.171402955444197e-06,
      "loss": 0.4899,
      "step": 1259500
    },
    {
      "epoch": 20.172587694721507,
      "grad_norm": 2.8930602073669434,
      "learning_rate": 4.153209784723849e-06,
      "loss": 0.4697,
      "step": 1260000
    },
    {
      "epoch": 20.18059268983846,
      "grad_norm": 4.406828880310059,
      "learning_rate": 4.135016614003502e-06,
      "loss": 0.4786,
      "step": 1260500
    },
    {
      "epoch": 20.18859768495541,
      "grad_norm": 2.463855504989624,
      "learning_rate": 4.116823443283154e-06,
      "loss": 0.4792,
      "step": 1261000
    },
    {
      "epoch": 20.196602680072367,
      "grad_norm": 4.602301120758057,
      "learning_rate": 4.0986302725628065e-06,
      "loss": 0.4645,
      "step": 1261500
    },
    {
      "epoch": 20.20460767518932,
      "grad_norm": 2.6447532176971436,
      "learning_rate": 4.080437101842459e-06,
      "loss": 0.4796,
      "step": 1262000
    },
    {
      "epoch": 20.21261267030627,
      "grad_norm": 1.8732367753982544,
      "learning_rate": 4.062243931122111e-06,
      "loss": 0.4813,
      "step": 1262500
    },
    {
      "epoch": 20.220617665423223,
      "grad_norm": 4.272820949554443,
      "learning_rate": 4.044050760401764e-06,
      "loss": 0.4807,
      "step": 1263000
    },
    {
      "epoch": 20.22862266054018,
      "grad_norm": 4.368076324462891,
      "learning_rate": 4.025857589681416e-06,
      "loss": 0.4814,
      "step": 1263500
    },
    {
      "epoch": 20.23662765565713,
      "grad_norm": 5.557406425476074,
      "learning_rate": 4.007664418961068e-06,
      "loss": 0.4726,
      "step": 1264000
    },
    {
      "epoch": 20.244632650774083,
      "grad_norm": 3.9282727241516113,
      "learning_rate": 3.98947124824072e-06,
      "loss": 0.4643,
      "step": 1264500
    },
    {
      "epoch": 20.252637645891035,
      "grad_norm": 4.686694622039795,
      "learning_rate": 3.9712780775203726e-06,
      "loss": 0.4815,
      "step": 1265000
    },
    {
      "epoch": 20.26064264100799,
      "grad_norm": 6.605167865753174,
      "learning_rate": 3.9530849068000256e-06,
      "loss": 0.4789,
      "step": 1265500
    },
    {
      "epoch": 20.268647636124943,
      "grad_norm": 3.883796215057373,
      "learning_rate": 3.934891736079678e-06,
      "loss": 0.4715,
      "step": 1266000
    },
    {
      "epoch": 20.276652631241895,
      "grad_norm": 6.86367130279541,
      "learning_rate": 3.91669856535933e-06,
      "loss": 0.4813,
      "step": 1266500
    },
    {
      "epoch": 20.284657626358847,
      "grad_norm": 2.8742406368255615,
      "learning_rate": 3.898505394638982e-06,
      "loss": 0.4712,
      "step": 1267000
    },
    {
      "epoch": 20.292662621475802,
      "grad_norm": 3.109863758087158,
      "learning_rate": 3.880312223918634e-06,
      "loss": 0.4807,
      "step": 1267500
    },
    {
      "epoch": 20.300667616592754,
      "grad_norm": 2.792987823486328,
      "learning_rate": 3.862119053198287e-06,
      "loss": 0.4676,
      "step": 1268000
    },
    {
      "epoch": 20.308672611709707,
      "grad_norm": 4.435531139373779,
      "learning_rate": 3.843925882477939e-06,
      "loss": 0.4809,
      "step": 1268500
    },
    {
      "epoch": 20.31667760682666,
      "grad_norm": 2.2499256134033203,
      "learning_rate": 3.825732711757592e-06,
      "loss": 0.4967,
      "step": 1269000
    },
    {
      "epoch": 20.324682601943614,
      "grad_norm": 10.732171058654785,
      "learning_rate": 3.8075395410372438e-06,
      "loss": 0.4632,
      "step": 1269500
    },
    {
      "epoch": 20.332687597060566,
      "grad_norm": 5.552268028259277,
      "learning_rate": 3.789346370316896e-06,
      "loss": 0.4746,
      "step": 1270000
    },
    {
      "epoch": 20.34069259217752,
      "grad_norm": 5.7691330909729,
      "learning_rate": 3.7711531995965485e-06,
      "loss": 0.4782,
      "step": 1270500
    },
    {
      "epoch": 20.34869758729447,
      "grad_norm": 4.165099620819092,
      "learning_rate": 3.7529600288762007e-06,
      "loss": 0.4783,
      "step": 1271000
    },
    {
      "epoch": 20.356702582411426,
      "grad_norm": 2.264695644378662,
      "learning_rate": 3.7347668581558533e-06,
      "loss": 0.4618,
      "step": 1271500
    },
    {
      "epoch": 20.364707577528378,
      "grad_norm": 9.512052536010742,
      "learning_rate": 3.7165736874355055e-06,
      "loss": 0.4816,
      "step": 1272000
    },
    {
      "epoch": 20.37271257264533,
      "grad_norm": 1.6941604614257812,
      "learning_rate": 3.698380516715157e-06,
      "loss": 0.4988,
      "step": 1272500
    },
    {
      "epoch": 20.380717567762282,
      "grad_norm": 6.213107585906982,
      "learning_rate": 3.68018734599481e-06,
      "loss": 0.4842,
      "step": 1273000
    },
    {
      "epoch": 20.388722562879238,
      "grad_norm": 5.993659496307373,
      "learning_rate": 3.661994175274462e-06,
      "loss": 0.4665,
      "step": 1273500
    },
    {
      "epoch": 20.39672755799619,
      "grad_norm": 2.709967613220215,
      "learning_rate": 3.643801004554115e-06,
      "loss": 0.4708,
      "step": 1274000
    },
    {
      "epoch": 20.404732553113142,
      "grad_norm": 5.341275691986084,
      "learning_rate": 3.6256078338337667e-06,
      "loss": 0.4716,
      "step": 1274500
    },
    {
      "epoch": 20.412737548230094,
      "grad_norm": 2.881283760070801,
      "learning_rate": 3.607414663113419e-06,
      "loss": 0.4807,
      "step": 1275000
    },
    {
      "epoch": 20.42074254334705,
      "grad_norm": 3.856577157974243,
      "learning_rate": 3.589221492393072e-06,
      "loss": 0.4807,
      "step": 1275500
    },
    {
      "epoch": 20.428747538464002,
      "grad_norm": 1.806747317314148,
      "learning_rate": 3.5710283216727236e-06,
      "loss": 0.4867,
      "step": 1276000
    },
    {
      "epoch": 20.436752533580954,
      "grad_norm": 4.266717433929443,
      "learning_rate": 3.5528351509523767e-06,
      "loss": 0.4685,
      "step": 1276500
    },
    {
      "epoch": 20.444757528697906,
      "grad_norm": 5.8850789070129395,
      "learning_rate": 3.5346419802320284e-06,
      "loss": 0.4682,
      "step": 1277000
    },
    {
      "epoch": 20.45276252381486,
      "grad_norm": 5.6740570068359375,
      "learning_rate": 3.5164488095116806e-06,
      "loss": 0.4817,
      "step": 1277500
    },
    {
      "epoch": 20.460767518931814,
      "grad_norm": 3.238737106323242,
      "learning_rate": 3.498255638791333e-06,
      "loss": 0.4772,
      "step": 1278000
    },
    {
      "epoch": 20.468772514048766,
      "grad_norm": 7.093743324279785,
      "learning_rate": 3.4800624680709853e-06,
      "loss": 0.4581,
      "step": 1278500
    },
    {
      "epoch": 20.476777509165718,
      "grad_norm": 5.657014846801758,
      "learning_rate": 3.461869297350638e-06,
      "loss": 0.4682,
      "step": 1279000
    },
    {
      "epoch": 20.484782504282673,
      "grad_norm": 6.9098992347717285,
      "learning_rate": 3.44367612663029e-06,
      "loss": 0.4744,
      "step": 1279500
    },
    {
      "epoch": 20.492787499399626,
      "grad_norm": 2.399848699569702,
      "learning_rate": 3.4254829559099423e-06,
      "loss": 0.4784,
      "step": 1280000
    },
    {
      "epoch": 20.500792494516578,
      "grad_norm": 6.079656600952148,
      "learning_rate": 3.407289785189595e-06,
      "loss": 0.479,
      "step": 1280500
    },
    {
      "epoch": 20.50879748963353,
      "grad_norm": 5.021154880523682,
      "learning_rate": 3.389096614469247e-06,
      "loss": 0.4899,
      "step": 1281000
    },
    {
      "epoch": 20.516802484750485,
      "grad_norm": 2.8989408016204834,
      "learning_rate": 3.3709034437488996e-06,
      "loss": 0.4614,
      "step": 1281500
    },
    {
      "epoch": 20.524807479867437,
      "grad_norm": 4.351700782775879,
      "learning_rate": 3.3527102730285518e-06,
      "loss": 0.4783,
      "step": 1282000
    },
    {
      "epoch": 20.53281247498439,
      "grad_norm": 11.386039733886719,
      "learning_rate": 3.334517102308204e-06,
      "loss": 0.4759,
      "step": 1282500
    },
    {
      "epoch": 20.540817470101345,
      "grad_norm": 8.509855270385742,
      "learning_rate": 3.3163239315878565e-06,
      "loss": 0.4892,
      "step": 1283000
    },
    {
      "epoch": 20.548822465218297,
      "grad_norm": 14.53320026397705,
      "learning_rate": 3.2981307608675087e-06,
      "loss": 0.4715,
      "step": 1283500
    },
    {
      "epoch": 20.55682746033525,
      "grad_norm": 6.739839553833008,
      "learning_rate": 3.2799375901471613e-06,
      "loss": 0.462,
      "step": 1284000
    },
    {
      "epoch": 20.5648324554522,
      "grad_norm": 4.160340309143066,
      "learning_rate": 3.2617444194268135e-06,
      "loss": 0.4719,
      "step": 1284500
    },
    {
      "epoch": 20.572837450569153,
      "grad_norm": 4.868349075317383,
      "learning_rate": 3.2435512487064652e-06,
      "loss": 0.4848,
      "step": 1285000
    },
    {
      "epoch": 20.58084244568611,
      "grad_norm": 3.8671247959136963,
      "learning_rate": 3.2253580779861182e-06,
      "loss": 0.4921,
      "step": 1285500
    },
    {
      "epoch": 20.58884744080306,
      "grad_norm": 2.2025949954986572,
      "learning_rate": 3.20716490726577e-06,
      "loss": 0.4759,
      "step": 1286000
    },
    {
      "epoch": 20.596852435920013,
      "grad_norm": 6.980596542358398,
      "learning_rate": 3.188971736545423e-06,
      "loss": 0.481,
      "step": 1286500
    },
    {
      "epoch": 20.60485743103697,
      "grad_norm": 3.7097182273864746,
      "learning_rate": 3.1707785658250747e-06,
      "loss": 0.4865,
      "step": 1287000
    },
    {
      "epoch": 20.61286242615392,
      "grad_norm": 3.475961685180664,
      "learning_rate": 3.152585395104727e-06,
      "loss": 0.4725,
      "step": 1287500
    },
    {
      "epoch": 20.620867421270873,
      "grad_norm": 3.9779319763183594,
      "learning_rate": 3.13439222438438e-06,
      "loss": 0.4992,
      "step": 1288000
    },
    {
      "epoch": 20.628872416387825,
      "grad_norm": 6.915774345397949,
      "learning_rate": 3.116199053664032e-06,
      "loss": 0.4655,
      "step": 1288500
    },
    {
      "epoch": 20.63687741150478,
      "grad_norm": 3.9235174655914307,
      "learning_rate": 3.0980058829436843e-06,
      "loss": 0.4851,
      "step": 1289000
    },
    {
      "epoch": 20.644882406621733,
      "grad_norm": 7.515087604522705,
      "learning_rate": 3.0798127122233364e-06,
      "loss": 0.4799,
      "step": 1289500
    },
    {
      "epoch": 20.652887401738685,
      "grad_norm": 3.862823963165283,
      "learning_rate": 3.061619541502989e-06,
      "loss": 0.4699,
      "step": 1290000
    },
    {
      "epoch": 20.660892396855637,
      "grad_norm": 2.6657795906066895,
      "learning_rate": 3.043426370782641e-06,
      "loss": 0.4729,
      "step": 1290500
    },
    {
      "epoch": 20.668897391972592,
      "grad_norm": 4.298985958099365,
      "learning_rate": 3.0252332000622938e-06,
      "loss": 0.4792,
      "step": 1291000
    },
    {
      "epoch": 20.676902387089545,
      "grad_norm": 2.7946105003356934,
      "learning_rate": 3.007040029341946e-06,
      "loss": 0.4891,
      "step": 1291500
    },
    {
      "epoch": 20.684907382206497,
      "grad_norm": 3.468841075897217,
      "learning_rate": 2.988846858621598e-06,
      "loss": 0.487,
      "step": 1292000
    },
    {
      "epoch": 20.69291237732345,
      "grad_norm": 5.516444206237793,
      "learning_rate": 2.9706536879012507e-06,
      "loss": 0.4826,
      "step": 1292500
    },
    {
      "epoch": 20.700917372440404,
      "grad_norm": 5.125144958496094,
      "learning_rate": 2.952460517180903e-06,
      "loss": 0.4786,
      "step": 1293000
    },
    {
      "epoch": 20.708922367557356,
      "grad_norm": 3.103795051574707,
      "learning_rate": 2.9342673464605555e-06,
      "loss": 0.4741,
      "step": 1293500
    },
    {
      "epoch": 20.71692736267431,
      "grad_norm": 2.6768155097961426,
      "learning_rate": 2.916074175740207e-06,
      "loss": 0.4858,
      "step": 1294000
    },
    {
      "epoch": 20.72493235779126,
      "grad_norm": 4.0404181480407715,
      "learning_rate": 2.89788100501986e-06,
      "loss": 0.4956,
      "step": 1294500
    },
    {
      "epoch": 20.732937352908216,
      "grad_norm": 3.9979939460754395,
      "learning_rate": 2.879687834299512e-06,
      "loss": 0.4682,
      "step": 1295000
    },
    {
      "epoch": 20.74094234802517,
      "grad_norm": 4.637016296386719,
      "learning_rate": 2.8614946635791646e-06,
      "loss": 0.4968,
      "step": 1295500
    },
    {
      "epoch": 20.74894734314212,
      "grad_norm": 1.5136313438415527,
      "learning_rate": 2.8433014928588167e-06,
      "loss": 0.4812,
      "step": 1296000
    },
    {
      "epoch": 20.756952338259072,
      "grad_norm": 5.364002227783203,
      "learning_rate": 2.825108322138469e-06,
      "loss": 0.4637,
      "step": 1296500
    },
    {
      "epoch": 20.764957333376028,
      "grad_norm": 2.9667088985443115,
      "learning_rate": 2.8069151514181215e-06,
      "loss": 0.4651,
      "step": 1297000
    },
    {
      "epoch": 20.77296232849298,
      "grad_norm": 3.726512908935547,
      "learning_rate": 2.7887219806977737e-06,
      "loss": 0.4999,
      "step": 1297500
    },
    {
      "epoch": 20.780967323609932,
      "grad_norm": 3.854017734527588,
      "learning_rate": 2.7705288099774262e-06,
      "loss": 0.4733,
      "step": 1298000
    },
    {
      "epoch": 20.788972318726884,
      "grad_norm": 4.622582912445068,
      "learning_rate": 2.7523356392570784e-06,
      "loss": 0.4732,
      "step": 1298500
    },
    {
      "epoch": 20.79697731384384,
      "grad_norm": 3.0209293365478516,
      "learning_rate": 2.7341424685367306e-06,
      "loss": 0.4595,
      "step": 1299000
    },
    {
      "epoch": 20.804982308960792,
      "grad_norm": 2.3082494735717773,
      "learning_rate": 2.7159492978163828e-06,
      "loss": 0.4901,
      "step": 1299500
    },
    {
      "epoch": 20.812987304077744,
      "grad_norm": 5.161916732788086,
      "learning_rate": 2.6977561270960353e-06,
      "loss": 0.486,
      "step": 1300000
    },
    {
      "epoch": 20.820992299194696,
      "grad_norm": 4.469276428222656,
      "learning_rate": 2.6795629563756875e-06,
      "loss": 0.4756,
      "step": 1300500
    },
    {
      "epoch": 20.82899729431165,
      "grad_norm": 6.332775115966797,
      "learning_rate": 2.66136978565534e-06,
      "loss": 0.478,
      "step": 1301000
    },
    {
      "epoch": 20.837002289428604,
      "grad_norm": 1.5998080968856812,
      "learning_rate": 2.6431766149349923e-06,
      "loss": 0.4806,
      "step": 1301500
    },
    {
      "epoch": 20.845007284545556,
      "grad_norm": 8.055403709411621,
      "learning_rate": 2.6249834442146444e-06,
      "loss": 0.4648,
      "step": 1302000
    },
    {
      "epoch": 20.853012279662508,
      "grad_norm": 3.069746255874634,
      "learning_rate": 2.606790273494297e-06,
      "loss": 0.4702,
      "step": 1302500
    },
    {
      "epoch": 20.861017274779464,
      "grad_norm": 3.634695291519165,
      "learning_rate": 2.588597102773949e-06,
      "loss": 0.4778,
      "step": 1303000
    },
    {
      "epoch": 20.869022269896416,
      "grad_norm": 5.441553115844727,
      "learning_rate": 2.570403932053602e-06,
      "loss": 0.4779,
      "step": 1303500
    },
    {
      "epoch": 20.877027265013368,
      "grad_norm": 2.6658997535705566,
      "learning_rate": 2.552210761333254e-06,
      "loss": 0.4623,
      "step": 1304000
    },
    {
      "epoch": 20.88503226013032,
      "grad_norm": 4.25723123550415,
      "learning_rate": 2.534017590612906e-06,
      "loss": 0.4753,
      "step": 1304500
    },
    {
      "epoch": 20.893037255247275,
      "grad_norm": 7.00356388092041,
      "learning_rate": 2.5158244198925587e-06,
      "loss": 0.4783,
      "step": 1305000
    },
    {
      "epoch": 20.901042250364227,
      "grad_norm": 2.875972032546997,
      "learning_rate": 2.497631249172211e-06,
      "loss": 0.4763,
      "step": 1305500
    },
    {
      "epoch": 20.90904724548118,
      "grad_norm": 3.8693013191223145,
      "learning_rate": 2.4794380784518635e-06,
      "loss": 0.4672,
      "step": 1306000
    },
    {
      "epoch": 20.91705224059813,
      "grad_norm": 4.118326187133789,
      "learning_rate": 2.4612449077315152e-06,
      "loss": 0.4671,
      "step": 1306500
    },
    {
      "epoch": 20.925057235715087,
      "grad_norm": 9.062288284301758,
      "learning_rate": 2.443051737011168e-06,
      "loss": 0.4808,
      "step": 1307000
    },
    {
      "epoch": 20.93306223083204,
      "grad_norm": 3.9118199348449707,
      "learning_rate": 2.42485856629082e-06,
      "loss": 0.4578,
      "step": 1307500
    },
    {
      "epoch": 20.94106722594899,
      "grad_norm": 1.8556244373321533,
      "learning_rate": 2.4066653955704726e-06,
      "loss": 0.4654,
      "step": 1308000
    },
    {
      "epoch": 20.949072221065943,
      "grad_norm": 2.445765733718872,
      "learning_rate": 2.3884722248501247e-06,
      "loss": 0.4872,
      "step": 1308500
    },
    {
      "epoch": 20.9570772161829,
      "grad_norm": 3.7631750106811523,
      "learning_rate": 2.370279054129777e-06,
      "loss": 0.4713,
      "step": 1309000
    },
    {
      "epoch": 20.96508221129985,
      "grad_norm": 3.799706220626831,
      "learning_rate": 2.3520858834094295e-06,
      "loss": 0.4768,
      "step": 1309500
    },
    {
      "epoch": 20.973087206416803,
      "grad_norm": 2.4190313816070557,
      "learning_rate": 2.3338927126890817e-06,
      "loss": 0.4847,
      "step": 1310000
    },
    {
      "epoch": 20.981092201533755,
      "grad_norm": 2.8222594261169434,
      "learning_rate": 2.3156995419687343e-06,
      "loss": 0.4543,
      "step": 1310500
    },
    {
      "epoch": 20.98909719665071,
      "grad_norm": 4.239200115203857,
      "learning_rate": 2.2975063712483864e-06,
      "loss": 0.4585,
      "step": 1311000
    },
    {
      "epoch": 20.997102191767663,
      "grad_norm": 3.6682355403900146,
      "learning_rate": 2.2793132005280386e-06,
      "loss": 0.4766,
      "step": 1311500
    },
    {
      "epoch": 21.005107186884615,
      "grad_norm": 8.684515953063965,
      "learning_rate": 2.2611200298076908e-06,
      "loss": 0.4614,
      "step": 1312000
    },
    {
      "epoch": 21.01311218200157,
      "grad_norm": 3.8256852626800537,
      "learning_rate": 2.2429268590873434e-06,
      "loss": 0.4718,
      "step": 1312500
    },
    {
      "epoch": 21.021117177118523,
      "grad_norm": 5.600568771362305,
      "learning_rate": 2.2247336883669955e-06,
      "loss": 0.4556,
      "step": 1313000
    },
    {
      "epoch": 21.029122172235475,
      "grad_norm": 2.7909505367279053,
      "learning_rate": 2.206540517646648e-06,
      "loss": 0.4643,
      "step": 1313500
    },
    {
      "epoch": 21.037127167352427,
      "grad_norm": 2.278294324874878,
      "learning_rate": 2.1883473469263003e-06,
      "loss": 0.4641,
      "step": 1314000
    },
    {
      "epoch": 21.045132162469383,
      "grad_norm": 2.6417417526245117,
      "learning_rate": 2.1701541762059525e-06,
      "loss": 0.4826,
      "step": 1314500
    },
    {
      "epoch": 21.053137157586335,
      "grad_norm": 3.266355276107788,
      "learning_rate": 2.151961005485605e-06,
      "loss": 0.4699,
      "step": 1315000
    },
    {
      "epoch": 21.061142152703287,
      "grad_norm": 4.552976131439209,
      "learning_rate": 2.1337678347652572e-06,
      "loss": 0.463,
      "step": 1315500
    },
    {
      "epoch": 21.06914714782024,
      "grad_norm": 8.188288688659668,
      "learning_rate": 2.11557466404491e-06,
      "loss": 0.4508,
      "step": 1316000
    },
    {
      "epoch": 21.077152142937194,
      "grad_norm": 5.026156902313232,
      "learning_rate": 2.0973814933245616e-06,
      "loss": 0.45,
      "step": 1316500
    },
    {
      "epoch": 21.085157138054146,
      "grad_norm": 2.1323907375335693,
      "learning_rate": 2.079188322604214e-06,
      "loss": 0.4564,
      "step": 1317000
    },
    {
      "epoch": 21.0931621331711,
      "grad_norm": 6.538811206817627,
      "learning_rate": 2.0609951518838667e-06,
      "loss": 0.4457,
      "step": 1317500
    },
    {
      "epoch": 21.10116712828805,
      "grad_norm": 5.207082271575928,
      "learning_rate": 2.042801981163519e-06,
      "loss": 0.4814,
      "step": 1318000
    },
    {
      "epoch": 21.109172123405006,
      "grad_norm": 7.2337470054626465,
      "learning_rate": 2.0246088104431715e-06,
      "loss": 0.4633,
      "step": 1318500
    },
    {
      "epoch": 21.11717711852196,
      "grad_norm": 9.943317413330078,
      "learning_rate": 2.0064156397228232e-06,
      "loss": 0.4619,
      "step": 1319000
    },
    {
      "epoch": 21.12518211363891,
      "grad_norm": 5.015784740447998,
      "learning_rate": 1.988222469002476e-06,
      "loss": 0.4555,
      "step": 1319500
    },
    {
      "epoch": 21.133187108755862,
      "grad_norm": 4.102297782897949,
      "learning_rate": 1.970029298282128e-06,
      "loss": 0.4661,
      "step": 1320000
    },
    {
      "epoch": 21.141192103872818,
      "grad_norm": 4.142017364501953,
      "learning_rate": 1.9518361275617806e-06,
      "loss": 0.4575,
      "step": 1320500
    },
    {
      "epoch": 21.14919709898977,
      "grad_norm": 5.591012954711914,
      "learning_rate": 1.9336429568414328e-06,
      "loss": 0.4707,
      "step": 1321000
    },
    {
      "epoch": 21.157202094106722,
      "grad_norm": 3.927985429763794,
      "learning_rate": 1.915449786121085e-06,
      "loss": 0.4699,
      "step": 1321500
    },
    {
      "epoch": 21.165207089223674,
      "grad_norm": 5.5796332359313965,
      "learning_rate": 1.8972566154007373e-06,
      "loss": 0.4567,
      "step": 1322000
    },
    {
      "epoch": 21.17321208434063,
      "grad_norm": 5.853757858276367,
      "learning_rate": 1.8790634446803897e-06,
      "loss": 0.4616,
      "step": 1322500
    },
    {
      "epoch": 21.181217079457582,
      "grad_norm": 4.4363884925842285,
      "learning_rate": 1.860870273960042e-06,
      "loss": 0.4634,
      "step": 1323000
    },
    {
      "epoch": 21.189222074574534,
      "grad_norm": 6.21535062789917,
      "learning_rate": 1.8426771032396944e-06,
      "loss": 0.4704,
      "step": 1323500
    },
    {
      "epoch": 21.197227069691486,
      "grad_norm": 4.02802038192749,
      "learning_rate": 1.8244839325193466e-06,
      "loss": 0.4686,
      "step": 1324000
    },
    {
      "epoch": 21.205232064808442,
      "grad_norm": 1.8681831359863281,
      "learning_rate": 1.806290761798999e-06,
      "loss": 0.4585,
      "step": 1324500
    },
    {
      "epoch": 21.213237059925394,
      "grad_norm": 7.244392395019531,
      "learning_rate": 1.7880975910786514e-06,
      "loss": 0.4667,
      "step": 1325000
    },
    {
      "epoch": 21.221242055042346,
      "grad_norm": 4.465539455413818,
      "learning_rate": 1.7699044203583038e-06,
      "loss": 0.4647,
      "step": 1325500
    },
    {
      "epoch": 21.229247050159298,
      "grad_norm": 3.967190980911255,
      "learning_rate": 1.7517112496379561e-06,
      "loss": 0.4592,
      "step": 1326000
    },
    {
      "epoch": 21.237252045276254,
      "grad_norm": 5.3954620361328125,
      "learning_rate": 1.733518078917608e-06,
      "loss": 0.465,
      "step": 1326500
    },
    {
      "epoch": 21.245257040393206,
      "grad_norm": 2.6012489795684814,
      "learning_rate": 1.7153249081972605e-06,
      "loss": 0.4689,
      "step": 1327000
    },
    {
      "epoch": 21.253262035510158,
      "grad_norm": 5.301387310028076,
      "learning_rate": 1.6971317374769129e-06,
      "loss": 0.4594,
      "step": 1327500
    },
    {
      "epoch": 21.26126703062711,
      "grad_norm": 3.3789665699005127,
      "learning_rate": 1.6789385667565652e-06,
      "loss": 0.4764,
      "step": 1328000
    },
    {
      "epoch": 21.269272025744066,
      "grad_norm": 2.993412733078003,
      "learning_rate": 1.6607453960362176e-06,
      "loss": 0.467,
      "step": 1328500
    },
    {
      "epoch": 21.277277020861018,
      "grad_norm": 3.5439486503601074,
      "learning_rate": 1.6425522253158698e-06,
      "loss": 0.4669,
      "step": 1329000
    },
    {
      "epoch": 21.28528201597797,
      "grad_norm": 14.126001358032227,
      "learning_rate": 1.6243590545955222e-06,
      "loss": 0.4531,
      "step": 1329500
    },
    {
      "epoch": 21.29328701109492,
      "grad_norm": 3.4157333374023438,
      "learning_rate": 1.6061658838751745e-06,
      "loss": 0.4577,
      "step": 1330000
    },
    {
      "epoch": 21.301292006211877,
      "grad_norm": 12.440792083740234,
      "learning_rate": 1.587972713154827e-06,
      "loss": 0.4813,
      "step": 1330500
    },
    {
      "epoch": 21.30929700132883,
      "grad_norm": 2.5936601161956787,
      "learning_rate": 1.5697795424344793e-06,
      "loss": 0.4533,
      "step": 1331000
    },
    {
      "epoch": 21.31730199644578,
      "grad_norm": 3.6646158695220947,
      "learning_rate": 1.5515863717141315e-06,
      "loss": 0.4757,
      "step": 1331500
    },
    {
      "epoch": 21.325306991562734,
      "grad_norm": 3.5541720390319824,
      "learning_rate": 1.5333932009937838e-06,
      "loss": 0.4444,
      "step": 1332000
    },
    {
      "epoch": 21.33331198667969,
      "grad_norm": 6.8809967041015625,
      "learning_rate": 1.5152000302734362e-06,
      "loss": 0.4459,
      "step": 1332500
    },
    {
      "epoch": 21.34131698179664,
      "grad_norm": 6.942759990692139,
      "learning_rate": 1.4970068595530886e-06,
      "loss": 0.4531,
      "step": 1333000
    },
    {
      "epoch": 21.349321976913593,
      "grad_norm": 2.3623225688934326,
      "learning_rate": 1.4788136888327408e-06,
      "loss": 0.4634,
      "step": 1333500
    },
    {
      "epoch": 21.357326972030545,
      "grad_norm": 5.810369491577148,
      "learning_rate": 1.4606205181123932e-06,
      "loss": 0.4823,
      "step": 1334000
    },
    {
      "epoch": 21.3653319671475,
      "grad_norm": 5.516749858856201,
      "learning_rate": 1.4424273473920453e-06,
      "loss": 0.459,
      "step": 1334500
    },
    {
      "epoch": 21.373336962264453,
      "grad_norm": 2.4392647743225098,
      "learning_rate": 1.4242341766716977e-06,
      "loss": 0.466,
      "step": 1335000
    },
    {
      "epoch": 21.381341957381405,
      "grad_norm": 4.112002849578857,
      "learning_rate": 1.40604100595135e-06,
      "loss": 0.4748,
      "step": 1335500
    },
    {
      "epoch": 21.389346952498357,
      "grad_norm": 3.411766529083252,
      "learning_rate": 1.3878478352310023e-06,
      "loss": 0.4675,
      "step": 1336000
    },
    {
      "epoch": 21.397351947615313,
      "grad_norm": 5.332498550415039,
      "learning_rate": 1.3696546645106546e-06,
      "loss": 0.4749,
      "step": 1336500
    },
    {
      "epoch": 21.405356942732265,
      "grad_norm": 2.1729061603546143,
      "learning_rate": 1.351461493790307e-06,
      "loss": 0.4708,
      "step": 1337000
    },
    {
      "epoch": 21.413361937849217,
      "grad_norm": 9.10794734954834,
      "learning_rate": 1.3332683230699594e-06,
      "loss": 0.4697,
      "step": 1337500
    },
    {
      "epoch": 21.421366932966173,
      "grad_norm": 4.9160075187683105,
      "learning_rate": 1.3150751523496118e-06,
      "loss": 0.4719,
      "step": 1338000
    },
    {
      "epoch": 21.429371928083125,
      "grad_norm": 5.598076820373535,
      "learning_rate": 1.296881981629264e-06,
      "loss": 0.4693,
      "step": 1338500
    },
    {
      "epoch": 21.437376923200077,
      "grad_norm": 2.7443320751190186,
      "learning_rate": 1.2786888109089163e-06,
      "loss": 0.4854,
      "step": 1339000
    },
    {
      "epoch": 21.44538191831703,
      "grad_norm": 1.810994029045105,
      "learning_rate": 1.2604956401885685e-06,
      "loss": 0.4633,
      "step": 1339500
    },
    {
      "epoch": 21.453386913433985,
      "grad_norm": 4.229599952697754,
      "learning_rate": 1.2423024694682209e-06,
      "loss": 0.4849,
      "step": 1340000
    },
    {
      "epoch": 21.461391908550937,
      "grad_norm": 4.110955238342285,
      "learning_rate": 1.2241092987478733e-06,
      "loss": 0.4617,
      "step": 1340500
    },
    {
      "epoch": 21.46939690366789,
      "grad_norm": 2.838547945022583,
      "learning_rate": 1.2059161280275256e-06,
      "loss": 0.4622,
      "step": 1341000
    },
    {
      "epoch": 21.47740189878484,
      "grad_norm": 3.2641828060150146,
      "learning_rate": 1.187722957307178e-06,
      "loss": 0.4631,
      "step": 1341500
    },
    {
      "epoch": 21.485406893901796,
      "grad_norm": 5.2352705001831055,
      "learning_rate": 1.1695297865868302e-06,
      "loss": 0.4577,
      "step": 1342000
    },
    {
      "epoch": 21.49341188901875,
      "grad_norm": 4.319412708282471,
      "learning_rate": 1.1513366158664826e-06,
      "loss": 0.4671,
      "step": 1342500
    },
    {
      "epoch": 21.5014168841357,
      "grad_norm": 3.420797824859619,
      "learning_rate": 1.133143445146135e-06,
      "loss": 0.4551,
      "step": 1343000
    },
    {
      "epoch": 21.509421879252653,
      "grad_norm": 5.705995559692383,
      "learning_rate": 1.1149502744257871e-06,
      "loss": 0.4611,
      "step": 1343500
    },
    {
      "epoch": 21.517426874369608,
      "grad_norm": 2.5727882385253906,
      "learning_rate": 1.0967571037054395e-06,
      "loss": 0.4731,
      "step": 1344000
    },
    {
      "epoch": 21.52543186948656,
      "grad_norm": 4.886480331420898,
      "learning_rate": 1.0785639329850917e-06,
      "loss": 0.4593,
      "step": 1344500
    },
    {
      "epoch": 21.533436864603512,
      "grad_norm": 6.082984447479248,
      "learning_rate": 1.0603707622647442e-06,
      "loss": 0.463,
      "step": 1345000
    },
    {
      "epoch": 21.541441859720464,
      "grad_norm": 3.2213034629821777,
      "learning_rate": 1.0421775915443966e-06,
      "loss": 0.4646,
      "step": 1345500
    },
    {
      "epoch": 21.54944685483742,
      "grad_norm": 2.3127734661102295,
      "learning_rate": 1.0239844208240488e-06,
      "loss": 0.4512,
      "step": 1346000
    },
    {
      "epoch": 21.557451849954372,
      "grad_norm": 5.654014587402344,
      "learning_rate": 1.0057912501037012e-06,
      "loss": 0.4662,
      "step": 1346500
    },
    {
      "epoch": 21.565456845071324,
      "grad_norm": 4.630213737487793,
      "learning_rate": 9.875980793833533e-07,
      "loss": 0.4648,
      "step": 1347000
    },
    {
      "epoch": 21.573461840188276,
      "grad_norm": 9.362059593200684,
      "learning_rate": 9.694049086630057e-07,
      "loss": 0.4679,
      "step": 1347500
    },
    {
      "epoch": 21.581466835305232,
      "grad_norm": 12.297755241394043,
      "learning_rate": 9.512117379426581e-07,
      "loss": 0.4545,
      "step": 1348000
    },
    {
      "epoch": 21.589471830422184,
      "grad_norm": 3.9447240829467773,
      "learning_rate": 9.330185672223104e-07,
      "loss": 0.4658,
      "step": 1348500
    },
    {
      "epoch": 21.597476825539136,
      "grad_norm": 5.9929375648498535,
      "learning_rate": 9.148253965019628e-07,
      "loss": 0.4465,
      "step": 1349000
    },
    {
      "epoch": 21.605481820656088,
      "grad_norm": 11.868329048156738,
      "learning_rate": 8.966322257816149e-07,
      "loss": 0.4579,
      "step": 1349500
    },
    {
      "epoch": 21.613486815773044,
      "grad_norm": 3.767779588699341,
      "learning_rate": 8.784390550612673e-07,
      "loss": 0.4625,
      "step": 1350000
    },
    {
      "epoch": 21.621491810889996,
      "grad_norm": 4.842666149139404,
      "learning_rate": 8.602458843409198e-07,
      "loss": 0.4571,
      "step": 1350500
    },
    {
      "epoch": 21.629496806006948,
      "grad_norm": 3.451061248779297,
      "learning_rate": 8.42052713620572e-07,
      "loss": 0.4701,
      "step": 1351000
    },
    {
      "epoch": 21.6375018011239,
      "grad_norm": 5.90523099899292,
      "learning_rate": 8.238595429002243e-07,
      "loss": 0.459,
      "step": 1351500
    },
    {
      "epoch": 21.645506796240856,
      "grad_norm": 6.485456943511963,
      "learning_rate": 8.056663721798765e-07,
      "loss": 0.4783,
      "step": 1352000
    },
    {
      "epoch": 21.653511791357808,
      "grad_norm": 5.236525535583496,
      "learning_rate": 7.87473201459529e-07,
      "loss": 0.4539,
      "step": 1352500
    },
    {
      "epoch": 21.66151678647476,
      "grad_norm": 3.7441556453704834,
      "learning_rate": 7.692800307391813e-07,
      "loss": 0.451,
      "step": 1353000
    },
    {
      "epoch": 21.669521781591712,
      "grad_norm": 3.7769031524658203,
      "learning_rate": 7.510868600188336e-07,
      "loss": 0.4619,
      "step": 1353500
    },
    {
      "epoch": 21.677526776708667,
      "grad_norm": 2.237231969833374,
      "learning_rate": 7.328936892984859e-07,
      "loss": 0.4466,
      "step": 1354000
    },
    {
      "epoch": 21.68553177182562,
      "grad_norm": 4.05128288269043,
      "learning_rate": 7.147005185781382e-07,
      "loss": 0.4699,
      "step": 1354500
    },
    {
      "epoch": 21.69353676694257,
      "grad_norm": 5.517542362213135,
      "learning_rate": 6.965073478577906e-07,
      "loss": 0.4604,
      "step": 1355000
    },
    {
      "epoch": 21.701541762059524,
      "grad_norm": 5.685943603515625,
      "learning_rate": 6.783141771374429e-07,
      "loss": 0.446,
      "step": 1355500
    },
    {
      "epoch": 21.70954675717648,
      "grad_norm": 5.941958427429199,
      "learning_rate": 6.601210064170952e-07,
      "loss": 0.4758,
      "step": 1356000
    },
    {
      "epoch": 21.71755175229343,
      "grad_norm": 3.327260732650757,
      "learning_rate": 6.419278356967475e-07,
      "loss": 0.4573,
      "step": 1356500
    },
    {
      "epoch": 21.725556747410383,
      "grad_norm": 3.671175956726074,
      "learning_rate": 6.237346649763999e-07,
      "loss": 0.4935,
      "step": 1357000
    },
    {
      "epoch": 21.733561742527336,
      "grad_norm": 6.440615177154541,
      "learning_rate": 6.055414942560522e-07,
      "loss": 0.4509,
      "step": 1357500
    },
    {
      "epoch": 21.74156673764429,
      "grad_norm": 5.3499345779418945,
      "learning_rate": 5.873483235357044e-07,
      "loss": 0.4625,
      "step": 1358000
    },
    {
      "epoch": 21.749571732761243,
      "grad_norm": 4.151652812957764,
      "learning_rate": 5.691551528153568e-07,
      "loss": 0.4654,
      "step": 1358500
    },
    {
      "epoch": 21.757576727878195,
      "grad_norm": 2.130013942718506,
      "learning_rate": 5.509619820950092e-07,
      "loss": 0.4713,
      "step": 1359000
    },
    {
      "epoch": 21.765581722995147,
      "grad_norm": 3.792884349822998,
      "learning_rate": 5.327688113746615e-07,
      "loss": 0.4822,
      "step": 1359500
    },
    {
      "epoch": 21.773586718112103,
      "grad_norm": 6.6150689125061035,
      "learning_rate": 5.145756406543137e-07,
      "loss": 0.4644,
      "step": 1360000
    },
    {
      "epoch": 21.781591713229055,
      "grad_norm": 3.33894419670105,
      "learning_rate": 4.96382469933966e-07,
      "loss": 0.4659,
      "step": 1360500
    },
    {
      "epoch": 21.789596708346007,
      "grad_norm": 4.575024127960205,
      "learning_rate": 4.781892992136184e-07,
      "loss": 0.4749,
      "step": 1361000
    },
    {
      "epoch": 21.79760170346296,
      "grad_norm": 5.294015407562256,
      "learning_rate": 4.599961284932708e-07,
      "loss": 0.4553,
      "step": 1361500
    },
    {
      "epoch": 21.805606698579915,
      "grad_norm": 9.032262802124023,
      "learning_rate": 4.4180295777292305e-07,
      "loss": 0.4747,
      "step": 1362000
    },
    {
      "epoch": 21.813611693696867,
      "grad_norm": 3.69316029548645,
      "learning_rate": 4.236097870525754e-07,
      "loss": 0.4502,
      "step": 1362500
    },
    {
      "epoch": 21.82161668881382,
      "grad_norm": 4.501170635223389,
      "learning_rate": 4.0541661633222765e-07,
      "loss": 0.4586,
      "step": 1363000
    },
    {
      "epoch": 21.829621683930775,
      "grad_norm": 5.57890510559082,
      "learning_rate": 3.8722344561188003e-07,
      "loss": 0.4646,
      "step": 1363500
    },
    {
      "epoch": 21.837626679047727,
      "grad_norm": 7.087954521179199,
      "learning_rate": 3.6903027489153236e-07,
      "loss": 0.4582,
      "step": 1364000
    },
    {
      "epoch": 21.84563167416468,
      "grad_norm": 3.7299726009368896,
      "learning_rate": 3.5083710417118463e-07,
      "loss": 0.4721,
      "step": 1364500
    },
    {
      "epoch": 21.85363666928163,
      "grad_norm": 9.383407592773438,
      "learning_rate": 3.3264393345083696e-07,
      "loss": 0.4648,
      "step": 1365000
    },
    {
      "epoch": 21.861641664398583,
      "grad_norm": 2.943709373474121,
      "learning_rate": 3.144507627304893e-07,
      "loss": 0.472,
      "step": 1365500
    },
    {
      "epoch": 21.86964665951554,
      "grad_norm": 2.953535318374634,
      "learning_rate": 2.962575920101416e-07,
      "loss": 0.4727,
      "step": 1366000
    },
    {
      "epoch": 21.87765165463249,
      "grad_norm": 8.915910720825195,
      "learning_rate": 2.7806442128979394e-07,
      "loss": 0.4708,
      "step": 1366500
    },
    {
      "epoch": 21.885656649749443,
      "grad_norm": 5.582467555999756,
      "learning_rate": 2.5987125056944627e-07,
      "loss": 0.4683,
      "step": 1367000
    },
    {
      "epoch": 21.8936616448664,
      "grad_norm": 7.589584827423096,
      "learning_rate": 2.4167807984909854e-07,
      "loss": 0.4526,
      "step": 1367500
    },
    {
      "epoch": 21.90166663998335,
      "grad_norm": 3.3254032135009766,
      "learning_rate": 2.234849091287509e-07,
      "loss": 0.4891,
      "step": 1368000
    },
    {
      "epoch": 21.909671635100302,
      "grad_norm": 4.145793437957764,
      "learning_rate": 2.052917384084032e-07,
      "loss": 0.4619,
      "step": 1368500
    },
    {
      "epoch": 21.917676630217255,
      "grad_norm": 7.307012557983398,
      "learning_rate": 1.8709856768805552e-07,
      "loss": 0.4657,
      "step": 1369000
    },
    {
      "epoch": 21.92568162533421,
      "grad_norm": 3.2890357971191406,
      "learning_rate": 1.6890539696770785e-07,
      "loss": 0.4559,
      "step": 1369500
    },
    {
      "epoch": 21.933686620451162,
      "grad_norm": 1.2905830144882202,
      "learning_rate": 1.5071222624736018e-07,
      "loss": 0.4661,
      "step": 1370000
    },
    {
      "epoch": 21.941691615568114,
      "grad_norm": 8.236288070678711,
      "learning_rate": 1.325190555270125e-07,
      "loss": 0.4665,
      "step": 1370500
    },
    {
      "epoch": 21.949696610685066,
      "grad_norm": 3.7642664909362793,
      "learning_rate": 1.1432588480666483e-07,
      "loss": 0.4682,
      "step": 1371000
    },
    {
      "epoch": 21.957701605802022,
      "grad_norm": 5.031809329986572,
      "learning_rate": 9.613271408631714e-08,
      "loss": 0.4712,
      "step": 1371500
    },
    {
      "epoch": 21.965706600918974,
      "grad_norm": 4.517037391662598,
      "learning_rate": 7.793954336596946e-08,
      "loss": 0.4479,
      "step": 1372000
    },
    {
      "epoch": 21.973711596035926,
      "grad_norm": 2.9844255447387695,
      "learning_rate": 5.974637264562178e-08,
      "loss": 0.4762,
      "step": 1372500
    },
    {
      "epoch": 21.98171659115288,
      "grad_norm": 5.044000625610352,
      "learning_rate": 4.15532019252741e-08,
      "loss": 0.4612,
      "step": 1373000
    },
    {
      "epoch": 21.989721586269834,
      "grad_norm": 3.060396432876587,
      "learning_rate": 2.3360031204926422e-08,
      "loss": 0.4484,
      "step": 1373500
    },
    {
      "epoch": 21.997726581386786,
      "grad_norm": 5.667359352111816,
      "learning_rate": 5.166860484578741e-09,
      "loss": 0.4688,
      "step": 1374000
    }
  ],
  "logging_steps": 500,
  "max_steps": 1374142,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 22,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4462282122580198e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
