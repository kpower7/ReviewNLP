{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 187305,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00800832866180828,
      "grad_norm": 4.341034412384033,
      "learning_rate": 4.986652785563653e-05,
      "loss": 0.7995,
      "step": 500
    },
    {
      "epoch": 0.01601665732361656,
      "grad_norm": 4.903304576873779,
      "learning_rate": 4.973305571127306e-05,
      "loss": 0.6779,
      "step": 1000
    },
    {
      "epoch": 0.02402498598542484,
      "grad_norm": 7.063520908355713,
      "learning_rate": 4.959958356690959e-05,
      "loss": 0.6516,
      "step": 1500
    },
    {
      "epoch": 0.03203331464723312,
      "grad_norm": 3.4154152870178223,
      "learning_rate": 4.946611142254612e-05,
      "loss": 0.6306,
      "step": 2000
    },
    {
      "epoch": 0.040041643309041405,
      "grad_norm": 4.487967014312744,
      "learning_rate": 4.9332639278182645e-05,
      "loss": 0.6084,
      "step": 2500
    },
    {
      "epoch": 0.04804997197084968,
      "grad_norm": 5.1551971435546875,
      "learning_rate": 4.9199167133819176e-05,
      "loss": 0.6201,
      "step": 3000
    },
    {
      "epoch": 0.056058300632657966,
      "grad_norm": 6.992980003356934,
      "learning_rate": 4.90656949894557e-05,
      "loss": 0.6138,
      "step": 3500
    },
    {
      "epoch": 0.06406662929446624,
      "grad_norm": 14.44216537475586,
      "learning_rate": 4.893222284509223e-05,
      "loss": 0.5986,
      "step": 4000
    },
    {
      "epoch": 0.07207495795627453,
      "grad_norm": 5.647764205932617,
      "learning_rate": 4.879875070072876e-05,
      "loss": 0.6008,
      "step": 4500
    },
    {
      "epoch": 0.08008328661808281,
      "grad_norm": 6.318582534790039,
      "learning_rate": 4.8665278556365293e-05,
      "loss": 0.5978,
      "step": 5000
    },
    {
      "epoch": 0.08809161527989108,
      "grad_norm": 3.0823676586151123,
      "learning_rate": 4.853180641200182e-05,
      "loss": 0.6041,
      "step": 5500
    },
    {
      "epoch": 0.09609994394169936,
      "grad_norm": 3.658535957336426,
      "learning_rate": 4.839833426763835e-05,
      "loss": 0.5961,
      "step": 6000
    },
    {
      "epoch": 0.10410827260350765,
      "grad_norm": 9.705808639526367,
      "learning_rate": 4.826486212327487e-05,
      "loss": 0.575,
      "step": 6500
    },
    {
      "epoch": 0.11211660126531593,
      "grad_norm": 4.798050880432129,
      "learning_rate": 4.81313899789114e-05,
      "loss": 0.5939,
      "step": 7000
    },
    {
      "epoch": 0.12012492992712422,
      "grad_norm": 3.745378255844116,
      "learning_rate": 4.7997917834547936e-05,
      "loss": 0.5876,
      "step": 7500
    },
    {
      "epoch": 0.12813325858893249,
      "grad_norm": 6.620859146118164,
      "learning_rate": 4.786444569018446e-05,
      "loss": 0.5834,
      "step": 8000
    },
    {
      "epoch": 0.13614158725074077,
      "grad_norm": 6.508619785308838,
      "learning_rate": 4.773097354582099e-05,
      "loss": 0.5811,
      "step": 8500
    },
    {
      "epoch": 0.14414991591254905,
      "grad_norm": 3.5845539569854736,
      "learning_rate": 4.7597501401457515e-05,
      "loss": 0.586,
      "step": 9000
    },
    {
      "epoch": 0.15215824457435734,
      "grad_norm": 3.8543505668640137,
      "learning_rate": 4.7464029257094047e-05,
      "loss": 0.5764,
      "step": 9500
    },
    {
      "epoch": 0.16016657323616562,
      "grad_norm": 6.021883964538574,
      "learning_rate": 4.733055711273058e-05,
      "loss": 0.5818,
      "step": 10000
    },
    {
      "epoch": 0.1681749018979739,
      "grad_norm": 5.576465129852295,
      "learning_rate": 4.719708496836711e-05,
      "loss": 0.5723,
      "step": 10500
    },
    {
      "epoch": 0.17618323055978216,
      "grad_norm": 24.022804260253906,
      "learning_rate": 4.706361282400363e-05,
      "loss": 0.5747,
      "step": 11000
    },
    {
      "epoch": 0.18419155922159045,
      "grad_norm": 8.683987617492676,
      "learning_rate": 4.6930140679640164e-05,
      "loss": 0.5779,
      "step": 11500
    },
    {
      "epoch": 0.19219988788339873,
      "grad_norm": 9.578516006469727,
      "learning_rate": 4.679666853527669e-05,
      "loss": 0.5902,
      "step": 12000
    },
    {
      "epoch": 0.200208216545207,
      "grad_norm": 6.387156963348389,
      "learning_rate": 4.666319639091321e-05,
      "loss": 0.5706,
      "step": 12500
    },
    {
      "epoch": 0.2082165452070153,
      "grad_norm": 7.460468292236328,
      "learning_rate": 4.652972424654975e-05,
      "loss": 0.5753,
      "step": 13000
    },
    {
      "epoch": 0.21622487386882358,
      "grad_norm": 2.5201504230499268,
      "learning_rate": 4.6396252102186275e-05,
      "loss": 0.5617,
      "step": 13500
    },
    {
      "epoch": 0.22423320253063186,
      "grad_norm": 10.917168617248535,
      "learning_rate": 4.6262779957822806e-05,
      "loss": 0.5789,
      "step": 14000
    },
    {
      "epoch": 0.23224153119244015,
      "grad_norm": 4.143006801605225,
      "learning_rate": 4.612930781345933e-05,
      "loss": 0.5796,
      "step": 14500
    },
    {
      "epoch": 0.24024985985424843,
      "grad_norm": 5.7884907722473145,
      "learning_rate": 4.599583566909586e-05,
      "loss": 0.5753,
      "step": 15000
    },
    {
      "epoch": 0.2482581885160567,
      "grad_norm": 3.828791856765747,
      "learning_rate": 4.5862363524732386e-05,
      "loss": 0.5697,
      "step": 15500
    },
    {
      "epoch": 0.25626651717786497,
      "grad_norm": 4.151005268096924,
      "learning_rate": 4.5728891380368924e-05,
      "loss": 0.5676,
      "step": 16000
    },
    {
      "epoch": 0.26427484583967326,
      "grad_norm": 2.498488426208496,
      "learning_rate": 4.559541923600545e-05,
      "loss": 0.584,
      "step": 16500
    },
    {
      "epoch": 0.27228317450148154,
      "grad_norm": 7.905001163482666,
      "learning_rate": 4.546194709164198e-05,
      "loss": 0.5704,
      "step": 17000
    },
    {
      "epoch": 0.2802915031632898,
      "grad_norm": 5.520227909088135,
      "learning_rate": 4.5328474947278504e-05,
      "loss": 0.5705,
      "step": 17500
    },
    {
      "epoch": 0.2882998318250981,
      "grad_norm": 3.0246224403381348,
      "learning_rate": 4.5195002802915035e-05,
      "loss": 0.5685,
      "step": 18000
    },
    {
      "epoch": 0.2963081604869064,
      "grad_norm": 3.9577836990356445,
      "learning_rate": 4.5061530658551566e-05,
      "loss": 0.5672,
      "step": 18500
    },
    {
      "epoch": 0.3043164891487147,
      "grad_norm": 3.936753988265991,
      "learning_rate": 4.492805851418809e-05,
      "loss": 0.5723,
      "step": 19000
    },
    {
      "epoch": 0.31232481781052296,
      "grad_norm": 9.493486404418945,
      "learning_rate": 4.479458636982462e-05,
      "loss": 0.5821,
      "step": 19500
    },
    {
      "epoch": 0.32033314647233124,
      "grad_norm": 4.156578063964844,
      "learning_rate": 4.4661114225461146e-05,
      "loss": 0.5514,
      "step": 20000
    },
    {
      "epoch": 0.3283414751341395,
      "grad_norm": 3.5540716648101807,
      "learning_rate": 4.452764208109768e-05,
      "loss": 0.5495,
      "step": 20500
    },
    {
      "epoch": 0.3363498037959478,
      "grad_norm": 3.091482162475586,
      "learning_rate": 4.43941699367342e-05,
      "loss": 0.5689,
      "step": 21000
    },
    {
      "epoch": 0.34435813245775604,
      "grad_norm": 2.9373323917388916,
      "learning_rate": 4.426069779237074e-05,
      "loss": 0.5602,
      "step": 21500
    },
    {
      "epoch": 0.3523664611195643,
      "grad_norm": 2.800938129425049,
      "learning_rate": 4.4127225648007264e-05,
      "loss": 0.5615,
      "step": 22000
    },
    {
      "epoch": 0.3603747897813726,
      "grad_norm": 5.757203102111816,
      "learning_rate": 4.3993753503643795e-05,
      "loss": 0.5678,
      "step": 22500
    },
    {
      "epoch": 0.3683831184431809,
      "grad_norm": 3.7301228046417236,
      "learning_rate": 4.386028135928032e-05,
      "loss": 0.5777,
      "step": 23000
    },
    {
      "epoch": 0.3763914471049892,
      "grad_norm": 32.20902633666992,
      "learning_rate": 4.372680921491685e-05,
      "loss": 0.576,
      "step": 23500
    },
    {
      "epoch": 0.38439977576679746,
      "grad_norm": 4.901029586791992,
      "learning_rate": 4.3593337070553375e-05,
      "loss": 0.588,
      "step": 24000
    },
    {
      "epoch": 0.39240810442860574,
      "grad_norm": 4.396781921386719,
      "learning_rate": 4.3459864926189906e-05,
      "loss": 0.5851,
      "step": 24500
    },
    {
      "epoch": 0.400416433090414,
      "grad_norm": 4.765368461608887,
      "learning_rate": 4.332639278182644e-05,
      "loss": 0.5777,
      "step": 25000
    },
    {
      "epoch": 0.4084247617522223,
      "grad_norm": 10.55105972290039,
      "learning_rate": 4.319292063746296e-05,
      "loss": 0.5738,
      "step": 25500
    },
    {
      "epoch": 0.4164330904140306,
      "grad_norm": 5.845773220062256,
      "learning_rate": 4.305944849309949e-05,
      "loss": 0.5807,
      "step": 26000
    },
    {
      "epoch": 0.4244414190758389,
      "grad_norm": 7.819371700286865,
      "learning_rate": 4.292597634873602e-05,
      "loss": 0.5591,
      "step": 26500
    },
    {
      "epoch": 0.43244974773764716,
      "grad_norm": 2.8302836418151855,
      "learning_rate": 4.2792504204372555e-05,
      "loss": 0.5691,
      "step": 27000
    },
    {
      "epoch": 0.44045807639945544,
      "grad_norm": 10.305069923400879,
      "learning_rate": 4.265903206000908e-05,
      "loss": 0.552,
      "step": 27500
    },
    {
      "epoch": 0.44846640506126373,
      "grad_norm": 6.313445091247559,
      "learning_rate": 4.252555991564561e-05,
      "loss": 0.5567,
      "step": 28000
    },
    {
      "epoch": 0.456474733723072,
      "grad_norm": 4.808748245239258,
      "learning_rate": 4.2392087771282135e-05,
      "loss": 0.5686,
      "step": 28500
    },
    {
      "epoch": 0.4644830623848803,
      "grad_norm": 5.541644096374512,
      "learning_rate": 4.2258615626918666e-05,
      "loss": 0.562,
      "step": 29000
    },
    {
      "epoch": 0.4724913910466886,
      "grad_norm": 3.016921281814575,
      "learning_rate": 4.212514348255519e-05,
      "loss": 0.5752,
      "step": 29500
    },
    {
      "epoch": 0.48049971970849686,
      "grad_norm": 1.342978596687317,
      "learning_rate": 4.199167133819172e-05,
      "loss": 0.5676,
      "step": 30000
    },
    {
      "epoch": 0.4885080483703051,
      "grad_norm": 2.043856620788574,
      "learning_rate": 4.185819919382825e-05,
      "loss": 0.5774,
      "step": 30500
    },
    {
      "epoch": 0.4965163770321134,
      "grad_norm": 5.0765299797058105,
      "learning_rate": 4.172472704946478e-05,
      "loss": 0.5716,
      "step": 31000
    },
    {
      "epoch": 0.5045247056939217,
      "grad_norm": 5.079748630523682,
      "learning_rate": 4.159125490510131e-05,
      "loss": 0.5598,
      "step": 31500
    },
    {
      "epoch": 0.5125330343557299,
      "grad_norm": 3.0093252658843994,
      "learning_rate": 4.145778276073783e-05,
      "loss": 0.5828,
      "step": 32000
    },
    {
      "epoch": 0.5205413630175383,
      "grad_norm": 3.962170362472534,
      "learning_rate": 4.1324310616374364e-05,
      "loss": 0.5496,
      "step": 32500
    },
    {
      "epoch": 0.5285496916793465,
      "grad_norm": 3.651639223098755,
      "learning_rate": 4.1190838472010895e-05,
      "loss": 0.5586,
      "step": 33000
    },
    {
      "epoch": 0.5365580203411549,
      "grad_norm": 4.153874397277832,
      "learning_rate": 4.1057366327647426e-05,
      "loss": 0.5602,
      "step": 33500
    },
    {
      "epoch": 0.5445663490029631,
      "grad_norm": 4.265336990356445,
      "learning_rate": 4.092389418328395e-05,
      "loss": 0.5628,
      "step": 34000
    },
    {
      "epoch": 0.5525746776647713,
      "grad_norm": 7.398928642272949,
      "learning_rate": 4.079042203892048e-05,
      "loss": 0.5575,
      "step": 34500
    },
    {
      "epoch": 0.5605830063265796,
      "grad_norm": 2.9150454998016357,
      "learning_rate": 4.0656949894557006e-05,
      "loss": 0.5655,
      "step": 35000
    },
    {
      "epoch": 0.5685913349883879,
      "grad_norm": 10.627225875854492,
      "learning_rate": 4.052347775019354e-05,
      "loss": 0.5556,
      "step": 35500
    },
    {
      "epoch": 0.5765996636501962,
      "grad_norm": 5.627581596374512,
      "learning_rate": 4.039000560583007e-05,
      "loss": 0.5658,
      "step": 36000
    },
    {
      "epoch": 0.5846079923120044,
      "grad_norm": 4.279469013214111,
      "learning_rate": 4.025653346146659e-05,
      "loss": 0.5489,
      "step": 36500
    },
    {
      "epoch": 0.5926163209738128,
      "grad_norm": 2.1071250438690186,
      "learning_rate": 4.0123061317103124e-05,
      "loss": 0.5657,
      "step": 37000
    },
    {
      "epoch": 0.600624649635621,
      "grad_norm": 4.208123207092285,
      "learning_rate": 3.998958917273965e-05,
      "loss": 0.5634,
      "step": 37500
    },
    {
      "epoch": 0.6086329782974294,
      "grad_norm": 6.990226745605469,
      "learning_rate": 3.985611702837618e-05,
      "loss": 0.5506,
      "step": 38000
    },
    {
      "epoch": 0.6166413069592376,
      "grad_norm": 3.9392850399017334,
      "learning_rate": 3.972264488401271e-05,
      "loss": 0.5653,
      "step": 38500
    },
    {
      "epoch": 0.6246496356210459,
      "grad_norm": 4.577337265014648,
      "learning_rate": 3.958917273964924e-05,
      "loss": 0.5529,
      "step": 39000
    },
    {
      "epoch": 0.6326579642828541,
      "grad_norm": 4.566893577575684,
      "learning_rate": 3.9455700595285766e-05,
      "loss": 0.5548,
      "step": 39500
    },
    {
      "epoch": 0.6406662929446625,
      "grad_norm": 5.411877632141113,
      "learning_rate": 3.93222284509223e-05,
      "loss": 0.5537,
      "step": 40000
    },
    {
      "epoch": 0.6486746216064707,
      "grad_norm": 4.104384422302246,
      "learning_rate": 3.918875630655882e-05,
      "loss": 0.5651,
      "step": 40500
    },
    {
      "epoch": 0.656682950268279,
      "grad_norm": 2.938718557357788,
      "learning_rate": 3.905528416219535e-05,
      "loss": 0.566,
      "step": 41000
    },
    {
      "epoch": 0.6646912789300873,
      "grad_norm": 3.550554037094116,
      "learning_rate": 3.8921812017831883e-05,
      "loss": 0.5546,
      "step": 41500
    },
    {
      "epoch": 0.6726996075918956,
      "grad_norm": 1.508626103401184,
      "learning_rate": 3.878833987346841e-05,
      "loss": 0.5534,
      "step": 42000
    },
    {
      "epoch": 0.6807079362537038,
      "grad_norm": 2.913686752319336,
      "learning_rate": 3.865486772910494e-05,
      "loss": 0.5476,
      "step": 42500
    },
    {
      "epoch": 0.6887162649155121,
      "grad_norm": 3.9451050758361816,
      "learning_rate": 3.852139558474146e-05,
      "loss": 0.5494,
      "step": 43000
    },
    {
      "epoch": 0.6967245935773204,
      "grad_norm": 3.3587934970855713,
      "learning_rate": 3.8387923440377994e-05,
      "loss": 0.5487,
      "step": 43500
    },
    {
      "epoch": 0.7047329222391286,
      "grad_norm": 1.7476997375488281,
      "learning_rate": 3.8254451296014526e-05,
      "loss": 0.537,
      "step": 44000
    },
    {
      "epoch": 0.712741250900937,
      "grad_norm": 4.126948833465576,
      "learning_rate": 3.812097915165106e-05,
      "loss": 0.5448,
      "step": 44500
    },
    {
      "epoch": 0.7207495795627452,
      "grad_norm": 4.941864967346191,
      "learning_rate": 3.798750700728758e-05,
      "loss": 0.567,
      "step": 45000
    },
    {
      "epoch": 0.7287579082245536,
      "grad_norm": 3.1921803951263428,
      "learning_rate": 3.785403486292411e-05,
      "loss": 0.5621,
      "step": 45500
    },
    {
      "epoch": 0.7367662368863618,
      "grad_norm": 3.7173779010772705,
      "learning_rate": 3.7720562718560637e-05,
      "loss": 0.5479,
      "step": 46000
    },
    {
      "epoch": 0.7447745655481701,
      "grad_norm": 2.963423252105713,
      "learning_rate": 3.758709057419717e-05,
      "loss": 0.5611,
      "step": 46500
    },
    {
      "epoch": 0.7527828942099783,
      "grad_norm": 5.177140235900879,
      "learning_rate": 3.74536184298337e-05,
      "loss": 0.5445,
      "step": 47000
    },
    {
      "epoch": 0.7607912228717867,
      "grad_norm": 3.394289255142212,
      "learning_rate": 3.732014628547022e-05,
      "loss": 0.5473,
      "step": 47500
    },
    {
      "epoch": 0.7687995515335949,
      "grad_norm": 10.127290725708008,
      "learning_rate": 3.7186674141106754e-05,
      "loss": 0.5494,
      "step": 48000
    },
    {
      "epoch": 0.7768078801954033,
      "grad_norm": 5.279999256134033,
      "learning_rate": 3.705320199674328e-05,
      "loss": 0.5584,
      "step": 48500
    },
    {
      "epoch": 0.7848162088572115,
      "grad_norm": 13.918063163757324,
      "learning_rate": 3.691972985237981e-05,
      "loss": 0.5715,
      "step": 49000
    },
    {
      "epoch": 0.7928245375190198,
      "grad_norm": 5.973888397216797,
      "learning_rate": 3.6786257708016334e-05,
      "loss": 0.5735,
      "step": 49500
    },
    {
      "epoch": 0.800832866180828,
      "grad_norm": 17.23167610168457,
      "learning_rate": 3.665278556365287e-05,
      "loss": 0.5735,
      "step": 50000
    },
    {
      "epoch": 0.8088411948426364,
      "grad_norm": 3.0458498001098633,
      "learning_rate": 3.6519313419289396e-05,
      "loss": 0.6101,
      "step": 50500
    },
    {
      "epoch": 0.8168495235044446,
      "grad_norm": 5.47400426864624,
      "learning_rate": 3.638584127492593e-05,
      "loss": 0.569,
      "step": 51000
    },
    {
      "epoch": 0.824857852166253,
      "grad_norm": 3.2479183673858643,
      "learning_rate": 3.625236913056245e-05,
      "loss": 0.5747,
      "step": 51500
    },
    {
      "epoch": 0.8328661808280612,
      "grad_norm": 8.307097434997559,
      "learning_rate": 3.611889698619898e-05,
      "loss": 0.5658,
      "step": 52000
    },
    {
      "epoch": 0.8408745094898694,
      "grad_norm": 3.029916763305664,
      "learning_rate": 3.5985424841835514e-05,
      "loss": 0.5746,
      "step": 52500
    },
    {
      "epoch": 0.8488828381516778,
      "grad_norm": 3.647195339202881,
      "learning_rate": 3.585195269747204e-05,
      "loss": 0.6002,
      "step": 53000
    },
    {
      "epoch": 0.856891166813486,
      "grad_norm": 2.2078123092651367,
      "learning_rate": 3.571848055310857e-05,
      "loss": 0.5973,
      "step": 53500
    },
    {
      "epoch": 0.8648994954752943,
      "grad_norm": 4.585103511810303,
      "learning_rate": 3.5585008408745094e-05,
      "loss": 0.6071,
      "step": 54000
    },
    {
      "epoch": 0.8729078241371026,
      "grad_norm": 30.27272605895996,
      "learning_rate": 3.5451536264381625e-05,
      "loss": 0.5817,
      "step": 54500
    },
    {
      "epoch": 0.8809161527989109,
      "grad_norm": 3.619690418243408,
      "learning_rate": 3.531806412001815e-05,
      "loss": 0.6166,
      "step": 55000
    },
    {
      "epoch": 0.8889244814607191,
      "grad_norm": 4.975050449371338,
      "learning_rate": 3.518459197565469e-05,
      "loss": 0.618,
      "step": 55500
    },
    {
      "epoch": 0.8969328101225275,
      "grad_norm": 2.7916760444641113,
      "learning_rate": 3.505111983129121e-05,
      "loss": 0.5827,
      "step": 56000
    },
    {
      "epoch": 0.9049411387843357,
      "grad_norm": 2.5821750164031982,
      "learning_rate": 3.491764768692774e-05,
      "loss": 0.5558,
      "step": 56500
    },
    {
      "epoch": 0.912949467446144,
      "grad_norm": 11.432326316833496,
      "learning_rate": 3.478417554256427e-05,
      "loss": 0.5964,
      "step": 57000
    },
    {
      "epoch": 0.9209577961079523,
      "grad_norm": 7.981192111968994,
      "learning_rate": 3.46507033982008e-05,
      "loss": 0.5874,
      "step": 57500
    },
    {
      "epoch": 0.9289661247697606,
      "grad_norm": 8.353496551513672,
      "learning_rate": 3.451723125383732e-05,
      "loss": 0.6429,
      "step": 58000
    },
    {
      "epoch": 0.9369744534315688,
      "grad_norm": 3.98939847946167,
      "learning_rate": 3.4383759109473854e-05,
      "loss": 0.6455,
      "step": 58500
    },
    {
      "epoch": 0.9449827820933772,
      "grad_norm": 8.27649974822998,
      "learning_rate": 3.4250286965110385e-05,
      "loss": 0.5744,
      "step": 59000
    },
    {
      "epoch": 0.9529911107551854,
      "grad_norm": 3.5062062740325928,
      "learning_rate": 3.411681482074691e-05,
      "loss": 0.5702,
      "step": 59500
    },
    {
      "epoch": 0.9609994394169937,
      "grad_norm": 15.206467628479004,
      "learning_rate": 3.398334267638344e-05,
      "loss": 0.5758,
      "step": 60000
    },
    {
      "epoch": 0.969007768078802,
      "grad_norm": 17.04636573791504,
      "learning_rate": 3.3849870532019965e-05,
      "loss": 0.5856,
      "step": 60500
    },
    {
      "epoch": 0.9770160967406102,
      "grad_norm": 6.201862812042236,
      "learning_rate": 3.37163983876565e-05,
      "loss": 0.6112,
      "step": 61000
    },
    {
      "epoch": 0.9850244254024185,
      "grad_norm": 5.430823802947998,
      "learning_rate": 3.358292624329303e-05,
      "loss": 0.6022,
      "step": 61500
    },
    {
      "epoch": 0.9930327540642268,
      "grad_norm": 12.381128311157227,
      "learning_rate": 3.344945409892956e-05,
      "loss": 0.5864,
      "step": 62000
    },
    {
      "epoch": 1.001041082726035,
      "grad_norm": 4.036890029907227,
      "learning_rate": 3.331598195456608e-05,
      "loss": 0.5762,
      "step": 62500
    },
    {
      "epoch": 1.0090494113878434,
      "grad_norm": 3.122976541519165,
      "learning_rate": 3.3182509810202614e-05,
      "loss": 0.6011,
      "step": 63000
    },
    {
      "epoch": 1.0170577400496517,
      "grad_norm": 2.9697132110595703,
      "learning_rate": 3.304903766583914e-05,
      "loss": 0.5758,
      "step": 63500
    },
    {
      "epoch": 1.0250660687114599,
      "grad_norm": 7.880277156829834,
      "learning_rate": 3.291556552147567e-05,
      "loss": 0.5538,
      "step": 64000
    },
    {
      "epoch": 1.0330743973732681,
      "grad_norm": 10.287847518920898,
      "learning_rate": 3.27820933771122e-05,
      "loss": 0.5684,
      "step": 64500
    },
    {
      "epoch": 1.0410827260350766,
      "grad_norm": 29.196542739868164,
      "learning_rate": 3.2648621232748725e-05,
      "loss": 0.5852,
      "step": 65000
    },
    {
      "epoch": 1.0490910546968848,
      "grad_norm": 13.388043403625488,
      "learning_rate": 3.2515149088385256e-05,
      "loss": 0.5929,
      "step": 65500
    },
    {
      "epoch": 1.057099383358693,
      "grad_norm": 2.2638657093048096,
      "learning_rate": 3.238167694402178e-05,
      "loss": 0.5985,
      "step": 66000
    },
    {
      "epoch": 1.0651077120205013,
      "grad_norm": 46.76168441772461,
      "learning_rate": 3.224820479965831e-05,
      "loss": 0.5722,
      "step": 66500
    },
    {
      "epoch": 1.0731160406823097,
      "grad_norm": 10.283295631408691,
      "learning_rate": 3.211473265529484e-05,
      "loss": 0.5599,
      "step": 67000
    },
    {
      "epoch": 1.081124369344118,
      "grad_norm": 4.871533393859863,
      "learning_rate": 3.1981260510931374e-05,
      "loss": 0.5426,
      "step": 67500
    },
    {
      "epoch": 1.0891326980059262,
      "grad_norm": 4.29221248626709,
      "learning_rate": 3.18477883665679e-05,
      "loss": 0.5692,
      "step": 68000
    },
    {
      "epoch": 1.0971410266677344,
      "grad_norm": 21.451339721679688,
      "learning_rate": 3.171431622220443e-05,
      "loss": 0.5576,
      "step": 68500
    },
    {
      "epoch": 1.1051493553295426,
      "grad_norm": 5.6028666496276855,
      "learning_rate": 3.1580844077840954e-05,
      "loss": 0.5685,
      "step": 69000
    },
    {
      "epoch": 1.113157683991351,
      "grad_norm": 4.469378471374512,
      "learning_rate": 3.1447371933477485e-05,
      "loss": 0.5372,
      "step": 69500
    },
    {
      "epoch": 1.1211660126531593,
      "grad_norm": 6.250676155090332,
      "learning_rate": 3.1313899789114016e-05,
      "loss": 0.528,
      "step": 70000
    },
    {
      "epoch": 1.1291743413149675,
      "grad_norm": 2.7597177028656006,
      "learning_rate": 3.118042764475054e-05,
      "loss": 0.525,
      "step": 70500
    },
    {
      "epoch": 1.1371826699767758,
      "grad_norm": 3.4835798740386963,
      "learning_rate": 3.104695550038707e-05,
      "loss": 0.5368,
      "step": 71000
    },
    {
      "epoch": 1.1451909986385842,
      "grad_norm": 4.725292205810547,
      "learning_rate": 3.0913483356023596e-05,
      "loss": 0.53,
      "step": 71500
    },
    {
      "epoch": 1.1531993273003924,
      "grad_norm": 4.831820487976074,
      "learning_rate": 3.078001121166013e-05,
      "loss": 0.5339,
      "step": 72000
    },
    {
      "epoch": 1.1612076559622007,
      "grad_norm": 3.886702537536621,
      "learning_rate": 3.064653906729666e-05,
      "loss": 0.5383,
      "step": 72500
    },
    {
      "epoch": 1.1692159846240089,
      "grad_norm": 2.8685832023620605,
      "learning_rate": 3.0513066922933186e-05,
      "loss": 0.5331,
      "step": 73000
    },
    {
      "epoch": 1.1772243132858173,
      "grad_norm": 13.397988319396973,
      "learning_rate": 3.0379594778569713e-05,
      "loss": 0.5253,
      "step": 73500
    },
    {
      "epoch": 1.1852326419476256,
      "grad_norm": 6.273265361785889,
      "learning_rate": 3.024612263420624e-05,
      "loss": 0.5244,
      "step": 74000
    },
    {
      "epoch": 1.1932409706094338,
      "grad_norm": 3.430068016052246,
      "learning_rate": 3.011265048984277e-05,
      "loss": 0.5544,
      "step": 74500
    },
    {
      "epoch": 1.201249299271242,
      "grad_norm": 160.6565704345703,
      "learning_rate": 2.9979178345479297e-05,
      "loss": 0.5334,
      "step": 75000
    },
    {
      "epoch": 1.2092576279330505,
      "grad_norm": 4.157841205596924,
      "learning_rate": 2.984570620111583e-05,
      "loss": 0.5325,
      "step": 75500
    },
    {
      "epoch": 1.2172659565948587,
      "grad_norm": 23.81913185119629,
      "learning_rate": 2.971223405675236e-05,
      "loss": 0.5385,
      "step": 76000
    },
    {
      "epoch": 1.225274285256667,
      "grad_norm": 6.362563133239746,
      "learning_rate": 2.9578761912388887e-05,
      "loss": 0.5406,
      "step": 76500
    },
    {
      "epoch": 1.2332826139184752,
      "grad_norm": 2.9250121116638184,
      "learning_rate": 2.9445289768025414e-05,
      "loss": 0.5285,
      "step": 77000
    },
    {
      "epoch": 1.2412909425802834,
      "grad_norm": 12.129765510559082,
      "learning_rate": 2.9311817623661942e-05,
      "loss": 0.5336,
      "step": 77500
    },
    {
      "epoch": 1.2492992712420918,
      "grad_norm": 4.581326007843018,
      "learning_rate": 2.9178345479298473e-05,
      "loss": 0.5289,
      "step": 78000
    },
    {
      "epoch": 1.2573075999039,
      "grad_norm": 10.895811080932617,
      "learning_rate": 2.9044873334935e-05,
      "loss": 0.544,
      "step": 78500
    },
    {
      "epoch": 1.2653159285657083,
      "grad_norm": 4.070727825164795,
      "learning_rate": 2.891140119057153e-05,
      "loss": 0.5162,
      "step": 79000
    },
    {
      "epoch": 1.2733242572275167,
      "grad_norm": 4.06965970993042,
      "learning_rate": 2.8777929046208057e-05,
      "loss": 0.5294,
      "step": 79500
    },
    {
      "epoch": 1.281332585889325,
      "grad_norm": 4.579061985015869,
      "learning_rate": 2.8644456901844584e-05,
      "loss": 0.52,
      "step": 80000
    },
    {
      "epoch": 1.2893409145511332,
      "grad_norm": 6.460152626037598,
      "learning_rate": 2.8510984757481112e-05,
      "loss": 0.5258,
      "step": 80500
    },
    {
      "epoch": 1.2973492432129414,
      "grad_norm": 7.23961877822876,
      "learning_rate": 2.8377512613117647e-05,
      "loss": 0.5414,
      "step": 81000
    },
    {
      "epoch": 1.3053575718747497,
      "grad_norm": 2.1204023361206055,
      "learning_rate": 2.8244040468754174e-05,
      "loss": 0.5283,
      "step": 81500
    },
    {
      "epoch": 1.3133659005365579,
      "grad_norm": 5.8042073249816895,
      "learning_rate": 2.8110568324390702e-05,
      "loss": 0.5444,
      "step": 82000
    },
    {
      "epoch": 1.3213742291983663,
      "grad_norm": 3.8973348140716553,
      "learning_rate": 2.797709618002723e-05,
      "loss": 0.5247,
      "step": 82500
    },
    {
      "epoch": 1.3293825578601746,
      "grad_norm": 2.659574508666992,
      "learning_rate": 2.7843624035663758e-05,
      "loss": 0.5288,
      "step": 83000
    },
    {
      "epoch": 1.3373908865219828,
      "grad_norm": 3.706261157989502,
      "learning_rate": 2.7710151891300285e-05,
      "loss": 0.532,
      "step": 83500
    },
    {
      "epoch": 1.3453992151837912,
      "grad_norm": 20.325761795043945,
      "learning_rate": 2.7576679746936817e-05,
      "loss": 0.5168,
      "step": 84000
    },
    {
      "epoch": 1.3534075438455995,
      "grad_norm": 2.9860475063323975,
      "learning_rate": 2.7443207602573344e-05,
      "loss": 0.5249,
      "step": 84500
    },
    {
      "epoch": 1.3614158725074077,
      "grad_norm": 5.427518367767334,
      "learning_rate": 2.7309735458209872e-05,
      "loss": 0.5167,
      "step": 85000
    },
    {
      "epoch": 1.369424201169216,
      "grad_norm": 6.439121246337891,
      "learning_rate": 2.71762633138464e-05,
      "loss": 0.5392,
      "step": 85500
    },
    {
      "epoch": 1.3774325298310242,
      "grad_norm": 8.334936141967773,
      "learning_rate": 2.7042791169482927e-05,
      "loss": 0.5157,
      "step": 86000
    },
    {
      "epoch": 1.3854408584928326,
      "grad_norm": 4.830845355987549,
      "learning_rate": 2.6909319025119462e-05,
      "loss": 0.5368,
      "step": 86500
    },
    {
      "epoch": 1.3934491871546408,
      "grad_norm": 2.1246495246887207,
      "learning_rate": 2.677584688075599e-05,
      "loss": 0.5088,
      "step": 87000
    },
    {
      "epoch": 1.401457515816449,
      "grad_norm": 2.3075292110443115,
      "learning_rate": 2.6642374736392518e-05,
      "loss": 0.5368,
      "step": 87500
    },
    {
      "epoch": 1.4094658444782575,
      "grad_norm": 2.213160514831543,
      "learning_rate": 2.6508902592029045e-05,
      "loss": 0.5422,
      "step": 88000
    },
    {
      "epoch": 1.4174741731400657,
      "grad_norm": 12.27783489227295,
      "learning_rate": 2.6375430447665573e-05,
      "loss": 0.5403,
      "step": 88500
    },
    {
      "epoch": 1.425482501801874,
      "grad_norm": 4.510140419006348,
      "learning_rate": 2.62419583033021e-05,
      "loss": 0.5352,
      "step": 89000
    },
    {
      "epoch": 1.4334908304636822,
      "grad_norm": 4.781023025512695,
      "learning_rate": 2.6108486158938632e-05,
      "loss": 0.5149,
      "step": 89500
    },
    {
      "epoch": 1.4414991591254904,
      "grad_norm": 6.79545783996582,
      "learning_rate": 2.597501401457516e-05,
      "loss": 0.5373,
      "step": 90000
    },
    {
      "epoch": 1.4495074877872987,
      "grad_norm": 49.147579193115234,
      "learning_rate": 2.5841541870211687e-05,
      "loss": 0.5313,
      "step": 90500
    },
    {
      "epoch": 1.457515816449107,
      "grad_norm": 2.4180684089660645,
      "learning_rate": 2.5708069725848215e-05,
      "loss": 0.5341,
      "step": 91000
    },
    {
      "epoch": 1.4655241451109153,
      "grad_norm": 4.6710968017578125,
      "learning_rate": 2.5574597581484743e-05,
      "loss": 0.5254,
      "step": 91500
    },
    {
      "epoch": 1.4735324737727236,
      "grad_norm": 13.610833168029785,
      "learning_rate": 2.544112543712127e-05,
      "loss": 0.5334,
      "step": 92000
    },
    {
      "epoch": 1.481540802434532,
      "grad_norm": 3.6180546283721924,
      "learning_rate": 2.5307653292757805e-05,
      "loss": 0.5404,
      "step": 92500
    },
    {
      "epoch": 1.4895491310963402,
      "grad_norm": 9.872357368469238,
      "learning_rate": 2.5174181148394333e-05,
      "loss": 0.5243,
      "step": 93000
    },
    {
      "epoch": 1.4975574597581485,
      "grad_norm": 11.83433723449707,
      "learning_rate": 2.504070900403086e-05,
      "loss": 0.5212,
      "step": 93500
    },
    {
      "epoch": 1.5055657884199567,
      "grad_norm": 5.849399566650391,
      "learning_rate": 2.490723685966739e-05,
      "loss": 0.536,
      "step": 94000
    },
    {
      "epoch": 1.513574117081765,
      "grad_norm": 6.792448043823242,
      "learning_rate": 2.477376471530392e-05,
      "loss": 0.5139,
      "step": 94500
    },
    {
      "epoch": 1.5215824457435732,
      "grad_norm": 10.482234954833984,
      "learning_rate": 2.4640292570940447e-05,
      "loss": 0.514,
      "step": 95000
    },
    {
      "epoch": 1.5295907744053816,
      "grad_norm": 3.7406227588653564,
      "learning_rate": 2.4506820426576975e-05,
      "loss": 0.5278,
      "step": 95500
    },
    {
      "epoch": 1.5375991030671898,
      "grad_norm": 5.730695724487305,
      "learning_rate": 2.4373348282213503e-05,
      "loss": 0.5093,
      "step": 96000
    },
    {
      "epoch": 1.5456074317289983,
      "grad_norm": 3.3730945587158203,
      "learning_rate": 2.423987613785003e-05,
      "loss": 0.5235,
      "step": 96500
    },
    {
      "epoch": 1.5536157603908065,
      "grad_norm": 3.492950916290283,
      "learning_rate": 2.4106403993486558e-05,
      "loss": 0.5175,
      "step": 97000
    },
    {
      "epoch": 1.5616240890526147,
      "grad_norm": 6.554967880249023,
      "learning_rate": 2.397293184912309e-05,
      "loss": 0.5368,
      "step": 97500
    },
    {
      "epoch": 1.569632417714423,
      "grad_norm": 2.112107515335083,
      "learning_rate": 2.3839459704759617e-05,
      "loss": 0.5272,
      "step": 98000
    },
    {
      "epoch": 1.5776407463762312,
      "grad_norm": 6.589860439300537,
      "learning_rate": 2.3705987560396145e-05,
      "loss": 0.5219,
      "step": 98500
    },
    {
      "epoch": 1.5856490750380394,
      "grad_norm": 9.147485733032227,
      "learning_rate": 2.3572515416032676e-05,
      "loss": 0.5768,
      "step": 99000
    },
    {
      "epoch": 1.5936574036998479,
      "grad_norm": 2.691847801208496,
      "learning_rate": 2.3439043271669204e-05,
      "loss": 0.5257,
      "step": 99500
    },
    {
      "epoch": 1.601665732361656,
      "grad_norm": 2.4653239250183105,
      "learning_rate": 2.3305571127305735e-05,
      "loss": 0.522,
      "step": 100000
    },
    {
      "epoch": 1.6096740610234646,
      "grad_norm": 11.085479736328125,
      "learning_rate": 2.3172098982942263e-05,
      "loss": 0.5168,
      "step": 100500
    },
    {
      "epoch": 1.6176823896852728,
      "grad_norm": 10.273331642150879,
      "learning_rate": 2.303862683857879e-05,
      "loss": 0.5353,
      "step": 101000
    },
    {
      "epoch": 1.625690718347081,
      "grad_norm": 2.038649559020996,
      "learning_rate": 2.2905154694215318e-05,
      "loss": 0.5202,
      "step": 101500
    },
    {
      "epoch": 1.6336990470088892,
      "grad_norm": 4.0400004386901855,
      "learning_rate": 2.2771682549851846e-05,
      "loss": 0.5176,
      "step": 102000
    },
    {
      "epoch": 1.6417073756706975,
      "grad_norm": 2.4204163551330566,
      "learning_rate": 2.2638210405488374e-05,
      "loss": 0.5092,
      "step": 102500
    },
    {
      "epoch": 1.6497157043325057,
      "grad_norm": 3.1426825523376465,
      "learning_rate": 2.2504738261124905e-05,
      "loss": 0.5198,
      "step": 103000
    },
    {
      "epoch": 1.657724032994314,
      "grad_norm": 8.044781684875488,
      "learning_rate": 2.2371266116761433e-05,
      "loss": 0.5194,
      "step": 103500
    },
    {
      "epoch": 1.6657323616561224,
      "grad_norm": 3.9673469066619873,
      "learning_rate": 2.223779397239796e-05,
      "loss": 0.5217,
      "step": 104000
    },
    {
      "epoch": 1.6737406903179306,
      "grad_norm": 11.519086837768555,
      "learning_rate": 2.210432182803449e-05,
      "loss": 0.5095,
      "step": 104500
    },
    {
      "epoch": 1.681749018979739,
      "grad_norm": 5.464014530181885,
      "learning_rate": 2.197084968367102e-05,
      "loss": 0.5355,
      "step": 105000
    },
    {
      "epoch": 1.6897573476415473,
      "grad_norm": 4.318820476531982,
      "learning_rate": 2.1837377539307547e-05,
      "loss": 0.5205,
      "step": 105500
    },
    {
      "epoch": 1.6977656763033555,
      "grad_norm": 6.407961845397949,
      "learning_rate": 2.1703905394944078e-05,
      "loss": 0.5122,
      "step": 106000
    },
    {
      "epoch": 1.7057740049651637,
      "grad_norm": 8.025530815124512,
      "learning_rate": 2.1570433250580606e-05,
      "loss": 0.5217,
      "step": 106500
    },
    {
      "epoch": 1.713782333626972,
      "grad_norm": 4.854086399078369,
      "learning_rate": 2.1436961106217134e-05,
      "loss": 0.5269,
      "step": 107000
    },
    {
      "epoch": 1.7217906622887802,
      "grad_norm": 4.895215034484863,
      "learning_rate": 2.130348896185366e-05,
      "loss": 0.5468,
      "step": 107500
    },
    {
      "epoch": 1.7297989909505886,
      "grad_norm": 14.803995132446289,
      "learning_rate": 2.117001681749019e-05,
      "loss": 0.5172,
      "step": 108000
    },
    {
      "epoch": 1.7378073196123969,
      "grad_norm": 4.8039021492004395,
      "learning_rate": 2.103654467312672e-05,
      "loss": 0.5071,
      "step": 108500
    },
    {
      "epoch": 1.7458156482742053,
      "grad_norm": 6.83394718170166,
      "learning_rate": 2.0903072528763248e-05,
      "loss": 0.5062,
      "step": 109000
    },
    {
      "epoch": 1.7538239769360136,
      "grad_norm": 3.1579933166503906,
      "learning_rate": 2.0769600384399776e-05,
      "loss": 0.5295,
      "step": 109500
    },
    {
      "epoch": 1.7618323055978218,
      "grad_norm": 5.3173370361328125,
      "learning_rate": 2.0636128240036307e-05,
      "loss": 0.5125,
      "step": 110000
    },
    {
      "epoch": 1.76984063425963,
      "grad_norm": 3.3504600524902344,
      "learning_rate": 2.0502656095672835e-05,
      "loss": 0.5289,
      "step": 110500
    },
    {
      "epoch": 1.7778489629214382,
      "grad_norm": 1.5318541526794434,
      "learning_rate": 2.0369183951309362e-05,
      "loss": 0.519,
      "step": 111000
    },
    {
      "epoch": 1.7858572915832465,
      "grad_norm": 6.516898155212402,
      "learning_rate": 2.0235711806945893e-05,
      "loss": 0.5218,
      "step": 111500
    },
    {
      "epoch": 1.7938656202450547,
      "grad_norm": 5.115590572357178,
      "learning_rate": 2.010223966258242e-05,
      "loss": 0.5187,
      "step": 112000
    },
    {
      "epoch": 1.8018739489068631,
      "grad_norm": 1.8095836639404297,
      "learning_rate": 1.996876751821895e-05,
      "loss": 0.5086,
      "step": 112500
    },
    {
      "epoch": 1.8098822775686714,
      "grad_norm": 4.258097171783447,
      "learning_rate": 1.9835295373855477e-05,
      "loss": 0.5159,
      "step": 113000
    },
    {
      "epoch": 1.8178906062304798,
      "grad_norm": 3.70536732673645,
      "learning_rate": 1.9701823229492004e-05,
      "loss": 0.5177,
      "step": 113500
    },
    {
      "epoch": 1.825898934892288,
      "grad_norm": 4.52922248840332,
      "learning_rate": 1.9568351085128532e-05,
      "loss": 0.5163,
      "step": 114000
    },
    {
      "epoch": 1.8339072635540963,
      "grad_norm": 4.456188678741455,
      "learning_rate": 1.9434878940765063e-05,
      "loss": 0.5166,
      "step": 114500
    },
    {
      "epoch": 1.8419155922159045,
      "grad_norm": 3.5256004333496094,
      "learning_rate": 1.930140679640159e-05,
      "loss": 0.5269,
      "step": 115000
    },
    {
      "epoch": 1.8499239208777127,
      "grad_norm": 3.6694319248199463,
      "learning_rate": 1.916793465203812e-05,
      "loss": 0.5113,
      "step": 115500
    },
    {
      "epoch": 1.857932249539521,
      "grad_norm": 4.273138046264648,
      "learning_rate": 1.903446250767465e-05,
      "loss": 0.5139,
      "step": 116000
    },
    {
      "epoch": 1.8659405782013294,
      "grad_norm": 13.877301216125488,
      "learning_rate": 1.8900990363311178e-05,
      "loss": 0.5241,
      "step": 116500
    },
    {
      "epoch": 1.8739489068631376,
      "grad_norm": 3.783949851989746,
      "learning_rate": 1.876751821894771e-05,
      "loss": 0.5213,
      "step": 117000
    },
    {
      "epoch": 1.881957235524946,
      "grad_norm": 4.728069305419922,
      "learning_rate": 1.8634046074584237e-05,
      "loss": 0.527,
      "step": 117500
    },
    {
      "epoch": 1.8899655641867543,
      "grad_norm": 6.048025131225586,
      "learning_rate": 1.8500573930220764e-05,
      "loss": 0.5311,
      "step": 118000
    },
    {
      "epoch": 1.8979738928485625,
      "grad_norm": 6.48728609085083,
      "learning_rate": 1.8367101785857292e-05,
      "loss": 0.5388,
      "step": 118500
    },
    {
      "epoch": 1.9059822215103708,
      "grad_norm": 4.1666388511657715,
      "learning_rate": 1.823362964149382e-05,
      "loss": 0.5276,
      "step": 119000
    },
    {
      "epoch": 1.913990550172179,
      "grad_norm": 5.920313835144043,
      "learning_rate": 1.8100157497130348e-05,
      "loss": 0.5154,
      "step": 119500
    },
    {
      "epoch": 1.9219988788339872,
      "grad_norm": 5.515158176422119,
      "learning_rate": 1.796668535276688e-05,
      "loss": 0.5377,
      "step": 120000
    },
    {
      "epoch": 1.9300072074957955,
      "grad_norm": 4.867806911468506,
      "learning_rate": 1.7833213208403406e-05,
      "loss": 0.5142,
      "step": 120500
    },
    {
      "epoch": 1.938015536157604,
      "grad_norm": 5.740201473236084,
      "learning_rate": 1.7699741064039934e-05,
      "loss": 0.5121,
      "step": 121000
    },
    {
      "epoch": 1.9460238648194121,
      "grad_norm": 6.0920305252075195,
      "learning_rate": 1.7566268919676465e-05,
      "loss": 0.5073,
      "step": 121500
    },
    {
      "epoch": 1.9540321934812206,
      "grad_norm": 3.8682684898376465,
      "learning_rate": 1.7432796775312993e-05,
      "loss": 0.5235,
      "step": 122000
    },
    {
      "epoch": 1.9620405221430288,
      "grad_norm": 11.944962501525879,
      "learning_rate": 1.729932463094952e-05,
      "loss": 0.5218,
      "step": 122500
    },
    {
      "epoch": 1.970048850804837,
      "grad_norm": 4.42965841293335,
      "learning_rate": 1.7165852486586052e-05,
      "loss": 0.5051,
      "step": 123000
    },
    {
      "epoch": 1.9780571794666453,
      "grad_norm": 2.3707287311553955,
      "learning_rate": 1.703238034222258e-05,
      "loss": 0.51,
      "step": 123500
    },
    {
      "epoch": 1.9860655081284535,
      "grad_norm": 3.7566802501678467,
      "learning_rate": 1.6898908197859107e-05,
      "loss": 0.5182,
      "step": 124000
    },
    {
      "epoch": 1.9940738367902617,
      "grad_norm": 3.688204765319824,
      "learning_rate": 1.6765436053495635e-05,
      "loss": 0.5216,
      "step": 124500
    },
    {
      "epoch": 2.00208216545207,
      "grad_norm": 4.6394548416137695,
      "learning_rate": 1.6631963909132163e-05,
      "loss": 0.52,
      "step": 125000
    },
    {
      "epoch": 2.0100904941138786,
      "grad_norm": 3.345848560333252,
      "learning_rate": 1.6498491764768694e-05,
      "loss": 0.491,
      "step": 125500
    },
    {
      "epoch": 2.018098822775687,
      "grad_norm": 6.613098621368408,
      "learning_rate": 1.6365019620405222e-05,
      "loss": 0.4849,
      "step": 126000
    },
    {
      "epoch": 2.026107151437495,
      "grad_norm": 5.686723232269287,
      "learning_rate": 1.623154747604175e-05,
      "loss": 0.4725,
      "step": 126500
    },
    {
      "epoch": 2.0341154800993033,
      "grad_norm": 4.618155479431152,
      "learning_rate": 1.609807533167828e-05,
      "loss": 0.4939,
      "step": 127000
    },
    {
      "epoch": 2.0421238087611115,
      "grad_norm": 7.990642547607422,
      "learning_rate": 1.596460318731481e-05,
      "loss": 0.4953,
      "step": 127500
    },
    {
      "epoch": 2.0501321374229198,
      "grad_norm": 6.9659929275512695,
      "learning_rate": 1.5831131042951336e-05,
      "loss": 0.4811,
      "step": 128000
    },
    {
      "epoch": 2.058140466084728,
      "grad_norm": 4.644782543182373,
      "learning_rate": 1.5697658898587867e-05,
      "loss": 0.4862,
      "step": 128500
    },
    {
      "epoch": 2.0661487947465362,
      "grad_norm": 8.351694107055664,
      "learning_rate": 1.5564186754224395e-05,
      "loss": 0.4901,
      "step": 129000
    },
    {
      "epoch": 2.0741571234083445,
      "grad_norm": 4.961209297180176,
      "learning_rate": 1.5430714609860923e-05,
      "loss": 0.4705,
      "step": 129500
    },
    {
      "epoch": 2.082165452070153,
      "grad_norm": 11.790966033935547,
      "learning_rate": 1.529724246549745e-05,
      "loss": 0.4914,
      "step": 130000
    },
    {
      "epoch": 2.0901737807319614,
      "grad_norm": 15.83209228515625,
      "learning_rate": 1.516377032113398e-05,
      "loss": 0.4955,
      "step": 130500
    },
    {
      "epoch": 2.0981821093937696,
      "grad_norm": 6.013333320617676,
      "learning_rate": 1.5030298176770508e-05,
      "loss": 0.4834,
      "step": 131000
    },
    {
      "epoch": 2.106190438055578,
      "grad_norm": 8.343938827514648,
      "learning_rate": 1.4896826032407037e-05,
      "loss": 0.4839,
      "step": 131500
    },
    {
      "epoch": 2.114198766717386,
      "grad_norm": 8.838165283203125,
      "learning_rate": 1.4763353888043565e-05,
      "loss": 0.4923,
      "step": 132000
    },
    {
      "epoch": 2.1222070953791943,
      "grad_norm": 3.5784082412719727,
      "learning_rate": 1.4629881743680093e-05,
      "loss": 0.4999,
      "step": 132500
    },
    {
      "epoch": 2.1302154240410025,
      "grad_norm": 6.789596080780029,
      "learning_rate": 1.4496409599316624e-05,
      "loss": 0.4979,
      "step": 133000
    },
    {
      "epoch": 2.1382237527028107,
      "grad_norm": 3.2773313522338867,
      "learning_rate": 1.4362937454953152e-05,
      "loss": 0.4843,
      "step": 133500
    },
    {
      "epoch": 2.1462320813646194,
      "grad_norm": 7.179351806640625,
      "learning_rate": 1.4229465310589681e-05,
      "loss": 0.4835,
      "step": 134000
    },
    {
      "epoch": 2.1542404100264276,
      "grad_norm": 6.873589992523193,
      "learning_rate": 1.4095993166226209e-05,
      "loss": 0.5038,
      "step": 134500
    },
    {
      "epoch": 2.162248738688236,
      "grad_norm": 5.110401153564453,
      "learning_rate": 1.3962521021862737e-05,
      "loss": 0.4966,
      "step": 135000
    },
    {
      "epoch": 2.170257067350044,
      "grad_norm": 3.0599632263183594,
      "learning_rate": 1.3829048877499268e-05,
      "loss": 0.4974,
      "step": 135500
    },
    {
      "epoch": 2.1782653960118523,
      "grad_norm": 5.403199672698975,
      "learning_rate": 1.3695576733135795e-05,
      "loss": 0.4883,
      "step": 136000
    },
    {
      "epoch": 2.1862737246736605,
      "grad_norm": 10.166191101074219,
      "learning_rate": 1.3562104588772323e-05,
      "loss": 0.4801,
      "step": 136500
    },
    {
      "epoch": 2.1942820533354688,
      "grad_norm": 3.9219131469726562,
      "learning_rate": 1.3428632444408853e-05,
      "loss": 0.4995,
      "step": 137000
    },
    {
      "epoch": 2.202290381997277,
      "grad_norm": 9.928403854370117,
      "learning_rate": 1.329516030004538e-05,
      "loss": 0.4904,
      "step": 137500
    },
    {
      "epoch": 2.2102987106590852,
      "grad_norm": 7.370477199554443,
      "learning_rate": 1.3161688155681908e-05,
      "loss": 0.4715,
      "step": 138000
    },
    {
      "epoch": 2.218307039320894,
      "grad_norm": 4.17348575592041,
      "learning_rate": 1.302821601131844e-05,
      "loss": 0.4946,
      "step": 138500
    },
    {
      "epoch": 2.226315367982702,
      "grad_norm": 4.908074378967285,
      "learning_rate": 1.2894743866954967e-05,
      "loss": 0.4942,
      "step": 139000
    },
    {
      "epoch": 2.2343236966445104,
      "grad_norm": 5.767423152923584,
      "learning_rate": 1.2761271722591495e-05,
      "loss": 0.5023,
      "step": 139500
    },
    {
      "epoch": 2.2423320253063186,
      "grad_norm": 6.429279327392578,
      "learning_rate": 1.2627799578228024e-05,
      "loss": 0.4977,
      "step": 140000
    },
    {
      "epoch": 2.250340353968127,
      "grad_norm": 3.7908496856689453,
      "learning_rate": 1.2494327433864552e-05,
      "loss": 0.49,
      "step": 140500
    },
    {
      "epoch": 2.258348682629935,
      "grad_norm": 3.7526662349700928,
      "learning_rate": 1.2360855289501081e-05,
      "loss": 0.4814,
      "step": 141000
    },
    {
      "epoch": 2.2663570112917433,
      "grad_norm": 5.195950984954834,
      "learning_rate": 1.2227383145137611e-05,
      "loss": 0.4966,
      "step": 141500
    },
    {
      "epoch": 2.2743653399535515,
      "grad_norm": 2.5419557094573975,
      "learning_rate": 1.2093911000774139e-05,
      "loss": 0.4843,
      "step": 142000
    },
    {
      "epoch": 2.28237366861536,
      "grad_norm": 4.773759365081787,
      "learning_rate": 1.1960438856410668e-05,
      "loss": 0.4959,
      "step": 142500
    },
    {
      "epoch": 2.2903819972771684,
      "grad_norm": 1.8961355686187744,
      "learning_rate": 1.1826966712047197e-05,
      "loss": 0.4822,
      "step": 143000
    },
    {
      "epoch": 2.2983903259389766,
      "grad_norm": 5.982407093048096,
      "learning_rate": 1.1693494567683724e-05,
      "loss": 0.468,
      "step": 143500
    },
    {
      "epoch": 2.306398654600785,
      "grad_norm": 9.01443862915039,
      "learning_rate": 1.1560022423320253e-05,
      "loss": 0.4931,
      "step": 144000
    },
    {
      "epoch": 2.314406983262593,
      "grad_norm": 5.718390941619873,
      "learning_rate": 1.1426550278956782e-05,
      "loss": 0.5046,
      "step": 144500
    },
    {
      "epoch": 2.3224153119244013,
      "grad_norm": 3.213411808013916,
      "learning_rate": 1.1293078134593312e-05,
      "loss": 0.502,
      "step": 145000
    },
    {
      "epoch": 2.3304236405862095,
      "grad_norm": 5.6064839363098145,
      "learning_rate": 1.115960599022984e-05,
      "loss": 0.4843,
      "step": 145500
    },
    {
      "epoch": 2.3384319692480178,
      "grad_norm": 12.452762603759766,
      "learning_rate": 1.1026133845866369e-05,
      "loss": 0.4709,
      "step": 146000
    },
    {
      "epoch": 2.346440297909826,
      "grad_norm": 7.003706455230713,
      "learning_rate": 1.0892661701502897e-05,
      "loss": 0.4828,
      "step": 146500
    },
    {
      "epoch": 2.3544486265716347,
      "grad_norm": 3.095332145690918,
      "learning_rate": 1.0759189557139425e-05,
      "loss": 0.4759,
      "step": 147000
    },
    {
      "epoch": 2.362456955233443,
      "grad_norm": 4.040911674499512,
      "learning_rate": 1.0625717412775954e-05,
      "loss": 0.4848,
      "step": 147500
    },
    {
      "epoch": 2.370465283895251,
      "grad_norm": 3.7055633068084717,
      "learning_rate": 1.0492245268412483e-05,
      "loss": 0.4879,
      "step": 148000
    },
    {
      "epoch": 2.3784736125570594,
      "grad_norm": 3.9870054721832275,
      "learning_rate": 1.0358773124049013e-05,
      "loss": 0.4896,
      "step": 148500
    },
    {
      "epoch": 2.3864819412188676,
      "grad_norm": 2.65230393409729,
      "learning_rate": 1.022530097968554e-05,
      "loss": 0.4927,
      "step": 149000
    },
    {
      "epoch": 2.394490269880676,
      "grad_norm": 8.814361572265625,
      "learning_rate": 1.0091828835322068e-05,
      "loss": 0.4972,
      "step": 149500
    },
    {
      "epoch": 2.402498598542484,
      "grad_norm": 7.068905830383301,
      "learning_rate": 9.958356690958598e-06,
      "loss": 0.4935,
      "step": 150000
    },
    {
      "epoch": 2.4105069272042923,
      "grad_norm": 6.750758171081543,
      "learning_rate": 9.824884546595126e-06,
      "loss": 0.4888,
      "step": 150500
    },
    {
      "epoch": 2.418515255866101,
      "grad_norm": 1.9101755619049072,
      "learning_rate": 9.691412402231655e-06,
      "loss": 0.4748,
      "step": 151000
    },
    {
      "epoch": 2.426523584527909,
      "grad_norm": 4.402417182922363,
      "learning_rate": 9.557940257868184e-06,
      "loss": 0.4644,
      "step": 151500
    },
    {
      "epoch": 2.4345319131897174,
      "grad_norm": 5.037499904632568,
      "learning_rate": 9.424468113504712e-06,
      "loss": 0.4858,
      "step": 152000
    },
    {
      "epoch": 2.4425402418515256,
      "grad_norm": 7.066239356994629,
      "learning_rate": 9.29099596914124e-06,
      "loss": 0.4827,
      "step": 152500
    },
    {
      "epoch": 2.450548570513334,
      "grad_norm": 6.070245742797852,
      "learning_rate": 9.15752382477777e-06,
      "loss": 0.465,
      "step": 153000
    },
    {
      "epoch": 2.458556899175142,
      "grad_norm": 4.008568286895752,
      "learning_rate": 9.024051680414299e-06,
      "loss": 0.4721,
      "step": 153500
    },
    {
      "epoch": 2.4665652278369503,
      "grad_norm": 7.347427845001221,
      "learning_rate": 8.890579536050827e-06,
      "loss": 0.4727,
      "step": 154000
    },
    {
      "epoch": 2.4745735564987585,
      "grad_norm": 3.486144781112671,
      "learning_rate": 8.757107391687356e-06,
      "loss": 0.5034,
      "step": 154500
    },
    {
      "epoch": 2.4825818851605668,
      "grad_norm": 3.234773874282837,
      "learning_rate": 8.623635247323884e-06,
      "loss": 0.4851,
      "step": 155000
    },
    {
      "epoch": 2.4905902138223754,
      "grad_norm": 4.342700958251953,
      "learning_rate": 8.490163102960412e-06,
      "loss": 0.48,
      "step": 155500
    },
    {
      "epoch": 2.4985985424841837,
      "grad_norm": 7.917558193206787,
      "learning_rate": 8.356690958596941e-06,
      "loss": 0.4699,
      "step": 156000
    },
    {
      "epoch": 2.506606871145992,
      "grad_norm": 4.106176853179932,
      "learning_rate": 8.22321881423347e-06,
      "loss": 0.4864,
      "step": 156500
    },
    {
      "epoch": 2.5146151998078,
      "grad_norm": 4.815080165863037,
      "learning_rate": 8.08974666987e-06,
      "loss": 0.4763,
      "step": 157000
    },
    {
      "epoch": 2.5226235284696084,
      "grad_norm": 5.983972072601318,
      "learning_rate": 7.956274525506528e-06,
      "loss": 0.5105,
      "step": 157500
    },
    {
      "epoch": 2.5306318571314166,
      "grad_norm": 4.014471530914307,
      "learning_rate": 7.822802381143055e-06,
      "loss": 0.4797,
      "step": 158000
    },
    {
      "epoch": 2.538640185793225,
      "grad_norm": 5.227955341339111,
      "learning_rate": 7.689330236779585e-06,
      "loss": 0.4953,
      "step": 158500
    },
    {
      "epoch": 2.5466485144550335,
      "grad_norm": 3.9301795959472656,
      "learning_rate": 7.5558580924161125e-06,
      "loss": 0.4772,
      "step": 159000
    },
    {
      "epoch": 2.5546568431168417,
      "grad_norm": 9.068924903869629,
      "learning_rate": 7.422385948052642e-06,
      "loss": 0.4985,
      "step": 159500
    },
    {
      "epoch": 2.56266517177865,
      "grad_norm": 15.708979606628418,
      "learning_rate": 7.2889138036891706e-06,
      "loss": 0.4753,
      "step": 160000
    },
    {
      "epoch": 2.570673500440458,
      "grad_norm": 5.85085391998291,
      "learning_rate": 7.155441659325698e-06,
      "loss": 0.4959,
      "step": 160500
    },
    {
      "epoch": 2.5786818291022664,
      "grad_norm": 10.106379508972168,
      "learning_rate": 7.021969514962228e-06,
      "loss": 0.4785,
      "step": 161000
    },
    {
      "epoch": 2.5866901577640746,
      "grad_norm": 3.3619790077209473,
      "learning_rate": 6.888497370598756e-06,
      "loss": 0.4888,
      "step": 161500
    },
    {
      "epoch": 2.594698486425883,
      "grad_norm": 7.827800750732422,
      "learning_rate": 6.755025226235286e-06,
      "loss": 0.492,
      "step": 162000
    },
    {
      "epoch": 2.602706815087691,
      "grad_norm": 6.471998691558838,
      "learning_rate": 6.6215530818718135e-06,
      "loss": 0.4951,
      "step": 162500
    },
    {
      "epoch": 2.6107151437494993,
      "grad_norm": 2.487199544906616,
      "learning_rate": 6.488080937508342e-06,
      "loss": 0.4862,
      "step": 163000
    },
    {
      "epoch": 2.6187234724113075,
      "grad_norm": 7.580549240112305,
      "learning_rate": 6.3546087931448716e-06,
      "loss": 0.4932,
      "step": 163500
    },
    {
      "epoch": 2.6267318010731158,
      "grad_norm": 2.266861915588379,
      "learning_rate": 6.2211366487814e-06,
      "loss": 0.4804,
      "step": 164000
    },
    {
      "epoch": 2.6347401297349244,
      "grad_norm": 5.710116386413574,
      "learning_rate": 6.087664504417928e-06,
      "loss": 0.4715,
      "step": 164500
    },
    {
      "epoch": 2.6427484583967327,
      "grad_norm": 5.276626110076904,
      "learning_rate": 5.954192360054457e-06,
      "loss": 0.4886,
      "step": 165000
    },
    {
      "epoch": 2.650756787058541,
      "grad_norm": 7.006643295288086,
      "learning_rate": 5.820720215690986e-06,
      "loss": 0.479,
      "step": 165500
    },
    {
      "epoch": 2.658765115720349,
      "grad_norm": 4.763871669769287,
      "learning_rate": 5.687248071327514e-06,
      "loss": 0.4769,
      "step": 166000
    },
    {
      "epoch": 2.6667734443821574,
      "grad_norm": 2.7046520709991455,
      "learning_rate": 5.553775926964043e-06,
      "loss": 0.4932,
      "step": 166500
    },
    {
      "epoch": 2.6747817730439656,
      "grad_norm": 4.3285064697265625,
      "learning_rate": 5.420303782600572e-06,
      "loss": 0.4792,
      "step": 167000
    },
    {
      "epoch": 2.6827901017057743,
      "grad_norm": 6.022039413452148,
      "learning_rate": 5.2868316382371e-06,
      "loss": 0.4909,
      "step": 167500
    },
    {
      "epoch": 2.6907984303675825,
      "grad_norm": 1.3580108880996704,
      "learning_rate": 5.153359493873629e-06,
      "loss": 0.4817,
      "step": 168000
    },
    {
      "epoch": 2.6988067590293907,
      "grad_norm": 9.179396629333496,
      "learning_rate": 5.0198873495101575e-06,
      "loss": 0.4846,
      "step": 168500
    },
    {
      "epoch": 2.706815087691199,
      "grad_norm": 4.357848644256592,
      "learning_rate": 4.886415205146686e-06,
      "loss": 0.4766,
      "step": 169000
    },
    {
      "epoch": 2.714823416353007,
      "grad_norm": 9.21454906463623,
      "learning_rate": 4.752943060783215e-06,
      "loss": 0.481,
      "step": 169500
    },
    {
      "epoch": 2.7228317450148154,
      "grad_norm": 3.493400812149048,
      "learning_rate": 4.619470916419743e-06,
      "loss": 0.4807,
      "step": 170000
    },
    {
      "epoch": 2.7308400736766236,
      "grad_norm": 4.984789848327637,
      "learning_rate": 4.485998772056272e-06,
      "loss": 0.4876,
      "step": 170500
    },
    {
      "epoch": 2.738848402338432,
      "grad_norm": 6.943166732788086,
      "learning_rate": 4.352526627692801e-06,
      "loss": 0.4854,
      "step": 171000
    },
    {
      "epoch": 2.74685673100024,
      "grad_norm": 4.886231422424316,
      "learning_rate": 4.219054483329329e-06,
      "loss": 0.4858,
      "step": 171500
    },
    {
      "epoch": 2.7548650596620483,
      "grad_norm": 6.6674041748046875,
      "learning_rate": 4.085582338965858e-06,
      "loss": 0.4793,
      "step": 172000
    },
    {
      "epoch": 2.7628733883238565,
      "grad_norm": 7.113861083984375,
      "learning_rate": 3.952110194602387e-06,
      "loss": 0.4859,
      "step": 172500
    },
    {
      "epoch": 2.770881716985665,
      "grad_norm": 8.29572868347168,
      "learning_rate": 3.818638050238915e-06,
      "loss": 0.4846,
      "step": 173000
    },
    {
      "epoch": 2.7788900456474734,
      "grad_norm": 12.40919303894043,
      "learning_rate": 3.6851659058754443e-06,
      "loss": 0.4897,
      "step": 173500
    },
    {
      "epoch": 2.7868983743092817,
      "grad_norm": 6.548822402954102,
      "learning_rate": 3.5516937615119725e-06,
      "loss": 0.4744,
      "step": 174000
    },
    {
      "epoch": 2.79490670297109,
      "grad_norm": 5.141139984130859,
      "learning_rate": 3.418221617148501e-06,
      "loss": 0.4862,
      "step": 174500
    },
    {
      "epoch": 2.802915031632898,
      "grad_norm": 4.598352432250977,
      "learning_rate": 3.28474947278503e-06,
      "loss": 0.4849,
      "step": 175000
    },
    {
      "epoch": 2.8109233602947064,
      "grad_norm": 12.266292572021484,
      "learning_rate": 3.1512773284215583e-06,
      "loss": 0.4788,
      "step": 175500
    },
    {
      "epoch": 2.818931688956515,
      "grad_norm": 3.243722677230835,
      "learning_rate": 3.0178051840580873e-06,
      "loss": 0.4959,
      "step": 176000
    },
    {
      "epoch": 2.8269400176183233,
      "grad_norm": 9.964518547058105,
      "learning_rate": 2.884333039694616e-06,
      "loss": 0.4876,
      "step": 176500
    },
    {
      "epoch": 2.8349483462801315,
      "grad_norm": 4.309595584869385,
      "learning_rate": 2.7508608953311445e-06,
      "loss": 0.4772,
      "step": 177000
    },
    {
      "epoch": 2.8429566749419397,
      "grad_norm": 3.8585689067840576,
      "learning_rate": 2.617388750967673e-06,
      "loss": 0.485,
      "step": 177500
    },
    {
      "epoch": 2.850965003603748,
      "grad_norm": 4.30459451675415,
      "learning_rate": 2.483916606604202e-06,
      "loss": 0.4755,
      "step": 178000
    },
    {
      "epoch": 2.858973332265556,
      "grad_norm": 5.043994903564453,
      "learning_rate": 2.3504444622407303e-06,
      "loss": 0.4654,
      "step": 178500
    },
    {
      "epoch": 2.8669816609273644,
      "grad_norm": 9.565049171447754,
      "learning_rate": 2.216972317877259e-06,
      "loss": 0.4872,
      "step": 179000
    },
    {
      "epoch": 2.8749899895891726,
      "grad_norm": 10.213244438171387,
      "learning_rate": 2.083500173513788e-06,
      "loss": 0.4844,
      "step": 179500
    },
    {
      "epoch": 2.882998318250981,
      "grad_norm": 2.163796901702881,
      "learning_rate": 1.9500280291503165e-06,
      "loss": 0.4827,
      "step": 180000
    },
    {
      "epoch": 2.891006646912789,
      "grad_norm": 6.1276044845581055,
      "learning_rate": 1.816555884786845e-06,
      "loss": 0.4786,
      "step": 180500
    },
    {
      "epoch": 2.8990149755745973,
      "grad_norm": 4.01878547668457,
      "learning_rate": 1.6830837404233739e-06,
      "loss": 0.4927,
      "step": 181000
    },
    {
      "epoch": 2.907023304236406,
      "grad_norm": 9.985204696655273,
      "learning_rate": 1.5496115960599025e-06,
      "loss": 0.478,
      "step": 181500
    },
    {
      "epoch": 2.915031632898214,
      "grad_norm": 2.642123222351074,
      "learning_rate": 1.416139451696431e-06,
      "loss": 0.4762,
      "step": 182000
    },
    {
      "epoch": 2.9230399615600224,
      "grad_norm": 1.8521270751953125,
      "learning_rate": 1.2826673073329597e-06,
      "loss": 0.4527,
      "step": 182500
    },
    {
      "epoch": 2.9310482902218307,
      "grad_norm": 10.468952178955078,
      "learning_rate": 1.1491951629694885e-06,
      "loss": 0.5051,
      "step": 183000
    },
    {
      "epoch": 2.939056618883639,
      "grad_norm": 1.8325883150100708,
      "learning_rate": 1.0157230186060169e-06,
      "loss": 0.4791,
      "step": 183500
    },
    {
      "epoch": 2.947064947545447,
      "grad_norm": 6.917385578155518,
      "learning_rate": 8.822508742425457e-07,
      "loss": 0.4678,
      "step": 184000
    },
    {
      "epoch": 2.955073276207256,
      "grad_norm": 2.300797939300537,
      "learning_rate": 7.487787298790743e-07,
      "loss": 0.4817,
      "step": 184500
    },
    {
      "epoch": 2.963081604869064,
      "grad_norm": 7.867565631866455,
      "learning_rate": 6.153065855156028e-07,
      "loss": 0.4738,
      "step": 185000
    },
    {
      "epoch": 2.9710899335308723,
      "grad_norm": 9.112363815307617,
      "learning_rate": 4.818344411521315e-07,
      "loss": 0.483,
      "step": 185500
    },
    {
      "epoch": 2.9790982621926805,
      "grad_norm": 6.043625354766846,
      "learning_rate": 3.483622967886602e-07,
      "loss": 0.4814,
      "step": 186000
    },
    {
      "epoch": 2.9871065908544887,
      "grad_norm": 5.873631000518799,
      "learning_rate": 2.148901524251889e-07,
      "loss": 0.4721,
      "step": 186500
    },
    {
      "epoch": 2.995114919516297,
      "grad_norm": 11.005748748779297,
      "learning_rate": 8.141800806171753e-08,
      "loss": 0.4793,
      "step": 187000
    }
  ],
  "logging_steps": 500,
  "max_steps": 187305,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.971310066335959e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
