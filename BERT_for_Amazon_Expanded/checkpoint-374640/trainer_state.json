{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 374640,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00800832866180828,
      "grad_norm": 4.341034412384033,
      "learning_rate": 4.986652785563653e-05,
      "loss": 0.7995,
      "step": 500
    },
    {
      "epoch": 0.01601665732361656,
      "grad_norm": 4.903304576873779,
      "learning_rate": 4.973305571127306e-05,
      "loss": 0.6779,
      "step": 1000
    },
    {
      "epoch": 0.02402498598542484,
      "grad_norm": 7.063520908355713,
      "learning_rate": 4.959958356690959e-05,
      "loss": 0.6516,
      "step": 1500
    },
    {
      "epoch": 0.03203331464723312,
      "grad_norm": 3.4154152870178223,
      "learning_rate": 4.946611142254612e-05,
      "loss": 0.6306,
      "step": 2000
    },
    {
      "epoch": 0.040041643309041405,
      "grad_norm": 4.487967014312744,
      "learning_rate": 4.9332639278182645e-05,
      "loss": 0.6084,
      "step": 2500
    },
    {
      "epoch": 0.04804997197084968,
      "grad_norm": 5.1551971435546875,
      "learning_rate": 4.9199167133819176e-05,
      "loss": 0.6201,
      "step": 3000
    },
    {
      "epoch": 0.056058300632657966,
      "grad_norm": 6.992980003356934,
      "learning_rate": 4.90656949894557e-05,
      "loss": 0.6138,
      "step": 3500
    },
    {
      "epoch": 0.06406662929446624,
      "grad_norm": 14.44216537475586,
      "learning_rate": 4.893222284509223e-05,
      "loss": 0.5986,
      "step": 4000
    },
    {
      "epoch": 0.07207495795627453,
      "grad_norm": 5.647764205932617,
      "learning_rate": 4.879875070072876e-05,
      "loss": 0.6008,
      "step": 4500
    },
    {
      "epoch": 0.08008328661808281,
      "grad_norm": 6.318582534790039,
      "learning_rate": 4.8665278556365293e-05,
      "loss": 0.5978,
      "step": 5000
    },
    {
      "epoch": 0.08809161527989108,
      "grad_norm": 3.0823676586151123,
      "learning_rate": 4.853180641200182e-05,
      "loss": 0.6041,
      "step": 5500
    },
    {
      "epoch": 0.09609994394169936,
      "grad_norm": 3.658535957336426,
      "learning_rate": 4.839833426763835e-05,
      "loss": 0.5961,
      "step": 6000
    },
    {
      "epoch": 0.10410827260350765,
      "grad_norm": 9.705808639526367,
      "learning_rate": 4.826486212327487e-05,
      "loss": 0.575,
      "step": 6500
    },
    {
      "epoch": 0.11211660126531593,
      "grad_norm": 4.798050880432129,
      "learning_rate": 4.81313899789114e-05,
      "loss": 0.5939,
      "step": 7000
    },
    {
      "epoch": 0.12012492992712422,
      "grad_norm": 3.745378255844116,
      "learning_rate": 4.7997917834547936e-05,
      "loss": 0.5876,
      "step": 7500
    },
    {
      "epoch": 0.12813325858893249,
      "grad_norm": 6.620859146118164,
      "learning_rate": 4.786444569018446e-05,
      "loss": 0.5834,
      "step": 8000
    },
    {
      "epoch": 0.13614158725074077,
      "grad_norm": 6.508619785308838,
      "learning_rate": 4.773097354582099e-05,
      "loss": 0.5811,
      "step": 8500
    },
    {
      "epoch": 0.14414991591254905,
      "grad_norm": 3.5845539569854736,
      "learning_rate": 4.7597501401457515e-05,
      "loss": 0.586,
      "step": 9000
    },
    {
      "epoch": 0.15215824457435734,
      "grad_norm": 3.8543505668640137,
      "learning_rate": 4.7464029257094047e-05,
      "loss": 0.5764,
      "step": 9500
    },
    {
      "epoch": 0.16016657323616562,
      "grad_norm": 6.021883964538574,
      "learning_rate": 4.733055711273058e-05,
      "loss": 0.5818,
      "step": 10000
    },
    {
      "epoch": 0.1681749018979739,
      "grad_norm": 5.576465129852295,
      "learning_rate": 4.719708496836711e-05,
      "loss": 0.5723,
      "step": 10500
    },
    {
      "epoch": 0.17618323055978216,
      "grad_norm": 24.022804260253906,
      "learning_rate": 4.706361282400363e-05,
      "loss": 0.5747,
      "step": 11000
    },
    {
      "epoch": 0.18419155922159045,
      "grad_norm": 8.683987617492676,
      "learning_rate": 4.6930140679640164e-05,
      "loss": 0.5779,
      "step": 11500
    },
    {
      "epoch": 0.19219988788339873,
      "grad_norm": 9.578516006469727,
      "learning_rate": 4.679666853527669e-05,
      "loss": 0.5902,
      "step": 12000
    },
    {
      "epoch": 0.200208216545207,
      "grad_norm": 6.387156963348389,
      "learning_rate": 4.666319639091321e-05,
      "loss": 0.5706,
      "step": 12500
    },
    {
      "epoch": 0.2082165452070153,
      "grad_norm": 7.460468292236328,
      "learning_rate": 4.652972424654975e-05,
      "loss": 0.5753,
      "step": 13000
    },
    {
      "epoch": 0.21622487386882358,
      "grad_norm": 2.5201504230499268,
      "learning_rate": 4.6396252102186275e-05,
      "loss": 0.5617,
      "step": 13500
    },
    {
      "epoch": 0.22423320253063186,
      "grad_norm": 10.917168617248535,
      "learning_rate": 4.6262779957822806e-05,
      "loss": 0.5789,
      "step": 14000
    },
    {
      "epoch": 0.23224153119244015,
      "grad_norm": 4.143006801605225,
      "learning_rate": 4.612930781345933e-05,
      "loss": 0.5796,
      "step": 14500
    },
    {
      "epoch": 0.24024985985424843,
      "grad_norm": 5.7884907722473145,
      "learning_rate": 4.599583566909586e-05,
      "loss": 0.5753,
      "step": 15000
    },
    {
      "epoch": 0.2482581885160567,
      "grad_norm": 3.828791856765747,
      "learning_rate": 4.5862363524732386e-05,
      "loss": 0.5697,
      "step": 15500
    },
    {
      "epoch": 0.25626651717786497,
      "grad_norm": 4.151005268096924,
      "learning_rate": 4.5728891380368924e-05,
      "loss": 0.5676,
      "step": 16000
    },
    {
      "epoch": 0.26427484583967326,
      "grad_norm": 2.498488426208496,
      "learning_rate": 4.559541923600545e-05,
      "loss": 0.584,
      "step": 16500
    },
    {
      "epoch": 0.27228317450148154,
      "grad_norm": 7.905001163482666,
      "learning_rate": 4.546194709164198e-05,
      "loss": 0.5704,
      "step": 17000
    },
    {
      "epoch": 0.2802915031632898,
      "grad_norm": 5.520227909088135,
      "learning_rate": 4.5328474947278504e-05,
      "loss": 0.5705,
      "step": 17500
    },
    {
      "epoch": 0.2882998318250981,
      "grad_norm": 3.0246224403381348,
      "learning_rate": 4.5195002802915035e-05,
      "loss": 0.5685,
      "step": 18000
    },
    {
      "epoch": 0.2963081604869064,
      "grad_norm": 3.9577836990356445,
      "learning_rate": 4.5061530658551566e-05,
      "loss": 0.5672,
      "step": 18500
    },
    {
      "epoch": 0.3043164891487147,
      "grad_norm": 3.936753988265991,
      "learning_rate": 4.492805851418809e-05,
      "loss": 0.5723,
      "step": 19000
    },
    {
      "epoch": 0.31232481781052296,
      "grad_norm": 9.493486404418945,
      "learning_rate": 4.479458636982462e-05,
      "loss": 0.5821,
      "step": 19500
    },
    {
      "epoch": 0.32033314647233124,
      "grad_norm": 4.156578063964844,
      "learning_rate": 4.4661114225461146e-05,
      "loss": 0.5514,
      "step": 20000
    },
    {
      "epoch": 0.3283414751341395,
      "grad_norm": 3.5540716648101807,
      "learning_rate": 4.452764208109768e-05,
      "loss": 0.5495,
      "step": 20500
    },
    {
      "epoch": 0.3363498037959478,
      "grad_norm": 3.091482162475586,
      "learning_rate": 4.43941699367342e-05,
      "loss": 0.5689,
      "step": 21000
    },
    {
      "epoch": 0.34435813245775604,
      "grad_norm": 2.9373323917388916,
      "learning_rate": 4.426069779237074e-05,
      "loss": 0.5602,
      "step": 21500
    },
    {
      "epoch": 0.3523664611195643,
      "grad_norm": 2.800938129425049,
      "learning_rate": 4.4127225648007264e-05,
      "loss": 0.5615,
      "step": 22000
    },
    {
      "epoch": 0.3603747897813726,
      "grad_norm": 5.757203102111816,
      "learning_rate": 4.3993753503643795e-05,
      "loss": 0.5678,
      "step": 22500
    },
    {
      "epoch": 0.3683831184431809,
      "grad_norm": 3.7301228046417236,
      "learning_rate": 4.386028135928032e-05,
      "loss": 0.5777,
      "step": 23000
    },
    {
      "epoch": 0.3763914471049892,
      "grad_norm": 32.20902633666992,
      "learning_rate": 4.372680921491685e-05,
      "loss": 0.576,
      "step": 23500
    },
    {
      "epoch": 0.38439977576679746,
      "grad_norm": 4.901029586791992,
      "learning_rate": 4.3593337070553375e-05,
      "loss": 0.588,
      "step": 24000
    },
    {
      "epoch": 0.39240810442860574,
      "grad_norm": 4.396781921386719,
      "learning_rate": 4.3459864926189906e-05,
      "loss": 0.5851,
      "step": 24500
    },
    {
      "epoch": 0.400416433090414,
      "grad_norm": 4.765368461608887,
      "learning_rate": 4.332639278182644e-05,
      "loss": 0.5777,
      "step": 25000
    },
    {
      "epoch": 0.4084247617522223,
      "grad_norm": 10.55105972290039,
      "learning_rate": 4.319292063746296e-05,
      "loss": 0.5738,
      "step": 25500
    },
    {
      "epoch": 0.4164330904140306,
      "grad_norm": 5.845773220062256,
      "learning_rate": 4.305944849309949e-05,
      "loss": 0.5807,
      "step": 26000
    },
    {
      "epoch": 0.4244414190758389,
      "grad_norm": 7.819371700286865,
      "learning_rate": 4.292597634873602e-05,
      "loss": 0.5591,
      "step": 26500
    },
    {
      "epoch": 0.43244974773764716,
      "grad_norm": 2.8302836418151855,
      "learning_rate": 4.2792504204372555e-05,
      "loss": 0.5691,
      "step": 27000
    },
    {
      "epoch": 0.44045807639945544,
      "grad_norm": 10.305069923400879,
      "learning_rate": 4.265903206000908e-05,
      "loss": 0.552,
      "step": 27500
    },
    {
      "epoch": 0.44846640506126373,
      "grad_norm": 6.313445091247559,
      "learning_rate": 4.252555991564561e-05,
      "loss": 0.5567,
      "step": 28000
    },
    {
      "epoch": 0.456474733723072,
      "grad_norm": 4.808748245239258,
      "learning_rate": 4.2392087771282135e-05,
      "loss": 0.5686,
      "step": 28500
    },
    {
      "epoch": 0.4644830623848803,
      "grad_norm": 5.541644096374512,
      "learning_rate": 4.2258615626918666e-05,
      "loss": 0.562,
      "step": 29000
    },
    {
      "epoch": 0.4724913910466886,
      "grad_norm": 3.016921281814575,
      "learning_rate": 4.212514348255519e-05,
      "loss": 0.5752,
      "step": 29500
    },
    {
      "epoch": 0.48049971970849686,
      "grad_norm": 1.342978596687317,
      "learning_rate": 4.199167133819172e-05,
      "loss": 0.5676,
      "step": 30000
    },
    {
      "epoch": 0.4885080483703051,
      "grad_norm": 2.043856620788574,
      "learning_rate": 4.185819919382825e-05,
      "loss": 0.5774,
      "step": 30500
    },
    {
      "epoch": 0.4965163770321134,
      "grad_norm": 5.0765299797058105,
      "learning_rate": 4.172472704946478e-05,
      "loss": 0.5716,
      "step": 31000
    },
    {
      "epoch": 0.5045247056939217,
      "grad_norm": 5.079748630523682,
      "learning_rate": 4.159125490510131e-05,
      "loss": 0.5598,
      "step": 31500
    },
    {
      "epoch": 0.5125330343557299,
      "grad_norm": 3.0093252658843994,
      "learning_rate": 4.145778276073783e-05,
      "loss": 0.5828,
      "step": 32000
    },
    {
      "epoch": 0.5205413630175383,
      "grad_norm": 3.962170362472534,
      "learning_rate": 4.1324310616374364e-05,
      "loss": 0.5496,
      "step": 32500
    },
    {
      "epoch": 0.5285496916793465,
      "grad_norm": 3.651639223098755,
      "learning_rate": 4.1190838472010895e-05,
      "loss": 0.5586,
      "step": 33000
    },
    {
      "epoch": 0.5365580203411549,
      "grad_norm": 4.153874397277832,
      "learning_rate": 4.1057366327647426e-05,
      "loss": 0.5602,
      "step": 33500
    },
    {
      "epoch": 0.5445663490029631,
      "grad_norm": 4.265336990356445,
      "learning_rate": 4.092389418328395e-05,
      "loss": 0.5628,
      "step": 34000
    },
    {
      "epoch": 0.5525746776647713,
      "grad_norm": 7.398928642272949,
      "learning_rate": 4.079042203892048e-05,
      "loss": 0.5575,
      "step": 34500
    },
    {
      "epoch": 0.5605830063265796,
      "grad_norm": 2.9150454998016357,
      "learning_rate": 4.0656949894557006e-05,
      "loss": 0.5655,
      "step": 35000
    },
    {
      "epoch": 0.5685913349883879,
      "grad_norm": 10.627225875854492,
      "learning_rate": 4.052347775019354e-05,
      "loss": 0.5556,
      "step": 35500
    },
    {
      "epoch": 0.5765996636501962,
      "grad_norm": 5.627581596374512,
      "learning_rate": 4.039000560583007e-05,
      "loss": 0.5658,
      "step": 36000
    },
    {
      "epoch": 0.5846079923120044,
      "grad_norm": 4.279469013214111,
      "learning_rate": 4.025653346146659e-05,
      "loss": 0.5489,
      "step": 36500
    },
    {
      "epoch": 0.5926163209738128,
      "grad_norm": 2.1071250438690186,
      "learning_rate": 4.0123061317103124e-05,
      "loss": 0.5657,
      "step": 37000
    },
    {
      "epoch": 0.600624649635621,
      "grad_norm": 4.208123207092285,
      "learning_rate": 3.998958917273965e-05,
      "loss": 0.5634,
      "step": 37500
    },
    {
      "epoch": 0.6086329782974294,
      "grad_norm": 6.990226745605469,
      "learning_rate": 3.985611702837618e-05,
      "loss": 0.5506,
      "step": 38000
    },
    {
      "epoch": 0.6166413069592376,
      "grad_norm": 3.9392850399017334,
      "learning_rate": 3.972264488401271e-05,
      "loss": 0.5653,
      "step": 38500
    },
    {
      "epoch": 0.6246496356210459,
      "grad_norm": 4.577337265014648,
      "learning_rate": 3.958917273964924e-05,
      "loss": 0.5529,
      "step": 39000
    },
    {
      "epoch": 0.6326579642828541,
      "grad_norm": 4.566893577575684,
      "learning_rate": 3.9455700595285766e-05,
      "loss": 0.5548,
      "step": 39500
    },
    {
      "epoch": 0.6406662929446625,
      "grad_norm": 5.411877632141113,
      "learning_rate": 3.93222284509223e-05,
      "loss": 0.5537,
      "step": 40000
    },
    {
      "epoch": 0.6486746216064707,
      "grad_norm": 4.104384422302246,
      "learning_rate": 3.918875630655882e-05,
      "loss": 0.5651,
      "step": 40500
    },
    {
      "epoch": 0.656682950268279,
      "grad_norm": 2.938718557357788,
      "learning_rate": 3.905528416219535e-05,
      "loss": 0.566,
      "step": 41000
    },
    {
      "epoch": 0.6646912789300873,
      "grad_norm": 3.550554037094116,
      "learning_rate": 3.8921812017831883e-05,
      "loss": 0.5546,
      "step": 41500
    },
    {
      "epoch": 0.6726996075918956,
      "grad_norm": 1.508626103401184,
      "learning_rate": 3.878833987346841e-05,
      "loss": 0.5534,
      "step": 42000
    },
    {
      "epoch": 0.6807079362537038,
      "grad_norm": 2.913686752319336,
      "learning_rate": 3.865486772910494e-05,
      "loss": 0.5476,
      "step": 42500
    },
    {
      "epoch": 0.6887162649155121,
      "grad_norm": 3.9451050758361816,
      "learning_rate": 3.852139558474146e-05,
      "loss": 0.5494,
      "step": 43000
    },
    {
      "epoch": 0.6967245935773204,
      "grad_norm": 3.3587934970855713,
      "learning_rate": 3.8387923440377994e-05,
      "loss": 0.5487,
      "step": 43500
    },
    {
      "epoch": 0.7047329222391286,
      "grad_norm": 1.7476997375488281,
      "learning_rate": 3.8254451296014526e-05,
      "loss": 0.537,
      "step": 44000
    },
    {
      "epoch": 0.712741250900937,
      "grad_norm": 4.126948833465576,
      "learning_rate": 3.812097915165106e-05,
      "loss": 0.5448,
      "step": 44500
    },
    {
      "epoch": 0.7207495795627452,
      "grad_norm": 4.941864967346191,
      "learning_rate": 3.798750700728758e-05,
      "loss": 0.567,
      "step": 45000
    },
    {
      "epoch": 0.7287579082245536,
      "grad_norm": 3.1921803951263428,
      "learning_rate": 3.785403486292411e-05,
      "loss": 0.5621,
      "step": 45500
    },
    {
      "epoch": 0.7367662368863618,
      "grad_norm": 3.7173779010772705,
      "learning_rate": 3.7720562718560637e-05,
      "loss": 0.5479,
      "step": 46000
    },
    {
      "epoch": 0.7447745655481701,
      "grad_norm": 2.963423252105713,
      "learning_rate": 3.758709057419717e-05,
      "loss": 0.5611,
      "step": 46500
    },
    {
      "epoch": 0.7527828942099783,
      "grad_norm": 5.177140235900879,
      "learning_rate": 3.74536184298337e-05,
      "loss": 0.5445,
      "step": 47000
    },
    {
      "epoch": 0.7607912228717867,
      "grad_norm": 3.394289255142212,
      "learning_rate": 3.732014628547022e-05,
      "loss": 0.5473,
      "step": 47500
    },
    {
      "epoch": 0.7687995515335949,
      "grad_norm": 10.127290725708008,
      "learning_rate": 3.7186674141106754e-05,
      "loss": 0.5494,
      "step": 48000
    },
    {
      "epoch": 0.7768078801954033,
      "grad_norm": 5.279999256134033,
      "learning_rate": 3.705320199674328e-05,
      "loss": 0.5584,
      "step": 48500
    },
    {
      "epoch": 0.7848162088572115,
      "grad_norm": 13.918063163757324,
      "learning_rate": 3.691972985237981e-05,
      "loss": 0.5715,
      "step": 49000
    },
    {
      "epoch": 0.7928245375190198,
      "grad_norm": 5.973888397216797,
      "learning_rate": 3.6786257708016334e-05,
      "loss": 0.5735,
      "step": 49500
    },
    {
      "epoch": 0.800832866180828,
      "grad_norm": 17.23167610168457,
      "learning_rate": 3.665278556365287e-05,
      "loss": 0.5735,
      "step": 50000
    },
    {
      "epoch": 0.8088411948426364,
      "grad_norm": 3.0458498001098633,
      "learning_rate": 3.6519313419289396e-05,
      "loss": 0.6101,
      "step": 50500
    },
    {
      "epoch": 0.8168495235044446,
      "grad_norm": 5.47400426864624,
      "learning_rate": 3.638584127492593e-05,
      "loss": 0.569,
      "step": 51000
    },
    {
      "epoch": 0.824857852166253,
      "grad_norm": 3.2479183673858643,
      "learning_rate": 3.625236913056245e-05,
      "loss": 0.5747,
      "step": 51500
    },
    {
      "epoch": 0.8328661808280612,
      "grad_norm": 8.307097434997559,
      "learning_rate": 3.611889698619898e-05,
      "loss": 0.5658,
      "step": 52000
    },
    {
      "epoch": 0.8408745094898694,
      "grad_norm": 3.029916763305664,
      "learning_rate": 3.5985424841835514e-05,
      "loss": 0.5746,
      "step": 52500
    },
    {
      "epoch": 0.8488828381516778,
      "grad_norm": 3.647195339202881,
      "learning_rate": 3.585195269747204e-05,
      "loss": 0.6002,
      "step": 53000
    },
    {
      "epoch": 0.856891166813486,
      "grad_norm": 2.2078123092651367,
      "learning_rate": 3.571848055310857e-05,
      "loss": 0.5973,
      "step": 53500
    },
    {
      "epoch": 0.8648994954752943,
      "grad_norm": 4.585103511810303,
      "learning_rate": 3.5585008408745094e-05,
      "loss": 0.6071,
      "step": 54000
    },
    {
      "epoch": 0.8729078241371026,
      "grad_norm": 30.27272605895996,
      "learning_rate": 3.5451536264381625e-05,
      "loss": 0.5817,
      "step": 54500
    },
    {
      "epoch": 0.8809161527989109,
      "grad_norm": 3.619690418243408,
      "learning_rate": 3.531806412001815e-05,
      "loss": 0.6166,
      "step": 55000
    },
    {
      "epoch": 0.8889244814607191,
      "grad_norm": 4.975050449371338,
      "learning_rate": 3.518459197565469e-05,
      "loss": 0.618,
      "step": 55500
    },
    {
      "epoch": 0.8969328101225275,
      "grad_norm": 2.7916760444641113,
      "learning_rate": 3.505111983129121e-05,
      "loss": 0.5827,
      "step": 56000
    },
    {
      "epoch": 0.9049411387843357,
      "grad_norm": 2.5821750164031982,
      "learning_rate": 3.491764768692774e-05,
      "loss": 0.5558,
      "step": 56500
    },
    {
      "epoch": 0.912949467446144,
      "grad_norm": 11.432326316833496,
      "learning_rate": 3.478417554256427e-05,
      "loss": 0.5964,
      "step": 57000
    },
    {
      "epoch": 0.9209577961079523,
      "grad_norm": 7.981192111968994,
      "learning_rate": 3.46507033982008e-05,
      "loss": 0.5874,
      "step": 57500
    },
    {
      "epoch": 0.9289661247697606,
      "grad_norm": 8.353496551513672,
      "learning_rate": 3.451723125383732e-05,
      "loss": 0.6429,
      "step": 58000
    },
    {
      "epoch": 0.9369744534315688,
      "grad_norm": 3.98939847946167,
      "learning_rate": 3.4383759109473854e-05,
      "loss": 0.6455,
      "step": 58500
    },
    {
      "epoch": 0.9449827820933772,
      "grad_norm": 8.27649974822998,
      "learning_rate": 3.4250286965110385e-05,
      "loss": 0.5744,
      "step": 59000
    },
    {
      "epoch": 0.9529911107551854,
      "grad_norm": 3.5062062740325928,
      "learning_rate": 3.411681482074691e-05,
      "loss": 0.5702,
      "step": 59500
    },
    {
      "epoch": 0.9609994394169937,
      "grad_norm": 15.206467628479004,
      "learning_rate": 3.398334267638344e-05,
      "loss": 0.5758,
      "step": 60000
    },
    {
      "epoch": 0.969007768078802,
      "grad_norm": 17.04636573791504,
      "learning_rate": 3.3849870532019965e-05,
      "loss": 0.5856,
      "step": 60500
    },
    {
      "epoch": 0.9770160967406102,
      "grad_norm": 6.201862812042236,
      "learning_rate": 3.37163983876565e-05,
      "loss": 0.6112,
      "step": 61000
    },
    {
      "epoch": 0.9850244254024185,
      "grad_norm": 5.430823802947998,
      "learning_rate": 3.358292624329303e-05,
      "loss": 0.6022,
      "step": 61500
    },
    {
      "epoch": 0.9930327540642268,
      "grad_norm": 12.381128311157227,
      "learning_rate": 3.344945409892956e-05,
      "loss": 0.5864,
      "step": 62000
    },
    {
      "epoch": 1.001041082726035,
      "grad_norm": 4.036890029907227,
      "learning_rate": 3.331598195456608e-05,
      "loss": 0.5762,
      "step": 62500
    },
    {
      "epoch": 1.0090494113878434,
      "grad_norm": 3.122976541519165,
      "learning_rate": 3.3182509810202614e-05,
      "loss": 0.6011,
      "step": 63000
    },
    {
      "epoch": 1.0170577400496517,
      "grad_norm": 2.9697132110595703,
      "learning_rate": 3.304903766583914e-05,
      "loss": 0.5758,
      "step": 63500
    },
    {
      "epoch": 1.0250660687114599,
      "grad_norm": 7.880277156829834,
      "learning_rate": 3.291556552147567e-05,
      "loss": 0.5538,
      "step": 64000
    },
    {
      "epoch": 1.0330743973732681,
      "grad_norm": 10.287847518920898,
      "learning_rate": 3.27820933771122e-05,
      "loss": 0.5684,
      "step": 64500
    },
    {
      "epoch": 1.0410827260350766,
      "grad_norm": 29.196542739868164,
      "learning_rate": 3.2648621232748725e-05,
      "loss": 0.5852,
      "step": 65000
    },
    {
      "epoch": 1.0490910546968848,
      "grad_norm": 13.388043403625488,
      "learning_rate": 3.2515149088385256e-05,
      "loss": 0.5929,
      "step": 65500
    },
    {
      "epoch": 1.057099383358693,
      "grad_norm": 2.2638657093048096,
      "learning_rate": 3.238167694402178e-05,
      "loss": 0.5985,
      "step": 66000
    },
    {
      "epoch": 1.0651077120205013,
      "grad_norm": 46.76168441772461,
      "learning_rate": 3.224820479965831e-05,
      "loss": 0.5722,
      "step": 66500
    },
    {
      "epoch": 1.0731160406823097,
      "grad_norm": 10.283295631408691,
      "learning_rate": 3.211473265529484e-05,
      "loss": 0.5599,
      "step": 67000
    },
    {
      "epoch": 1.081124369344118,
      "grad_norm": 4.871533393859863,
      "learning_rate": 3.1981260510931374e-05,
      "loss": 0.5426,
      "step": 67500
    },
    {
      "epoch": 1.0891326980059262,
      "grad_norm": 4.29221248626709,
      "learning_rate": 3.18477883665679e-05,
      "loss": 0.5692,
      "step": 68000
    },
    {
      "epoch": 1.0971410266677344,
      "grad_norm": 21.451339721679688,
      "learning_rate": 3.171431622220443e-05,
      "loss": 0.5576,
      "step": 68500
    },
    {
      "epoch": 1.1051493553295426,
      "grad_norm": 5.6028666496276855,
      "learning_rate": 3.1580844077840954e-05,
      "loss": 0.5685,
      "step": 69000
    },
    {
      "epoch": 1.113157683991351,
      "grad_norm": 4.469378471374512,
      "learning_rate": 3.1447371933477485e-05,
      "loss": 0.5372,
      "step": 69500
    },
    {
      "epoch": 1.1211660126531593,
      "grad_norm": 6.250676155090332,
      "learning_rate": 3.1313899789114016e-05,
      "loss": 0.528,
      "step": 70000
    },
    {
      "epoch": 1.1291743413149675,
      "grad_norm": 2.7597177028656006,
      "learning_rate": 3.118042764475054e-05,
      "loss": 0.525,
      "step": 70500
    },
    {
      "epoch": 1.1371826699767758,
      "grad_norm": 3.4835798740386963,
      "learning_rate": 3.104695550038707e-05,
      "loss": 0.5368,
      "step": 71000
    },
    {
      "epoch": 1.1451909986385842,
      "grad_norm": 4.725292205810547,
      "learning_rate": 3.0913483356023596e-05,
      "loss": 0.53,
      "step": 71500
    },
    {
      "epoch": 1.1531993273003924,
      "grad_norm": 4.831820487976074,
      "learning_rate": 3.078001121166013e-05,
      "loss": 0.5339,
      "step": 72000
    },
    {
      "epoch": 1.1612076559622007,
      "grad_norm": 3.886702537536621,
      "learning_rate": 3.064653906729666e-05,
      "loss": 0.5383,
      "step": 72500
    },
    {
      "epoch": 1.1692159846240089,
      "grad_norm": 2.8685832023620605,
      "learning_rate": 3.0513066922933186e-05,
      "loss": 0.5331,
      "step": 73000
    },
    {
      "epoch": 1.1772243132858173,
      "grad_norm": 13.397988319396973,
      "learning_rate": 3.0379594778569713e-05,
      "loss": 0.5253,
      "step": 73500
    },
    {
      "epoch": 1.1852326419476256,
      "grad_norm": 6.273265361785889,
      "learning_rate": 3.024612263420624e-05,
      "loss": 0.5244,
      "step": 74000
    },
    {
      "epoch": 1.1932409706094338,
      "grad_norm": 3.430068016052246,
      "learning_rate": 3.011265048984277e-05,
      "loss": 0.5544,
      "step": 74500
    },
    {
      "epoch": 1.201249299271242,
      "grad_norm": 160.6565704345703,
      "learning_rate": 2.9979178345479297e-05,
      "loss": 0.5334,
      "step": 75000
    },
    {
      "epoch": 1.2092576279330505,
      "grad_norm": 4.157841205596924,
      "learning_rate": 2.984570620111583e-05,
      "loss": 0.5325,
      "step": 75500
    },
    {
      "epoch": 1.2172659565948587,
      "grad_norm": 23.81913185119629,
      "learning_rate": 2.971223405675236e-05,
      "loss": 0.5385,
      "step": 76000
    },
    {
      "epoch": 1.225274285256667,
      "grad_norm": 6.362563133239746,
      "learning_rate": 2.9578761912388887e-05,
      "loss": 0.5406,
      "step": 76500
    },
    {
      "epoch": 1.2332826139184752,
      "grad_norm": 2.9250121116638184,
      "learning_rate": 2.9445289768025414e-05,
      "loss": 0.5285,
      "step": 77000
    },
    {
      "epoch": 1.2412909425802834,
      "grad_norm": 12.129765510559082,
      "learning_rate": 2.9311817623661942e-05,
      "loss": 0.5336,
      "step": 77500
    },
    {
      "epoch": 1.2492992712420918,
      "grad_norm": 4.581326007843018,
      "learning_rate": 2.9178345479298473e-05,
      "loss": 0.5289,
      "step": 78000
    },
    {
      "epoch": 1.2573075999039,
      "grad_norm": 10.895811080932617,
      "learning_rate": 2.9044873334935e-05,
      "loss": 0.544,
      "step": 78500
    },
    {
      "epoch": 1.2653159285657083,
      "grad_norm": 4.070727825164795,
      "learning_rate": 2.891140119057153e-05,
      "loss": 0.5162,
      "step": 79000
    },
    {
      "epoch": 1.2733242572275167,
      "grad_norm": 4.06965970993042,
      "learning_rate": 2.8777929046208057e-05,
      "loss": 0.5294,
      "step": 79500
    },
    {
      "epoch": 1.281332585889325,
      "grad_norm": 4.579061985015869,
      "learning_rate": 2.8644456901844584e-05,
      "loss": 0.52,
      "step": 80000
    },
    {
      "epoch": 1.2893409145511332,
      "grad_norm": 6.460152626037598,
      "learning_rate": 2.8510984757481112e-05,
      "loss": 0.5258,
      "step": 80500
    },
    {
      "epoch": 1.2973492432129414,
      "grad_norm": 7.23961877822876,
      "learning_rate": 2.8377512613117647e-05,
      "loss": 0.5414,
      "step": 81000
    },
    {
      "epoch": 1.3053575718747497,
      "grad_norm": 2.1204023361206055,
      "learning_rate": 2.8244040468754174e-05,
      "loss": 0.5283,
      "step": 81500
    },
    {
      "epoch": 1.3133659005365579,
      "grad_norm": 5.8042073249816895,
      "learning_rate": 2.8110568324390702e-05,
      "loss": 0.5444,
      "step": 82000
    },
    {
      "epoch": 1.3213742291983663,
      "grad_norm": 3.8973348140716553,
      "learning_rate": 2.797709618002723e-05,
      "loss": 0.5247,
      "step": 82500
    },
    {
      "epoch": 1.3293825578601746,
      "grad_norm": 2.659574508666992,
      "learning_rate": 2.7843624035663758e-05,
      "loss": 0.5288,
      "step": 83000
    },
    {
      "epoch": 1.3373908865219828,
      "grad_norm": 3.706261157989502,
      "learning_rate": 2.7710151891300285e-05,
      "loss": 0.532,
      "step": 83500
    },
    {
      "epoch": 1.3453992151837912,
      "grad_norm": 20.325761795043945,
      "learning_rate": 2.7576679746936817e-05,
      "loss": 0.5168,
      "step": 84000
    },
    {
      "epoch": 1.3534075438455995,
      "grad_norm": 2.9860475063323975,
      "learning_rate": 2.7443207602573344e-05,
      "loss": 0.5249,
      "step": 84500
    },
    {
      "epoch": 1.3614158725074077,
      "grad_norm": 5.427518367767334,
      "learning_rate": 2.7309735458209872e-05,
      "loss": 0.5167,
      "step": 85000
    },
    {
      "epoch": 1.369424201169216,
      "grad_norm": 6.439121246337891,
      "learning_rate": 2.71762633138464e-05,
      "loss": 0.5392,
      "step": 85500
    },
    {
      "epoch": 1.3774325298310242,
      "grad_norm": 8.334936141967773,
      "learning_rate": 2.7042791169482927e-05,
      "loss": 0.5157,
      "step": 86000
    },
    {
      "epoch": 1.3854408584928326,
      "grad_norm": 4.830845355987549,
      "learning_rate": 2.6909319025119462e-05,
      "loss": 0.5368,
      "step": 86500
    },
    {
      "epoch": 1.3934491871546408,
      "grad_norm": 2.1246495246887207,
      "learning_rate": 2.677584688075599e-05,
      "loss": 0.5088,
      "step": 87000
    },
    {
      "epoch": 1.401457515816449,
      "grad_norm": 2.3075292110443115,
      "learning_rate": 2.6642374736392518e-05,
      "loss": 0.5368,
      "step": 87500
    },
    {
      "epoch": 1.4094658444782575,
      "grad_norm": 2.213160514831543,
      "learning_rate": 2.6508902592029045e-05,
      "loss": 0.5422,
      "step": 88000
    },
    {
      "epoch": 1.4174741731400657,
      "grad_norm": 12.27783489227295,
      "learning_rate": 2.6375430447665573e-05,
      "loss": 0.5403,
      "step": 88500
    },
    {
      "epoch": 1.425482501801874,
      "grad_norm": 4.510140419006348,
      "learning_rate": 2.62419583033021e-05,
      "loss": 0.5352,
      "step": 89000
    },
    {
      "epoch": 1.4334908304636822,
      "grad_norm": 4.781023025512695,
      "learning_rate": 2.6108486158938632e-05,
      "loss": 0.5149,
      "step": 89500
    },
    {
      "epoch": 1.4414991591254904,
      "grad_norm": 6.79545783996582,
      "learning_rate": 2.597501401457516e-05,
      "loss": 0.5373,
      "step": 90000
    },
    {
      "epoch": 1.4495074877872987,
      "grad_norm": 49.147579193115234,
      "learning_rate": 2.5841541870211687e-05,
      "loss": 0.5313,
      "step": 90500
    },
    {
      "epoch": 1.457515816449107,
      "grad_norm": 2.4180684089660645,
      "learning_rate": 2.5708069725848215e-05,
      "loss": 0.5341,
      "step": 91000
    },
    {
      "epoch": 1.4655241451109153,
      "grad_norm": 4.6710968017578125,
      "learning_rate": 2.5574597581484743e-05,
      "loss": 0.5254,
      "step": 91500
    },
    {
      "epoch": 1.4735324737727236,
      "grad_norm": 13.610833168029785,
      "learning_rate": 2.544112543712127e-05,
      "loss": 0.5334,
      "step": 92000
    },
    {
      "epoch": 1.481540802434532,
      "grad_norm": 3.6180546283721924,
      "learning_rate": 2.5307653292757805e-05,
      "loss": 0.5404,
      "step": 92500
    },
    {
      "epoch": 1.4895491310963402,
      "grad_norm": 9.872357368469238,
      "learning_rate": 2.5174181148394333e-05,
      "loss": 0.5243,
      "step": 93000
    },
    {
      "epoch": 1.4975574597581485,
      "grad_norm": 11.83433723449707,
      "learning_rate": 2.504070900403086e-05,
      "loss": 0.5212,
      "step": 93500
    },
    {
      "epoch": 1.5055657884199567,
      "grad_norm": 5.849399566650391,
      "learning_rate": 2.490723685966739e-05,
      "loss": 0.536,
      "step": 94000
    },
    {
      "epoch": 1.513574117081765,
      "grad_norm": 6.792448043823242,
      "learning_rate": 2.477376471530392e-05,
      "loss": 0.5139,
      "step": 94500
    },
    {
      "epoch": 1.5215824457435732,
      "grad_norm": 10.482234954833984,
      "learning_rate": 2.4640292570940447e-05,
      "loss": 0.514,
      "step": 95000
    },
    {
      "epoch": 1.5295907744053816,
      "grad_norm": 3.7406227588653564,
      "learning_rate": 2.4506820426576975e-05,
      "loss": 0.5278,
      "step": 95500
    },
    {
      "epoch": 1.5375991030671898,
      "grad_norm": 5.730695724487305,
      "learning_rate": 2.4373348282213503e-05,
      "loss": 0.5093,
      "step": 96000
    },
    {
      "epoch": 1.5456074317289983,
      "grad_norm": 3.3730945587158203,
      "learning_rate": 2.423987613785003e-05,
      "loss": 0.5235,
      "step": 96500
    },
    {
      "epoch": 1.5536157603908065,
      "grad_norm": 3.492950916290283,
      "learning_rate": 2.4106403993486558e-05,
      "loss": 0.5175,
      "step": 97000
    },
    {
      "epoch": 1.5616240890526147,
      "grad_norm": 6.554967880249023,
      "learning_rate": 2.397293184912309e-05,
      "loss": 0.5368,
      "step": 97500
    },
    {
      "epoch": 1.569632417714423,
      "grad_norm": 2.112107515335083,
      "learning_rate": 2.3839459704759617e-05,
      "loss": 0.5272,
      "step": 98000
    },
    {
      "epoch": 1.5776407463762312,
      "grad_norm": 6.589860439300537,
      "learning_rate": 2.3705987560396145e-05,
      "loss": 0.5219,
      "step": 98500
    },
    {
      "epoch": 1.5856490750380394,
      "grad_norm": 9.147485733032227,
      "learning_rate": 2.3572515416032676e-05,
      "loss": 0.5768,
      "step": 99000
    },
    {
      "epoch": 1.5936574036998479,
      "grad_norm": 2.691847801208496,
      "learning_rate": 2.3439043271669204e-05,
      "loss": 0.5257,
      "step": 99500
    },
    {
      "epoch": 1.601665732361656,
      "grad_norm": 2.4653239250183105,
      "learning_rate": 2.3305571127305735e-05,
      "loss": 0.522,
      "step": 100000
    },
    {
      "epoch": 1.6096740610234646,
      "grad_norm": 11.085479736328125,
      "learning_rate": 2.3172098982942263e-05,
      "loss": 0.5168,
      "step": 100500
    },
    {
      "epoch": 1.6176823896852728,
      "grad_norm": 10.273331642150879,
      "learning_rate": 2.303862683857879e-05,
      "loss": 0.5353,
      "step": 101000
    },
    {
      "epoch": 1.625690718347081,
      "grad_norm": 2.038649559020996,
      "learning_rate": 2.2905154694215318e-05,
      "loss": 0.5202,
      "step": 101500
    },
    {
      "epoch": 1.6336990470088892,
      "grad_norm": 4.0400004386901855,
      "learning_rate": 2.2771682549851846e-05,
      "loss": 0.5176,
      "step": 102000
    },
    {
      "epoch": 1.6417073756706975,
      "grad_norm": 2.4204163551330566,
      "learning_rate": 2.2638210405488374e-05,
      "loss": 0.5092,
      "step": 102500
    },
    {
      "epoch": 1.6497157043325057,
      "grad_norm": 3.1426825523376465,
      "learning_rate": 2.2504738261124905e-05,
      "loss": 0.5198,
      "step": 103000
    },
    {
      "epoch": 1.657724032994314,
      "grad_norm": 8.044781684875488,
      "learning_rate": 2.2371266116761433e-05,
      "loss": 0.5194,
      "step": 103500
    },
    {
      "epoch": 1.6657323616561224,
      "grad_norm": 3.9673469066619873,
      "learning_rate": 2.223779397239796e-05,
      "loss": 0.5217,
      "step": 104000
    },
    {
      "epoch": 1.6737406903179306,
      "grad_norm": 11.519086837768555,
      "learning_rate": 2.210432182803449e-05,
      "loss": 0.5095,
      "step": 104500
    },
    {
      "epoch": 1.681749018979739,
      "grad_norm": 5.464014530181885,
      "learning_rate": 2.197084968367102e-05,
      "loss": 0.5355,
      "step": 105000
    },
    {
      "epoch": 1.6897573476415473,
      "grad_norm": 4.318820476531982,
      "learning_rate": 2.1837377539307547e-05,
      "loss": 0.5205,
      "step": 105500
    },
    {
      "epoch": 1.6977656763033555,
      "grad_norm": 6.407961845397949,
      "learning_rate": 2.1703905394944078e-05,
      "loss": 0.5122,
      "step": 106000
    },
    {
      "epoch": 1.7057740049651637,
      "grad_norm": 8.025530815124512,
      "learning_rate": 2.1570433250580606e-05,
      "loss": 0.5217,
      "step": 106500
    },
    {
      "epoch": 1.713782333626972,
      "grad_norm": 4.854086399078369,
      "learning_rate": 2.1436961106217134e-05,
      "loss": 0.5269,
      "step": 107000
    },
    {
      "epoch": 1.7217906622887802,
      "grad_norm": 4.895215034484863,
      "learning_rate": 2.130348896185366e-05,
      "loss": 0.5468,
      "step": 107500
    },
    {
      "epoch": 1.7297989909505886,
      "grad_norm": 14.803995132446289,
      "learning_rate": 2.117001681749019e-05,
      "loss": 0.5172,
      "step": 108000
    },
    {
      "epoch": 1.7378073196123969,
      "grad_norm": 4.8039021492004395,
      "learning_rate": 2.103654467312672e-05,
      "loss": 0.5071,
      "step": 108500
    },
    {
      "epoch": 1.7458156482742053,
      "grad_norm": 6.83394718170166,
      "learning_rate": 2.0903072528763248e-05,
      "loss": 0.5062,
      "step": 109000
    },
    {
      "epoch": 1.7538239769360136,
      "grad_norm": 3.1579933166503906,
      "learning_rate": 2.0769600384399776e-05,
      "loss": 0.5295,
      "step": 109500
    },
    {
      "epoch": 1.7618323055978218,
      "grad_norm": 5.3173370361328125,
      "learning_rate": 2.0636128240036307e-05,
      "loss": 0.5125,
      "step": 110000
    },
    {
      "epoch": 1.76984063425963,
      "grad_norm": 3.3504600524902344,
      "learning_rate": 2.0502656095672835e-05,
      "loss": 0.5289,
      "step": 110500
    },
    {
      "epoch": 1.7778489629214382,
      "grad_norm": 1.5318541526794434,
      "learning_rate": 2.0369183951309362e-05,
      "loss": 0.519,
      "step": 111000
    },
    {
      "epoch": 1.7858572915832465,
      "grad_norm": 6.516898155212402,
      "learning_rate": 2.0235711806945893e-05,
      "loss": 0.5218,
      "step": 111500
    },
    {
      "epoch": 1.7938656202450547,
      "grad_norm": 5.115590572357178,
      "learning_rate": 2.010223966258242e-05,
      "loss": 0.5187,
      "step": 112000
    },
    {
      "epoch": 1.8018739489068631,
      "grad_norm": 1.8095836639404297,
      "learning_rate": 1.996876751821895e-05,
      "loss": 0.5086,
      "step": 112500
    },
    {
      "epoch": 1.8098822775686714,
      "grad_norm": 4.258097171783447,
      "learning_rate": 1.9835295373855477e-05,
      "loss": 0.5159,
      "step": 113000
    },
    {
      "epoch": 1.8178906062304798,
      "grad_norm": 3.70536732673645,
      "learning_rate": 1.9701823229492004e-05,
      "loss": 0.5177,
      "step": 113500
    },
    {
      "epoch": 1.825898934892288,
      "grad_norm": 4.52922248840332,
      "learning_rate": 1.9568351085128532e-05,
      "loss": 0.5163,
      "step": 114000
    },
    {
      "epoch": 1.8339072635540963,
      "grad_norm": 4.456188678741455,
      "learning_rate": 1.9434878940765063e-05,
      "loss": 0.5166,
      "step": 114500
    },
    {
      "epoch": 1.8419155922159045,
      "grad_norm": 3.5256004333496094,
      "learning_rate": 1.930140679640159e-05,
      "loss": 0.5269,
      "step": 115000
    },
    {
      "epoch": 1.8499239208777127,
      "grad_norm": 3.6694319248199463,
      "learning_rate": 1.916793465203812e-05,
      "loss": 0.5113,
      "step": 115500
    },
    {
      "epoch": 1.857932249539521,
      "grad_norm": 4.273138046264648,
      "learning_rate": 1.903446250767465e-05,
      "loss": 0.5139,
      "step": 116000
    },
    {
      "epoch": 1.8659405782013294,
      "grad_norm": 13.877301216125488,
      "learning_rate": 1.8900990363311178e-05,
      "loss": 0.5241,
      "step": 116500
    },
    {
      "epoch": 1.8739489068631376,
      "grad_norm": 3.783949851989746,
      "learning_rate": 1.876751821894771e-05,
      "loss": 0.5213,
      "step": 117000
    },
    {
      "epoch": 1.881957235524946,
      "grad_norm": 4.728069305419922,
      "learning_rate": 1.8634046074584237e-05,
      "loss": 0.527,
      "step": 117500
    },
    {
      "epoch": 1.8899655641867543,
      "grad_norm": 6.048025131225586,
      "learning_rate": 1.8500573930220764e-05,
      "loss": 0.5311,
      "step": 118000
    },
    {
      "epoch": 1.8979738928485625,
      "grad_norm": 6.48728609085083,
      "learning_rate": 1.8367101785857292e-05,
      "loss": 0.5388,
      "step": 118500
    },
    {
      "epoch": 1.9059822215103708,
      "grad_norm": 4.1666388511657715,
      "learning_rate": 1.823362964149382e-05,
      "loss": 0.5276,
      "step": 119000
    },
    {
      "epoch": 1.913990550172179,
      "grad_norm": 5.920313835144043,
      "learning_rate": 1.8100157497130348e-05,
      "loss": 0.5154,
      "step": 119500
    },
    {
      "epoch": 1.9219988788339872,
      "grad_norm": 5.515158176422119,
      "learning_rate": 1.796668535276688e-05,
      "loss": 0.5377,
      "step": 120000
    },
    {
      "epoch": 1.9300072074957955,
      "grad_norm": 4.867806911468506,
      "learning_rate": 1.7833213208403406e-05,
      "loss": 0.5142,
      "step": 120500
    },
    {
      "epoch": 1.938015536157604,
      "grad_norm": 5.740201473236084,
      "learning_rate": 1.7699741064039934e-05,
      "loss": 0.5121,
      "step": 121000
    },
    {
      "epoch": 1.9460238648194121,
      "grad_norm": 6.0920305252075195,
      "learning_rate": 1.7566268919676465e-05,
      "loss": 0.5073,
      "step": 121500
    },
    {
      "epoch": 1.9540321934812206,
      "grad_norm": 3.8682684898376465,
      "learning_rate": 1.7432796775312993e-05,
      "loss": 0.5235,
      "step": 122000
    },
    {
      "epoch": 1.9620405221430288,
      "grad_norm": 11.944962501525879,
      "learning_rate": 1.729932463094952e-05,
      "loss": 0.5218,
      "step": 122500
    },
    {
      "epoch": 1.970048850804837,
      "grad_norm": 4.42965841293335,
      "learning_rate": 1.7165852486586052e-05,
      "loss": 0.5051,
      "step": 123000
    },
    {
      "epoch": 1.9780571794666453,
      "grad_norm": 2.3707287311553955,
      "learning_rate": 1.703238034222258e-05,
      "loss": 0.51,
      "step": 123500
    },
    {
      "epoch": 1.9860655081284535,
      "grad_norm": 3.7566802501678467,
      "learning_rate": 1.6898908197859107e-05,
      "loss": 0.5182,
      "step": 124000
    },
    {
      "epoch": 1.9940738367902617,
      "grad_norm": 3.688204765319824,
      "learning_rate": 1.6765436053495635e-05,
      "loss": 0.5216,
      "step": 124500
    },
    {
      "epoch": 2.00208216545207,
      "grad_norm": 4.6394548416137695,
      "learning_rate": 1.6631963909132163e-05,
      "loss": 0.52,
      "step": 125000
    },
    {
      "epoch": 2.0100904941138786,
      "grad_norm": 3.345848560333252,
      "learning_rate": 1.6498491764768694e-05,
      "loss": 0.491,
      "step": 125500
    },
    {
      "epoch": 2.018098822775687,
      "grad_norm": 6.613098621368408,
      "learning_rate": 1.6365019620405222e-05,
      "loss": 0.4849,
      "step": 126000
    },
    {
      "epoch": 2.026107151437495,
      "grad_norm": 5.686723232269287,
      "learning_rate": 1.623154747604175e-05,
      "loss": 0.4725,
      "step": 126500
    },
    {
      "epoch": 2.0341154800993033,
      "grad_norm": 4.618155479431152,
      "learning_rate": 1.609807533167828e-05,
      "loss": 0.4939,
      "step": 127000
    },
    {
      "epoch": 2.0421238087611115,
      "grad_norm": 7.990642547607422,
      "learning_rate": 1.596460318731481e-05,
      "loss": 0.4953,
      "step": 127500
    },
    {
      "epoch": 2.0501321374229198,
      "grad_norm": 6.9659929275512695,
      "learning_rate": 1.5831131042951336e-05,
      "loss": 0.4811,
      "step": 128000
    },
    {
      "epoch": 2.058140466084728,
      "grad_norm": 4.644782543182373,
      "learning_rate": 1.5697658898587867e-05,
      "loss": 0.4862,
      "step": 128500
    },
    {
      "epoch": 2.0661487947465362,
      "grad_norm": 8.351694107055664,
      "learning_rate": 1.5564186754224395e-05,
      "loss": 0.4901,
      "step": 129000
    },
    {
      "epoch": 2.0741571234083445,
      "grad_norm": 4.961209297180176,
      "learning_rate": 1.5430714609860923e-05,
      "loss": 0.4705,
      "step": 129500
    },
    {
      "epoch": 2.082165452070153,
      "grad_norm": 11.790966033935547,
      "learning_rate": 1.529724246549745e-05,
      "loss": 0.4914,
      "step": 130000
    },
    {
      "epoch": 2.0901737807319614,
      "grad_norm": 15.83209228515625,
      "learning_rate": 1.516377032113398e-05,
      "loss": 0.4955,
      "step": 130500
    },
    {
      "epoch": 2.0981821093937696,
      "grad_norm": 6.013333320617676,
      "learning_rate": 1.5030298176770508e-05,
      "loss": 0.4834,
      "step": 131000
    },
    {
      "epoch": 2.106190438055578,
      "grad_norm": 8.343938827514648,
      "learning_rate": 1.4896826032407037e-05,
      "loss": 0.4839,
      "step": 131500
    },
    {
      "epoch": 2.114198766717386,
      "grad_norm": 8.838165283203125,
      "learning_rate": 1.4763353888043565e-05,
      "loss": 0.4923,
      "step": 132000
    },
    {
      "epoch": 2.1222070953791943,
      "grad_norm": 3.5784082412719727,
      "learning_rate": 1.4629881743680093e-05,
      "loss": 0.4999,
      "step": 132500
    },
    {
      "epoch": 2.1302154240410025,
      "grad_norm": 6.789596080780029,
      "learning_rate": 1.4496409599316624e-05,
      "loss": 0.4979,
      "step": 133000
    },
    {
      "epoch": 2.1382237527028107,
      "grad_norm": 3.2773313522338867,
      "learning_rate": 1.4362937454953152e-05,
      "loss": 0.4843,
      "step": 133500
    },
    {
      "epoch": 2.1462320813646194,
      "grad_norm": 7.179351806640625,
      "learning_rate": 1.4229465310589681e-05,
      "loss": 0.4835,
      "step": 134000
    },
    {
      "epoch": 2.1542404100264276,
      "grad_norm": 6.873589992523193,
      "learning_rate": 1.4095993166226209e-05,
      "loss": 0.5038,
      "step": 134500
    },
    {
      "epoch": 2.162248738688236,
      "grad_norm": 5.110401153564453,
      "learning_rate": 1.3962521021862737e-05,
      "loss": 0.4966,
      "step": 135000
    },
    {
      "epoch": 2.170257067350044,
      "grad_norm": 3.0599632263183594,
      "learning_rate": 1.3829048877499268e-05,
      "loss": 0.4974,
      "step": 135500
    },
    {
      "epoch": 2.1782653960118523,
      "grad_norm": 5.403199672698975,
      "learning_rate": 1.3695576733135795e-05,
      "loss": 0.4883,
      "step": 136000
    },
    {
      "epoch": 2.1862737246736605,
      "grad_norm": 10.166191101074219,
      "learning_rate": 1.3562104588772323e-05,
      "loss": 0.4801,
      "step": 136500
    },
    {
      "epoch": 2.1942820533354688,
      "grad_norm": 3.9219131469726562,
      "learning_rate": 1.3428632444408853e-05,
      "loss": 0.4995,
      "step": 137000
    },
    {
      "epoch": 2.202290381997277,
      "grad_norm": 9.928403854370117,
      "learning_rate": 1.329516030004538e-05,
      "loss": 0.4904,
      "step": 137500
    },
    {
      "epoch": 2.2102987106590852,
      "grad_norm": 7.370477199554443,
      "learning_rate": 1.3161688155681908e-05,
      "loss": 0.4715,
      "step": 138000
    },
    {
      "epoch": 2.218307039320894,
      "grad_norm": 4.17348575592041,
      "learning_rate": 1.302821601131844e-05,
      "loss": 0.4946,
      "step": 138500
    },
    {
      "epoch": 2.226315367982702,
      "grad_norm": 4.908074378967285,
      "learning_rate": 1.2894743866954967e-05,
      "loss": 0.4942,
      "step": 139000
    },
    {
      "epoch": 2.2343236966445104,
      "grad_norm": 5.767423152923584,
      "learning_rate": 1.2761271722591495e-05,
      "loss": 0.5023,
      "step": 139500
    },
    {
      "epoch": 2.2423320253063186,
      "grad_norm": 6.429279327392578,
      "learning_rate": 1.2627799578228024e-05,
      "loss": 0.4977,
      "step": 140000
    },
    {
      "epoch": 2.250340353968127,
      "grad_norm": 3.7908496856689453,
      "learning_rate": 1.2494327433864552e-05,
      "loss": 0.49,
      "step": 140500
    },
    {
      "epoch": 2.258348682629935,
      "grad_norm": 3.7526662349700928,
      "learning_rate": 1.2360855289501081e-05,
      "loss": 0.4814,
      "step": 141000
    },
    {
      "epoch": 2.2663570112917433,
      "grad_norm": 5.195950984954834,
      "learning_rate": 1.2227383145137611e-05,
      "loss": 0.4966,
      "step": 141500
    },
    {
      "epoch": 2.2743653399535515,
      "grad_norm": 2.5419557094573975,
      "learning_rate": 1.2093911000774139e-05,
      "loss": 0.4843,
      "step": 142000
    },
    {
      "epoch": 2.28237366861536,
      "grad_norm": 4.773759365081787,
      "learning_rate": 1.1960438856410668e-05,
      "loss": 0.4959,
      "step": 142500
    },
    {
      "epoch": 2.2903819972771684,
      "grad_norm": 1.8961355686187744,
      "learning_rate": 1.1826966712047197e-05,
      "loss": 0.4822,
      "step": 143000
    },
    {
      "epoch": 2.2983903259389766,
      "grad_norm": 5.982407093048096,
      "learning_rate": 1.1693494567683724e-05,
      "loss": 0.468,
      "step": 143500
    },
    {
      "epoch": 2.306398654600785,
      "grad_norm": 9.01443862915039,
      "learning_rate": 1.1560022423320253e-05,
      "loss": 0.4931,
      "step": 144000
    },
    {
      "epoch": 2.314406983262593,
      "grad_norm": 5.718390941619873,
      "learning_rate": 1.1426550278956782e-05,
      "loss": 0.5046,
      "step": 144500
    },
    {
      "epoch": 2.3224153119244013,
      "grad_norm": 3.213411808013916,
      "learning_rate": 1.1293078134593312e-05,
      "loss": 0.502,
      "step": 145000
    },
    {
      "epoch": 2.3304236405862095,
      "grad_norm": 5.6064839363098145,
      "learning_rate": 1.115960599022984e-05,
      "loss": 0.4843,
      "step": 145500
    },
    {
      "epoch": 2.3384319692480178,
      "grad_norm": 12.452762603759766,
      "learning_rate": 1.1026133845866369e-05,
      "loss": 0.4709,
      "step": 146000
    },
    {
      "epoch": 2.346440297909826,
      "grad_norm": 7.003706455230713,
      "learning_rate": 1.0892661701502897e-05,
      "loss": 0.4828,
      "step": 146500
    },
    {
      "epoch": 2.3544486265716347,
      "grad_norm": 3.095332145690918,
      "learning_rate": 1.0759189557139425e-05,
      "loss": 0.4759,
      "step": 147000
    },
    {
      "epoch": 2.362456955233443,
      "grad_norm": 4.040911674499512,
      "learning_rate": 1.0625717412775954e-05,
      "loss": 0.4848,
      "step": 147500
    },
    {
      "epoch": 2.370465283895251,
      "grad_norm": 3.7055633068084717,
      "learning_rate": 1.0492245268412483e-05,
      "loss": 0.4879,
      "step": 148000
    },
    {
      "epoch": 2.3784736125570594,
      "grad_norm": 3.9870054721832275,
      "learning_rate": 1.0358773124049013e-05,
      "loss": 0.4896,
      "step": 148500
    },
    {
      "epoch": 2.3864819412188676,
      "grad_norm": 2.65230393409729,
      "learning_rate": 1.022530097968554e-05,
      "loss": 0.4927,
      "step": 149000
    },
    {
      "epoch": 2.394490269880676,
      "grad_norm": 8.814361572265625,
      "learning_rate": 1.0091828835322068e-05,
      "loss": 0.4972,
      "step": 149500
    },
    {
      "epoch": 2.402498598542484,
      "grad_norm": 7.068905830383301,
      "learning_rate": 9.958356690958598e-06,
      "loss": 0.4935,
      "step": 150000
    },
    {
      "epoch": 2.4105069272042923,
      "grad_norm": 6.750758171081543,
      "learning_rate": 9.824884546595126e-06,
      "loss": 0.4888,
      "step": 150500
    },
    {
      "epoch": 2.418515255866101,
      "grad_norm": 1.9101755619049072,
      "learning_rate": 9.691412402231655e-06,
      "loss": 0.4748,
      "step": 151000
    },
    {
      "epoch": 2.426523584527909,
      "grad_norm": 4.402417182922363,
      "learning_rate": 9.557940257868184e-06,
      "loss": 0.4644,
      "step": 151500
    },
    {
      "epoch": 2.4345319131897174,
      "grad_norm": 5.037499904632568,
      "learning_rate": 9.424468113504712e-06,
      "loss": 0.4858,
      "step": 152000
    },
    {
      "epoch": 2.4425402418515256,
      "grad_norm": 7.066239356994629,
      "learning_rate": 9.29099596914124e-06,
      "loss": 0.4827,
      "step": 152500
    },
    {
      "epoch": 2.450548570513334,
      "grad_norm": 6.070245742797852,
      "learning_rate": 9.15752382477777e-06,
      "loss": 0.465,
      "step": 153000
    },
    {
      "epoch": 2.458556899175142,
      "grad_norm": 4.008568286895752,
      "learning_rate": 9.024051680414299e-06,
      "loss": 0.4721,
      "step": 153500
    },
    {
      "epoch": 2.4665652278369503,
      "grad_norm": 7.347427845001221,
      "learning_rate": 8.890579536050827e-06,
      "loss": 0.4727,
      "step": 154000
    },
    {
      "epoch": 2.4745735564987585,
      "grad_norm": 3.486144781112671,
      "learning_rate": 8.757107391687356e-06,
      "loss": 0.5034,
      "step": 154500
    },
    {
      "epoch": 2.4825818851605668,
      "grad_norm": 3.234773874282837,
      "learning_rate": 8.623635247323884e-06,
      "loss": 0.4851,
      "step": 155000
    },
    {
      "epoch": 2.4905902138223754,
      "grad_norm": 4.342700958251953,
      "learning_rate": 8.490163102960412e-06,
      "loss": 0.48,
      "step": 155500
    },
    {
      "epoch": 2.4985985424841837,
      "grad_norm": 7.917558193206787,
      "learning_rate": 8.356690958596941e-06,
      "loss": 0.4699,
      "step": 156000
    },
    {
      "epoch": 2.506606871145992,
      "grad_norm": 4.106176853179932,
      "learning_rate": 8.22321881423347e-06,
      "loss": 0.4864,
      "step": 156500
    },
    {
      "epoch": 2.5146151998078,
      "grad_norm": 4.815080165863037,
      "learning_rate": 8.08974666987e-06,
      "loss": 0.4763,
      "step": 157000
    },
    {
      "epoch": 2.5226235284696084,
      "grad_norm": 5.983972072601318,
      "learning_rate": 7.956274525506528e-06,
      "loss": 0.5105,
      "step": 157500
    },
    {
      "epoch": 2.5306318571314166,
      "grad_norm": 4.014471530914307,
      "learning_rate": 7.822802381143055e-06,
      "loss": 0.4797,
      "step": 158000
    },
    {
      "epoch": 2.538640185793225,
      "grad_norm": 5.227955341339111,
      "learning_rate": 7.689330236779585e-06,
      "loss": 0.4953,
      "step": 158500
    },
    {
      "epoch": 2.5466485144550335,
      "grad_norm": 3.9301795959472656,
      "learning_rate": 7.5558580924161125e-06,
      "loss": 0.4772,
      "step": 159000
    },
    {
      "epoch": 2.5546568431168417,
      "grad_norm": 9.068924903869629,
      "learning_rate": 7.422385948052642e-06,
      "loss": 0.4985,
      "step": 159500
    },
    {
      "epoch": 2.56266517177865,
      "grad_norm": 15.708979606628418,
      "learning_rate": 7.2889138036891706e-06,
      "loss": 0.4753,
      "step": 160000
    },
    {
      "epoch": 2.570673500440458,
      "grad_norm": 5.85085391998291,
      "learning_rate": 7.155441659325698e-06,
      "loss": 0.4959,
      "step": 160500
    },
    {
      "epoch": 2.5786818291022664,
      "grad_norm": 10.106379508972168,
      "learning_rate": 7.021969514962228e-06,
      "loss": 0.4785,
      "step": 161000
    },
    {
      "epoch": 2.5866901577640746,
      "grad_norm": 3.3619790077209473,
      "learning_rate": 6.888497370598756e-06,
      "loss": 0.4888,
      "step": 161500
    },
    {
      "epoch": 2.594698486425883,
      "grad_norm": 7.827800750732422,
      "learning_rate": 6.755025226235286e-06,
      "loss": 0.492,
      "step": 162000
    },
    {
      "epoch": 2.602706815087691,
      "grad_norm": 6.471998691558838,
      "learning_rate": 6.6215530818718135e-06,
      "loss": 0.4951,
      "step": 162500
    },
    {
      "epoch": 2.6107151437494993,
      "grad_norm": 2.487199544906616,
      "learning_rate": 6.488080937508342e-06,
      "loss": 0.4862,
      "step": 163000
    },
    {
      "epoch": 2.6187234724113075,
      "grad_norm": 7.580549240112305,
      "learning_rate": 6.3546087931448716e-06,
      "loss": 0.4932,
      "step": 163500
    },
    {
      "epoch": 2.6267318010731158,
      "grad_norm": 2.266861915588379,
      "learning_rate": 6.2211366487814e-06,
      "loss": 0.4804,
      "step": 164000
    },
    {
      "epoch": 2.6347401297349244,
      "grad_norm": 5.710116386413574,
      "learning_rate": 6.087664504417928e-06,
      "loss": 0.4715,
      "step": 164500
    },
    {
      "epoch": 2.6427484583967327,
      "grad_norm": 5.276626110076904,
      "learning_rate": 5.954192360054457e-06,
      "loss": 0.4886,
      "step": 165000
    },
    {
      "epoch": 2.650756787058541,
      "grad_norm": 7.006643295288086,
      "learning_rate": 5.820720215690986e-06,
      "loss": 0.479,
      "step": 165500
    },
    {
      "epoch": 2.658765115720349,
      "grad_norm": 4.763871669769287,
      "learning_rate": 5.687248071327514e-06,
      "loss": 0.4769,
      "step": 166000
    },
    {
      "epoch": 2.6667734443821574,
      "grad_norm": 2.7046520709991455,
      "learning_rate": 5.553775926964043e-06,
      "loss": 0.4932,
      "step": 166500
    },
    {
      "epoch": 2.6747817730439656,
      "grad_norm": 4.3285064697265625,
      "learning_rate": 5.420303782600572e-06,
      "loss": 0.4792,
      "step": 167000
    },
    {
      "epoch": 2.6827901017057743,
      "grad_norm": 6.022039413452148,
      "learning_rate": 5.2868316382371e-06,
      "loss": 0.4909,
      "step": 167500
    },
    {
      "epoch": 2.6907984303675825,
      "grad_norm": 1.3580108880996704,
      "learning_rate": 5.153359493873629e-06,
      "loss": 0.4817,
      "step": 168000
    },
    {
      "epoch": 2.6988067590293907,
      "grad_norm": 9.179396629333496,
      "learning_rate": 5.0198873495101575e-06,
      "loss": 0.4846,
      "step": 168500
    },
    {
      "epoch": 2.706815087691199,
      "grad_norm": 4.357848644256592,
      "learning_rate": 4.886415205146686e-06,
      "loss": 0.4766,
      "step": 169000
    },
    {
      "epoch": 2.714823416353007,
      "grad_norm": 9.21454906463623,
      "learning_rate": 4.752943060783215e-06,
      "loss": 0.481,
      "step": 169500
    },
    {
      "epoch": 2.7228317450148154,
      "grad_norm": 3.493400812149048,
      "learning_rate": 4.619470916419743e-06,
      "loss": 0.4807,
      "step": 170000
    },
    {
      "epoch": 2.7308400736766236,
      "grad_norm": 4.984789848327637,
      "learning_rate": 4.485998772056272e-06,
      "loss": 0.4876,
      "step": 170500
    },
    {
      "epoch": 2.738848402338432,
      "grad_norm": 6.943166732788086,
      "learning_rate": 4.352526627692801e-06,
      "loss": 0.4854,
      "step": 171000
    },
    {
      "epoch": 2.74685673100024,
      "grad_norm": 4.886231422424316,
      "learning_rate": 4.219054483329329e-06,
      "loss": 0.4858,
      "step": 171500
    },
    {
      "epoch": 2.7548650596620483,
      "grad_norm": 6.6674041748046875,
      "learning_rate": 4.085582338965858e-06,
      "loss": 0.4793,
      "step": 172000
    },
    {
      "epoch": 2.7628733883238565,
      "grad_norm": 7.113861083984375,
      "learning_rate": 3.952110194602387e-06,
      "loss": 0.4859,
      "step": 172500
    },
    {
      "epoch": 2.770881716985665,
      "grad_norm": 8.29572868347168,
      "learning_rate": 3.818638050238915e-06,
      "loss": 0.4846,
      "step": 173000
    },
    {
      "epoch": 2.7788900456474734,
      "grad_norm": 12.40919303894043,
      "learning_rate": 3.6851659058754443e-06,
      "loss": 0.4897,
      "step": 173500
    },
    {
      "epoch": 2.7868983743092817,
      "grad_norm": 6.548822402954102,
      "learning_rate": 3.5516937615119725e-06,
      "loss": 0.4744,
      "step": 174000
    },
    {
      "epoch": 2.79490670297109,
      "grad_norm": 5.141139984130859,
      "learning_rate": 3.418221617148501e-06,
      "loss": 0.4862,
      "step": 174500
    },
    {
      "epoch": 2.802915031632898,
      "grad_norm": 4.598352432250977,
      "learning_rate": 3.28474947278503e-06,
      "loss": 0.4849,
      "step": 175000
    },
    {
      "epoch": 2.8109233602947064,
      "grad_norm": 12.266292572021484,
      "learning_rate": 3.1512773284215583e-06,
      "loss": 0.4788,
      "step": 175500
    },
    {
      "epoch": 2.818931688956515,
      "grad_norm": 3.243722677230835,
      "learning_rate": 3.0178051840580873e-06,
      "loss": 0.4959,
      "step": 176000
    },
    {
      "epoch": 2.8269400176183233,
      "grad_norm": 9.964518547058105,
      "learning_rate": 2.884333039694616e-06,
      "loss": 0.4876,
      "step": 176500
    },
    {
      "epoch": 2.8349483462801315,
      "grad_norm": 4.309595584869385,
      "learning_rate": 2.7508608953311445e-06,
      "loss": 0.4772,
      "step": 177000
    },
    {
      "epoch": 2.8429566749419397,
      "grad_norm": 3.8585689067840576,
      "learning_rate": 2.617388750967673e-06,
      "loss": 0.485,
      "step": 177500
    },
    {
      "epoch": 2.850965003603748,
      "grad_norm": 4.30459451675415,
      "learning_rate": 2.483916606604202e-06,
      "loss": 0.4755,
      "step": 178000
    },
    {
      "epoch": 2.858973332265556,
      "grad_norm": 5.043994903564453,
      "learning_rate": 2.3504444622407303e-06,
      "loss": 0.4654,
      "step": 178500
    },
    {
      "epoch": 2.8669816609273644,
      "grad_norm": 9.565049171447754,
      "learning_rate": 2.216972317877259e-06,
      "loss": 0.4872,
      "step": 179000
    },
    {
      "epoch": 2.8749899895891726,
      "grad_norm": 10.213244438171387,
      "learning_rate": 2.083500173513788e-06,
      "loss": 0.4844,
      "step": 179500
    },
    {
      "epoch": 2.882998318250981,
      "grad_norm": 2.163796901702881,
      "learning_rate": 1.9500280291503165e-06,
      "loss": 0.4827,
      "step": 180000
    },
    {
      "epoch": 2.891006646912789,
      "grad_norm": 6.1276044845581055,
      "learning_rate": 1.816555884786845e-06,
      "loss": 0.4786,
      "step": 180500
    },
    {
      "epoch": 2.8990149755745973,
      "grad_norm": 4.01878547668457,
      "learning_rate": 1.6830837404233739e-06,
      "loss": 0.4927,
      "step": 181000
    },
    {
      "epoch": 2.907023304236406,
      "grad_norm": 9.985204696655273,
      "learning_rate": 1.5496115960599025e-06,
      "loss": 0.478,
      "step": 181500
    },
    {
      "epoch": 2.915031632898214,
      "grad_norm": 2.642123222351074,
      "learning_rate": 1.416139451696431e-06,
      "loss": 0.4762,
      "step": 182000
    },
    {
      "epoch": 2.9230399615600224,
      "grad_norm": 1.8521270751953125,
      "learning_rate": 1.2826673073329597e-06,
      "loss": 0.4527,
      "step": 182500
    },
    {
      "epoch": 2.9310482902218307,
      "grad_norm": 10.468952178955078,
      "learning_rate": 1.1491951629694885e-06,
      "loss": 0.5051,
      "step": 183000
    },
    {
      "epoch": 2.939056618883639,
      "grad_norm": 1.8325883150100708,
      "learning_rate": 1.0157230186060169e-06,
      "loss": 0.4791,
      "step": 183500
    },
    {
      "epoch": 2.947064947545447,
      "grad_norm": 6.917385578155518,
      "learning_rate": 8.822508742425457e-07,
      "loss": 0.4678,
      "step": 184000
    },
    {
      "epoch": 2.955073276207256,
      "grad_norm": 2.300797939300537,
      "learning_rate": 7.487787298790743e-07,
      "loss": 0.4817,
      "step": 184500
    },
    {
      "epoch": 2.963081604869064,
      "grad_norm": 7.867565631866455,
      "learning_rate": 6.153065855156028e-07,
      "loss": 0.4738,
      "step": 185000
    },
    {
      "epoch": 2.9710899335308723,
      "grad_norm": 9.112363815307617,
      "learning_rate": 4.818344411521315e-07,
      "loss": 0.483,
      "step": 185500
    },
    {
      "epoch": 2.9790982621926805,
      "grad_norm": 6.043625354766846,
      "learning_rate": 3.483622967886602e-07,
      "loss": 0.4814,
      "step": 186000
    },
    {
      "epoch": 2.9871065908544887,
      "grad_norm": 5.873631000518799,
      "learning_rate": 2.148901524251889e-07,
      "loss": 0.4721,
      "step": 186500
    },
    {
      "epoch": 2.995114919516297,
      "grad_norm": 11.005748748779297,
      "learning_rate": 8.141800806171753e-08,
      "loss": 0.4793,
      "step": 187000
    },
    {
      "epoch": 3.0028827674567586,
      "grad_norm": 4.015688419342041,
      "learning_rate": 2.4975976937860345e-05,
      "loss": 0.534,
      "step": 187500
    },
    {
      "epoch": 3.0108904548366433,
      "grad_norm": 5.157755374908447,
      "learning_rate": 2.4909246209694642e-05,
      "loss": 0.5275,
      "step": 188000
    },
    {
      "epoch": 3.018898142216528,
      "grad_norm": 5.690236568450928,
      "learning_rate": 2.4842515481528936e-05,
      "loss": 0.5143,
      "step": 188500
    },
    {
      "epoch": 3.0269058295964126,
      "grad_norm": 3.1381242275238037,
      "learning_rate": 2.477578475336323e-05,
      "loss": 0.5168,
      "step": 189000
    },
    {
      "epoch": 3.0349135169762973,
      "grad_norm": 5.241495609283447,
      "learning_rate": 2.4709054025197524e-05,
      "loss": 0.5325,
      "step": 189500
    },
    {
      "epoch": 3.042921204356182,
      "grad_norm": 25.723737716674805,
      "learning_rate": 2.4642323297031818e-05,
      "loss": 0.514,
      "step": 190000
    },
    {
      "epoch": 3.0509288917360666,
      "grad_norm": 4.157095909118652,
      "learning_rate": 2.457559256886611e-05,
      "loss": 0.5106,
      "step": 190500
    },
    {
      "epoch": 3.0589365791159513,
      "grad_norm": 2.689317226409912,
      "learning_rate": 2.4508861840700405e-05,
      "loss": 0.5235,
      "step": 191000
    },
    {
      "epoch": 3.066944266495836,
      "grad_norm": 3.8834471702575684,
      "learning_rate": 2.4442131112534703e-05,
      "loss": 0.5163,
      "step": 191500
    },
    {
      "epoch": 3.0749519538757206,
      "grad_norm": 4.402151107788086,
      "learning_rate": 2.4375400384368996e-05,
      "loss": 0.5401,
      "step": 192000
    },
    {
      "epoch": 3.0829596412556053,
      "grad_norm": 5.29595947265625,
      "learning_rate": 2.430866965620329e-05,
      "loss": 0.5272,
      "step": 192500
    },
    {
      "epoch": 3.09096732863549,
      "grad_norm": 4.661130428314209,
      "learning_rate": 2.4241938928037584e-05,
      "loss": 0.5213,
      "step": 193000
    },
    {
      "epoch": 3.0989750160153746,
      "grad_norm": 4.388880729675293,
      "learning_rate": 2.4175208199871878e-05,
      "loss": 0.5206,
      "step": 193500
    },
    {
      "epoch": 3.1069827033952593,
      "grad_norm": 3.181835412979126,
      "learning_rate": 2.4108477471706172e-05,
      "loss": 0.5336,
      "step": 194000
    },
    {
      "epoch": 3.114990390775144,
      "grad_norm": 3.5677480697631836,
      "learning_rate": 2.4041746743540466e-05,
      "loss": 0.537,
      "step": 194500
    },
    {
      "epoch": 3.122998078155029,
      "grad_norm": 8.93224048614502,
      "learning_rate": 2.397501601537476e-05,
      "loss": 0.5083,
      "step": 195000
    },
    {
      "epoch": 3.1310057655349137,
      "grad_norm": 2.1595449447631836,
      "learning_rate": 2.3908285287209057e-05,
      "loss": 0.5143,
      "step": 195500
    },
    {
      "epoch": 3.1390134529147984,
      "grad_norm": 2.918813943862915,
      "learning_rate": 2.384155455904335e-05,
      "loss": 0.5363,
      "step": 196000
    },
    {
      "epoch": 3.147021140294683,
      "grad_norm": 4.768782138824463,
      "learning_rate": 2.3774823830877645e-05,
      "loss": 0.5343,
      "step": 196500
    },
    {
      "epoch": 3.1550288276745677,
      "grad_norm": 3.8610031604766846,
      "learning_rate": 2.3708093102711935e-05,
      "loss": 0.5143,
      "step": 197000
    },
    {
      "epoch": 3.1630365150544524,
      "grad_norm": 4.565296173095703,
      "learning_rate": 2.3641362374546233e-05,
      "loss": 0.5154,
      "step": 197500
    },
    {
      "epoch": 3.171044202434337,
      "grad_norm": 3.290900468826294,
      "learning_rate": 2.3574631646380526e-05,
      "loss": 0.5269,
      "step": 198000
    },
    {
      "epoch": 3.1790518898142217,
      "grad_norm": 2.7042393684387207,
      "learning_rate": 2.350790091821482e-05,
      "loss": 0.525,
      "step": 198500
    },
    {
      "epoch": 3.1870595771941064,
      "grad_norm": 3.5647687911987305,
      "learning_rate": 2.3441170190049114e-05,
      "loss": 0.5401,
      "step": 199000
    },
    {
      "epoch": 3.195067264573991,
      "grad_norm": 9.507119178771973,
      "learning_rate": 2.337443946188341e-05,
      "loss": 0.5097,
      "step": 199500
    },
    {
      "epoch": 3.2030749519538757,
      "grad_norm": 4.8094940185546875,
      "learning_rate": 2.3307708733717702e-05,
      "loss": 0.5413,
      "step": 200000
    },
    {
      "epoch": 3.2110826393337604,
      "grad_norm": 2.002091646194458,
      "learning_rate": 2.3240978005551996e-05,
      "loss": 0.5189,
      "step": 200500
    },
    {
      "epoch": 3.219090326713645,
      "grad_norm": 3.932798147201538,
      "learning_rate": 2.317424727738629e-05,
      "loss": 0.5298,
      "step": 201000
    },
    {
      "epoch": 3.2270980140935297,
      "grad_norm": 3.5405900478363037,
      "learning_rate": 2.3107516549220587e-05,
      "loss": 0.5306,
      "step": 201500
    },
    {
      "epoch": 3.2351057014734144,
      "grad_norm": 2.8857152462005615,
      "learning_rate": 2.304078582105488e-05,
      "loss": 0.5109,
      "step": 202000
    },
    {
      "epoch": 3.243113388853299,
      "grad_norm": 5.128933429718018,
      "learning_rate": 2.2974055092889175e-05,
      "loss": 0.5245,
      "step": 202500
    },
    {
      "epoch": 3.2511210762331837,
      "grad_norm": 3.2885260581970215,
      "learning_rate": 2.2907324364723472e-05,
      "loss": 0.529,
      "step": 203000
    },
    {
      "epoch": 3.2591287636130684,
      "grad_norm": 3.4679770469665527,
      "learning_rate": 2.2840593636557762e-05,
      "loss": 0.5214,
      "step": 203500
    },
    {
      "epoch": 3.267136450992953,
      "grad_norm": 6.298839092254639,
      "learning_rate": 2.2773862908392056e-05,
      "loss": 0.5263,
      "step": 204000
    },
    {
      "epoch": 3.2751441383728377,
      "grad_norm": 7.017887115478516,
      "learning_rate": 2.270713218022635e-05,
      "loss": 0.5144,
      "step": 204500
    },
    {
      "epoch": 3.283151825752723,
      "grad_norm": 4.024120330810547,
      "learning_rate": 2.2640401452060647e-05,
      "loss": 0.5206,
      "step": 205000
    },
    {
      "epoch": 3.2911595131326075,
      "grad_norm": 5.880560874938965,
      "learning_rate": 2.257367072389494e-05,
      "loss": 0.5137,
      "step": 205500
    },
    {
      "epoch": 3.299167200512492,
      "grad_norm": 7.701251983642578,
      "learning_rate": 2.2506939995729235e-05,
      "loss": 0.5226,
      "step": 206000
    },
    {
      "epoch": 3.307174887892377,
      "grad_norm": 3.6405086517333984,
      "learning_rate": 2.244020926756353e-05,
      "loss": 0.5285,
      "step": 206500
    },
    {
      "epoch": 3.3151825752722615,
      "grad_norm": 5.838491439819336,
      "learning_rate": 2.2373478539397823e-05,
      "loss": 0.5199,
      "step": 207000
    },
    {
      "epoch": 3.323190262652146,
      "grad_norm": 3.035240888595581,
      "learning_rate": 2.2306747811232117e-05,
      "loss": 0.5235,
      "step": 207500
    },
    {
      "epoch": 3.331197950032031,
      "grad_norm": 25.145437240600586,
      "learning_rate": 2.224001708306641e-05,
      "loss": 0.511,
      "step": 208000
    },
    {
      "epoch": 3.3392056374119155,
      "grad_norm": 2.441861867904663,
      "learning_rate": 2.2173286354900705e-05,
      "loss": 0.5395,
      "step": 208500
    },
    {
      "epoch": 3.3472133247918,
      "grad_norm": 6.42642068862915,
      "learning_rate": 2.2106555626735002e-05,
      "loss": 0.5188,
      "step": 209000
    },
    {
      "epoch": 3.355221012171685,
      "grad_norm": 5.771033763885498,
      "learning_rate": 2.2039824898569296e-05,
      "loss": 0.5285,
      "step": 209500
    },
    {
      "epoch": 3.3632286995515694,
      "grad_norm": 5.8149309158325195,
      "learning_rate": 2.197309417040359e-05,
      "loss": 0.5206,
      "step": 210000
    },
    {
      "epoch": 3.371236386931454,
      "grad_norm": 7.800816059112549,
      "learning_rate": 2.190636344223788e-05,
      "loss": 0.5128,
      "step": 210500
    },
    {
      "epoch": 3.3792440743113388,
      "grad_norm": 6.287461280822754,
      "learning_rate": 2.1839632714072177e-05,
      "loss": 0.5238,
      "step": 211000
    },
    {
      "epoch": 3.3872517616912234,
      "grad_norm": 2.292232036590576,
      "learning_rate": 2.177290198590647e-05,
      "loss": 0.519,
      "step": 211500
    },
    {
      "epoch": 3.395259449071108,
      "grad_norm": 5.177983283996582,
      "learning_rate": 2.1706171257740765e-05,
      "loss": 0.5271,
      "step": 212000
    },
    {
      "epoch": 3.4032671364509928,
      "grad_norm": 5.217452049255371,
      "learning_rate": 2.163944052957506e-05,
      "loss": 0.5236,
      "step": 212500
    },
    {
      "epoch": 3.411274823830878,
      "grad_norm": 8.582279205322266,
      "learning_rate": 2.1572709801409356e-05,
      "loss": 0.5243,
      "step": 213000
    },
    {
      "epoch": 3.4192825112107625,
      "grad_norm": 4.369421005249023,
      "learning_rate": 2.1505979073243647e-05,
      "loss": 0.5211,
      "step": 213500
    },
    {
      "epoch": 3.427290198590647,
      "grad_norm": 4.6067962646484375,
      "learning_rate": 2.143924834507794e-05,
      "loss": 0.5287,
      "step": 214000
    },
    {
      "epoch": 3.435297885970532,
      "grad_norm": 9.103211402893066,
      "learning_rate": 2.1372517616912234e-05,
      "loss": 0.4971,
      "step": 214500
    },
    {
      "epoch": 3.4433055733504165,
      "grad_norm": 5.65288782119751,
      "learning_rate": 2.1305786888746532e-05,
      "loss": 0.5173,
      "step": 215000
    },
    {
      "epoch": 3.451313260730301,
      "grad_norm": 3.130932569503784,
      "learning_rate": 2.1239056160580826e-05,
      "loss": 0.5258,
      "step": 215500
    },
    {
      "epoch": 3.459320948110186,
      "grad_norm": 6.442335605621338,
      "learning_rate": 2.117232543241512e-05,
      "loss": 0.5342,
      "step": 216000
    },
    {
      "epoch": 3.4673286354900705,
      "grad_norm": 6.625487804412842,
      "learning_rate": 2.1105594704249417e-05,
      "loss": 0.5194,
      "step": 216500
    },
    {
      "epoch": 3.475336322869955,
      "grad_norm": 3.204094171524048,
      "learning_rate": 2.1038863976083707e-05,
      "loss": 0.542,
      "step": 217000
    },
    {
      "epoch": 3.48334401024984,
      "grad_norm": 4.633195400238037,
      "learning_rate": 2.0972133247918e-05,
      "loss": 0.5281,
      "step": 217500
    },
    {
      "epoch": 3.4913516976297245,
      "grad_norm": 12.156996726989746,
      "learning_rate": 2.0905402519752295e-05,
      "loss": 0.5345,
      "step": 218000
    },
    {
      "epoch": 3.499359385009609,
      "grad_norm": 13.965764999389648,
      "learning_rate": 2.0838671791586592e-05,
      "loss": 0.5416,
      "step": 218500
    },
    {
      "epoch": 3.507367072389494,
      "grad_norm": 4.767992973327637,
      "learning_rate": 2.0771941063420886e-05,
      "loss": 0.5072,
      "step": 219000
    },
    {
      "epoch": 3.5153747597693785,
      "grad_norm": 7.496109962463379,
      "learning_rate": 2.070521033525518e-05,
      "loss": 0.5229,
      "step": 219500
    },
    {
      "epoch": 3.523382447149263,
      "grad_norm": 3.921424388885498,
      "learning_rate": 2.0638479607089474e-05,
      "loss": 0.5199,
      "step": 220000
    },
    {
      "epoch": 3.531390134529148,
      "grad_norm": 3.760723829269409,
      "learning_rate": 2.0571748878923768e-05,
      "loss": 0.5144,
      "step": 220500
    },
    {
      "epoch": 3.5393978219090325,
      "grad_norm": 6.502561569213867,
      "learning_rate": 2.050501815075806e-05,
      "loss": 0.5143,
      "step": 221000
    },
    {
      "epoch": 3.547405509288917,
      "grad_norm": 3.0126633644104004,
      "learning_rate": 2.0438287422592356e-05,
      "loss": 0.5317,
      "step": 221500
    },
    {
      "epoch": 3.555413196668802,
      "grad_norm": 5.682426929473877,
      "learning_rate": 2.037155669442665e-05,
      "loss": 0.5149,
      "step": 222000
    },
    {
      "epoch": 3.5634208840486865,
      "grad_norm": 3.9135797023773193,
      "learning_rate": 2.0304825966260947e-05,
      "loss": 0.5168,
      "step": 222500
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 1.7475917339324951,
      "learning_rate": 2.023809523809524e-05,
      "loss": 0.5209,
      "step": 223000
    },
    {
      "epoch": 3.5794362588084563,
      "grad_norm": 2.3430371284484863,
      "learning_rate": 2.0171364509929534e-05,
      "loss": 0.5237,
      "step": 223500
    },
    {
      "epoch": 3.587443946188341,
      "grad_norm": 3.2557568550109863,
      "learning_rate": 2.0104633781763825e-05,
      "loss": 0.5139,
      "step": 224000
    },
    {
      "epoch": 3.5954516335682256,
      "grad_norm": 3.662773370742798,
      "learning_rate": 2.0037903053598122e-05,
      "loss": 0.5045,
      "step": 224500
    },
    {
      "epoch": 3.6034593209481103,
      "grad_norm": 1.573135256767273,
      "learning_rate": 1.9971172325432416e-05,
      "loss": 0.5054,
      "step": 225000
    },
    {
      "epoch": 3.611467008327995,
      "grad_norm": 6.390472888946533,
      "learning_rate": 1.990444159726671e-05,
      "loss": 0.5076,
      "step": 225500
    },
    {
      "epoch": 3.6194746957078796,
      "grad_norm": 4.968250274658203,
      "learning_rate": 1.9837710869101004e-05,
      "loss": 0.5206,
      "step": 226000
    },
    {
      "epoch": 3.6274823830877643,
      "grad_norm": 6.153519153594971,
      "learning_rate": 1.97709801409353e-05,
      "loss": 0.5154,
      "step": 226500
    },
    {
      "epoch": 3.635490070467649,
      "grad_norm": 2.077425003051758,
      "learning_rate": 1.970424941276959e-05,
      "loss": 0.5295,
      "step": 227000
    },
    {
      "epoch": 3.6434977578475336,
      "grad_norm": 15.79783821105957,
      "learning_rate": 1.9637518684603885e-05,
      "loss": 0.5065,
      "step": 227500
    },
    {
      "epoch": 3.6515054452274183,
      "grad_norm": 2.5502419471740723,
      "learning_rate": 1.9570787956438183e-05,
      "loss": 0.514,
      "step": 228000
    },
    {
      "epoch": 3.659513132607303,
      "grad_norm": 4.73549222946167,
      "learning_rate": 1.9504057228272477e-05,
      "loss": 0.5015,
      "step": 228500
    },
    {
      "epoch": 3.6675208199871876,
      "grad_norm": 4.1045122146606445,
      "learning_rate": 1.943732650010677e-05,
      "loss": 0.5226,
      "step": 229000
    },
    {
      "epoch": 3.6755285073670723,
      "grad_norm": 4.976570129394531,
      "learning_rate": 1.9370595771941064e-05,
      "loss": 0.507,
      "step": 229500
    },
    {
      "epoch": 3.683536194746957,
      "grad_norm": 2.324946403503418,
      "learning_rate": 1.930386504377536e-05,
      "loss": 0.5313,
      "step": 230000
    },
    {
      "epoch": 3.691543882126842,
      "grad_norm": 2.331120252609253,
      "learning_rate": 1.9237134315609652e-05,
      "loss": 0.505,
      "step": 230500
    },
    {
      "epoch": 3.6995515695067267,
      "grad_norm": 7.613391399383545,
      "learning_rate": 1.9170403587443946e-05,
      "loss": 0.5106,
      "step": 231000
    },
    {
      "epoch": 3.7075592568866114,
      "grad_norm": 7.033883094787598,
      "learning_rate": 1.910367285927824e-05,
      "loss": 0.5171,
      "step": 231500
    },
    {
      "epoch": 3.715566944266496,
      "grad_norm": 5.399119853973389,
      "learning_rate": 1.9036942131112537e-05,
      "loss": 0.561,
      "step": 232000
    },
    {
      "epoch": 3.7235746316463807,
      "grad_norm": 6.534111022949219,
      "learning_rate": 1.897021140294683e-05,
      "loss": 0.5443,
      "step": 232500
    },
    {
      "epoch": 3.7315823190262654,
      "grad_norm": 4.875268459320068,
      "learning_rate": 1.8903480674781125e-05,
      "loss": 0.5745,
      "step": 233000
    },
    {
      "epoch": 3.73959000640615,
      "grad_norm": 2.143779993057251,
      "learning_rate": 1.883674994661542e-05,
      "loss": 0.5668,
      "step": 233500
    },
    {
      "epoch": 3.7475976937860347,
      "grad_norm": 5.978457450866699,
      "learning_rate": 1.8770019218449713e-05,
      "loss": 0.5561,
      "step": 234000
    },
    {
      "epoch": 3.7556053811659194,
      "grad_norm": 34.79189682006836,
      "learning_rate": 1.8703288490284006e-05,
      "loss": 0.6064,
      "step": 234500
    },
    {
      "epoch": 3.763613068545804,
      "grad_norm": 1.356866478919983,
      "learning_rate": 1.86365577621183e-05,
      "loss": 0.5811,
      "step": 235000
    },
    {
      "epoch": 3.7716207559256887,
      "grad_norm": 144.53857421875,
      "learning_rate": 1.8569827033952594e-05,
      "loss": 0.5538,
      "step": 235500
    },
    {
      "epoch": 3.7796284433055733,
      "grad_norm": 7.587902545928955,
      "learning_rate": 1.850309630578689e-05,
      "loss": 0.5843,
      "step": 236000
    },
    {
      "epoch": 3.787636130685458,
      "grad_norm": 9.950390815734863,
      "learning_rate": 1.8436365577621185e-05,
      "loss": 0.5302,
      "step": 236500
    },
    {
      "epoch": 3.7956438180653427,
      "grad_norm": 3.351644515991211,
      "learning_rate": 1.836963484945548e-05,
      "loss": 0.5496,
      "step": 237000
    },
    {
      "epoch": 3.8036515054452273,
      "grad_norm": 6.246541500091553,
      "learning_rate": 1.830290412128977e-05,
      "loss": 0.5585,
      "step": 237500
    },
    {
      "epoch": 3.811659192825112,
      "grad_norm": 3.85917329788208,
      "learning_rate": 1.8236173393124067e-05,
      "loss": 0.5273,
      "step": 238000
    },
    {
      "epoch": 3.8196668802049967,
      "grad_norm": 8.33314323425293,
      "learning_rate": 1.816944266495836e-05,
      "loss": 0.5375,
      "step": 238500
    },
    {
      "epoch": 3.8276745675848813,
      "grad_norm": 5.771579742431641,
      "learning_rate": 1.8102711936792655e-05,
      "loss": 0.5332,
      "step": 239000
    },
    {
      "epoch": 3.835682254964766,
      "grad_norm": 3.8213882446289062,
      "learning_rate": 1.803598120862695e-05,
      "loss": 0.5309,
      "step": 239500
    },
    {
      "epoch": 3.8436899423446507,
      "grad_norm": 5.864470958709717,
      "learning_rate": 1.7969250480461246e-05,
      "loss": 0.5356,
      "step": 240000
    },
    {
      "epoch": 3.8516976297245353,
      "grad_norm": 4.482711315155029,
      "learning_rate": 1.7902519752295536e-05,
      "loss": 0.5407,
      "step": 240500
    },
    {
      "epoch": 3.85970531710442,
      "grad_norm": 6.829257488250732,
      "learning_rate": 1.783578902412983e-05,
      "loss": 0.52,
      "step": 241000
    },
    {
      "epoch": 3.8677130044843047,
      "grad_norm": 6.028481483459473,
      "learning_rate": 1.7769058295964127e-05,
      "loss": 0.5301,
      "step": 241500
    },
    {
      "epoch": 3.8757206918641898,
      "grad_norm": 5.929718494415283,
      "learning_rate": 1.770232756779842e-05,
      "loss": 0.5516,
      "step": 242000
    },
    {
      "epoch": 3.8837283792440744,
      "grad_norm": 7.70997428894043,
      "learning_rate": 1.7635596839632715e-05,
      "loss": 0.5652,
      "step": 242500
    },
    {
      "epoch": 3.891736066623959,
      "grad_norm": 5.585678577423096,
      "learning_rate": 1.756886611146701e-05,
      "loss": 0.5486,
      "step": 243000
    },
    {
      "epoch": 3.8997437540038438,
      "grad_norm": 3.879845142364502,
      "learning_rate": 1.7502135383301303e-05,
      "loss": 0.5159,
      "step": 243500
    },
    {
      "epoch": 3.9077514413837284,
      "grad_norm": 2.580503463745117,
      "learning_rate": 1.7435404655135597e-05,
      "loss": 0.5339,
      "step": 244000
    },
    {
      "epoch": 3.915759128763613,
      "grad_norm": 2.3360297679901123,
      "learning_rate": 1.736867392696989e-05,
      "loss": 0.5402,
      "step": 244500
    },
    {
      "epoch": 3.9237668161434978,
      "grad_norm": 24.927274703979492,
      "learning_rate": 1.7301943198804185e-05,
      "loss": 0.5554,
      "step": 245000
    },
    {
      "epoch": 3.9317745035233824,
      "grad_norm": 8.378798484802246,
      "learning_rate": 1.7235212470638482e-05,
      "loss": 0.5453,
      "step": 245500
    },
    {
      "epoch": 3.939782190903267,
      "grad_norm": 8.902276992797852,
      "learning_rate": 1.7168481742472776e-05,
      "loss": 0.5816,
      "step": 246000
    },
    {
      "epoch": 3.9477898782831518,
      "grad_norm": 3.1411001682281494,
      "learning_rate": 1.710175101430707e-05,
      "loss": 0.5308,
      "step": 246500
    },
    {
      "epoch": 3.9557975656630364,
      "grad_norm": 3.4038777351379395,
      "learning_rate": 1.7035020286141364e-05,
      "loss": 0.5373,
      "step": 247000
    },
    {
      "epoch": 3.963805253042921,
      "grad_norm": 2.4442849159240723,
      "learning_rate": 1.6968289557975657e-05,
      "loss": 0.564,
      "step": 247500
    },
    {
      "epoch": 3.9718129404228057,
      "grad_norm": 4.104819297790527,
      "learning_rate": 1.690155882980995e-05,
      "loss": 0.5598,
      "step": 248000
    },
    {
      "epoch": 3.979820627802691,
      "grad_norm": 2.071009874343872,
      "learning_rate": 1.6834828101644245e-05,
      "loss": 0.5699,
      "step": 248500
    },
    {
      "epoch": 3.9878283151825755,
      "grad_norm": 6.5647149085998535,
      "learning_rate": 1.676809737347854e-05,
      "loss": 0.5705,
      "step": 249000
    },
    {
      "epoch": 3.99583600256246,
      "grad_norm": 2.4963955879211426,
      "learning_rate": 1.6701366645312836e-05,
      "loss": 0.5634,
      "step": 249500
    },
    {
      "epoch": 4.003843689942345,
      "grad_norm": 150.49623107910156,
      "learning_rate": 1.663463591714713e-05,
      "loss": 0.5454,
      "step": 250000
    },
    {
      "epoch": 4.0118513773222295,
      "grad_norm": 7.540205955505371,
      "learning_rate": 1.6567905188981424e-05,
      "loss": 0.5346,
      "step": 250500
    },
    {
      "epoch": 4.019859064702114,
      "grad_norm": 10.86510181427002,
      "learning_rate": 1.6501174460815715e-05,
      "loss": 0.5441,
      "step": 251000
    },
    {
      "epoch": 4.027866752081999,
      "grad_norm": 182.1198272705078,
      "learning_rate": 1.6434443732650012e-05,
      "loss": 0.524,
      "step": 251500
    },
    {
      "epoch": 4.0358744394618835,
      "grad_norm": 2.2060883045196533,
      "learning_rate": 1.6367713004484306e-05,
      "loss": 0.5263,
      "step": 252000
    },
    {
      "epoch": 4.043882126841768,
      "grad_norm": 9.623943328857422,
      "learning_rate": 1.63009822763186e-05,
      "loss": 0.5536,
      "step": 252500
    },
    {
      "epoch": 4.051889814221653,
      "grad_norm": 4.34499979019165,
      "learning_rate": 1.6234251548152893e-05,
      "loss": 0.5182,
      "step": 253000
    },
    {
      "epoch": 4.0598975016015375,
      "grad_norm": 5.492642879486084,
      "learning_rate": 1.616752081998719e-05,
      "loss": 0.5241,
      "step": 253500
    },
    {
      "epoch": 4.067905188981422,
      "grad_norm": 3.2466022968292236,
      "learning_rate": 1.610079009182148e-05,
      "loss": 0.5434,
      "step": 254000
    },
    {
      "epoch": 4.075912876361307,
      "grad_norm": 3.040654420852661,
      "learning_rate": 1.6034059363655775e-05,
      "loss": 0.5893,
      "step": 254500
    },
    {
      "epoch": 4.0839205637411915,
      "grad_norm": 2.047579288482666,
      "learning_rate": 1.5967328635490072e-05,
      "loss": 0.5916,
      "step": 255000
    },
    {
      "epoch": 4.091928251121076,
      "grad_norm": 6.849547386169434,
      "learning_rate": 1.5900597907324366e-05,
      "loss": 0.5689,
      "step": 255500
    },
    {
      "epoch": 4.099935938500961,
      "grad_norm": 3.4308221340179443,
      "learning_rate": 1.583386717915866e-05,
      "loss": 0.5307,
      "step": 256000
    },
    {
      "epoch": 4.1079436258808455,
      "grad_norm": 2.9829118251800537,
      "learning_rate": 1.5767136450992954e-05,
      "loss": 0.5431,
      "step": 256500
    },
    {
      "epoch": 4.11595131326073,
      "grad_norm": 6.21203088760376,
      "learning_rate": 1.5700405722827248e-05,
      "loss": 0.5076,
      "step": 257000
    },
    {
      "epoch": 4.123959000640615,
      "grad_norm": 6.3341803550720215,
      "learning_rate": 1.563367499466154e-05,
      "loss": 0.5167,
      "step": 257500
    },
    {
      "epoch": 4.1319666880204995,
      "grad_norm": 6.211516857147217,
      "learning_rate": 1.5566944266495836e-05,
      "loss": 0.5062,
      "step": 258000
    },
    {
      "epoch": 4.139974375400384,
      "grad_norm": 3.269399881362915,
      "learning_rate": 1.550021353833013e-05,
      "loss": 0.5045,
      "step": 258500
    },
    {
      "epoch": 4.147982062780269,
      "grad_norm": 4.230234146118164,
      "learning_rate": 1.5433482810164427e-05,
      "loss": 0.5039,
      "step": 259000
    },
    {
      "epoch": 4.1559897501601535,
      "grad_norm": 51.506507873535156,
      "learning_rate": 1.536675208199872e-05,
      "loss": 0.5041,
      "step": 259500
    },
    {
      "epoch": 4.163997437540038,
      "grad_norm": 5.417057991027832,
      "learning_rate": 1.5300021353833014e-05,
      "loss": 0.5228,
      "step": 260000
    },
    {
      "epoch": 4.172005124919923,
      "grad_norm": 4.610279560089111,
      "learning_rate": 1.5233290625667307e-05,
      "loss": 0.5001,
      "step": 260500
    },
    {
      "epoch": 4.1800128122998075,
      "grad_norm": 4.606158256530762,
      "learning_rate": 1.5166559897501604e-05,
      "loss": 0.5168,
      "step": 261000
    },
    {
      "epoch": 4.188020499679692,
      "grad_norm": 4.496291160583496,
      "learning_rate": 1.5099829169335896e-05,
      "loss": 0.499,
      "step": 261500
    },
    {
      "epoch": 4.196028187059577,
      "grad_norm": 6.866757869720459,
      "learning_rate": 1.503309844117019e-05,
      "loss": 0.5048,
      "step": 262000
    },
    {
      "epoch": 4.2040358744394615,
      "grad_norm": 6.067983627319336,
      "learning_rate": 1.4966367713004484e-05,
      "loss": 0.5123,
      "step": 262500
    },
    {
      "epoch": 4.212043561819346,
      "grad_norm": 8.032356262207031,
      "learning_rate": 1.489963698483878e-05,
      "loss": 0.5052,
      "step": 263000
    },
    {
      "epoch": 4.220051249199232,
      "grad_norm": 6.138614177703857,
      "learning_rate": 1.4832906256673073e-05,
      "loss": 0.4918,
      "step": 263500
    },
    {
      "epoch": 4.228058936579116,
      "grad_norm": 4.1537957191467285,
      "learning_rate": 1.4766175528507367e-05,
      "loss": 0.509,
      "step": 264000
    },
    {
      "epoch": 4.236066623959001,
      "grad_norm": 4.48830509185791,
      "learning_rate": 1.4699444800341661e-05,
      "loss": 0.5048,
      "step": 264500
    },
    {
      "epoch": 4.244074311338886,
      "grad_norm": 2.930828809738159,
      "learning_rate": 1.4632714072175957e-05,
      "loss": 0.5077,
      "step": 265000
    },
    {
      "epoch": 4.25208199871877,
      "grad_norm": 1.3002238273620605,
      "learning_rate": 1.456598334401025e-05,
      "loss": 0.5012,
      "step": 265500
    },
    {
      "epoch": 4.260089686098655,
      "grad_norm": 15.42573070526123,
      "learning_rate": 1.4499252615844544e-05,
      "loss": 0.5163,
      "step": 266000
    },
    {
      "epoch": 4.26809737347854,
      "grad_norm": 4.384527206420898,
      "learning_rate": 1.443252188767884e-05,
      "loss": 0.5119,
      "step": 266500
    },
    {
      "epoch": 4.276105060858424,
      "grad_norm": 43.1288948059082,
      "learning_rate": 1.4365791159513134e-05,
      "loss": 0.5252,
      "step": 267000
    },
    {
      "epoch": 4.284112748238309,
      "grad_norm": 6.192732810974121,
      "learning_rate": 1.4299060431347428e-05,
      "loss": 0.5008,
      "step": 267500
    },
    {
      "epoch": 4.292120435618194,
      "grad_norm": 3.314314603805542,
      "learning_rate": 1.4232329703181722e-05,
      "loss": 0.5118,
      "step": 268000
    },
    {
      "epoch": 4.300128122998078,
      "grad_norm": 4.360948085784912,
      "learning_rate": 1.4165598975016017e-05,
      "loss": 0.5008,
      "step": 268500
    },
    {
      "epoch": 4.308135810377963,
      "grad_norm": 3.090627908706665,
      "learning_rate": 1.4098868246850311e-05,
      "loss": 0.5119,
      "step": 269000
    },
    {
      "epoch": 4.316143497757848,
      "grad_norm": 4.3881683349609375,
      "learning_rate": 1.4032137518684605e-05,
      "loss": 0.5238,
      "step": 269500
    },
    {
      "epoch": 4.324151185137732,
      "grad_norm": 6.906801700592041,
      "learning_rate": 1.3965406790518897e-05,
      "loss": 0.5201,
      "step": 270000
    },
    {
      "epoch": 4.332158872517617,
      "grad_norm": 19.744882583618164,
      "learning_rate": 1.3898676062353194e-05,
      "loss": 0.4947,
      "step": 270500
    },
    {
      "epoch": 4.340166559897502,
      "grad_norm": 10.030303001403809,
      "learning_rate": 1.3831945334187488e-05,
      "loss": 0.5004,
      "step": 271000
    },
    {
      "epoch": 4.348174247277386,
      "grad_norm": 4.8604254722595215,
      "learning_rate": 1.3765214606021782e-05,
      "loss": 0.5005,
      "step": 271500
    },
    {
      "epoch": 4.356181934657271,
      "grad_norm": 3.4427003860473633,
      "learning_rate": 1.3698483877856074e-05,
      "loss": 0.5288,
      "step": 272000
    },
    {
      "epoch": 4.364189622037156,
      "grad_norm": 3.0980567932128906,
      "learning_rate": 1.3631753149690372e-05,
      "loss": 0.5289,
      "step": 272500
    },
    {
      "epoch": 4.37219730941704,
      "grad_norm": 6.897751331329346,
      "learning_rate": 1.3565022421524665e-05,
      "loss": 0.5225,
      "step": 273000
    },
    {
      "epoch": 4.380204996796925,
      "grad_norm": 7.022542476654053,
      "learning_rate": 1.3498291693358958e-05,
      "loss": 0.5182,
      "step": 273500
    },
    {
      "epoch": 4.38821268417681,
      "grad_norm": 4.048801898956299,
      "learning_rate": 1.3431560965193251e-05,
      "loss": 0.501,
      "step": 274000
    },
    {
      "epoch": 4.396220371556694,
      "grad_norm": 7.635848522186279,
      "learning_rate": 1.3364830237027549e-05,
      "loss": 0.5309,
      "step": 274500
    },
    {
      "epoch": 4.404228058936579,
      "grad_norm": 5.4705424308776855,
      "learning_rate": 1.3298099508861841e-05,
      "loss": 0.5232,
      "step": 275000
    },
    {
      "epoch": 4.412235746316464,
      "grad_norm": 5.873051643371582,
      "learning_rate": 1.3231368780696135e-05,
      "loss": 0.5298,
      "step": 275500
    },
    {
      "epoch": 4.420243433696348,
      "grad_norm": 14.253281593322754,
      "learning_rate": 1.3164638052530429e-05,
      "loss": 0.5122,
      "step": 276000
    },
    {
      "epoch": 4.428251121076233,
      "grad_norm": 10.148263931274414,
      "learning_rate": 1.3097907324364724e-05,
      "loss": 0.5012,
      "step": 276500
    },
    {
      "epoch": 4.436258808456118,
      "grad_norm": 10.017847061157227,
      "learning_rate": 1.3031176596199018e-05,
      "loss": 0.4908,
      "step": 277000
    },
    {
      "epoch": 4.444266495836002,
      "grad_norm": 4.911150932312012,
      "learning_rate": 1.2964445868033312e-05,
      "loss": 0.5063,
      "step": 277500
    },
    {
      "epoch": 4.452274183215887,
      "grad_norm": 3.4809353351593018,
      "learning_rate": 1.2897715139867606e-05,
      "loss": 0.5049,
      "step": 278000
    },
    {
      "epoch": 4.460281870595772,
      "grad_norm": 1.6174324750900269,
      "learning_rate": 1.2830984411701901e-05,
      "loss": 0.5078,
      "step": 278500
    },
    {
      "epoch": 4.468289557975656,
      "grad_norm": 6.694969654083252,
      "learning_rate": 1.2764253683536195e-05,
      "loss": 0.5172,
      "step": 279000
    },
    {
      "epoch": 4.476297245355541,
      "grad_norm": 5.052237510681152,
      "learning_rate": 1.269752295537049e-05,
      "loss": 0.517,
      "step": 279500
    },
    {
      "epoch": 4.484304932735426,
      "grad_norm": 7.702732563018799,
      "learning_rate": 1.2630792227204785e-05,
      "loss": 0.5022,
      "step": 280000
    },
    {
      "epoch": 4.49231262011531,
      "grad_norm": 1.7527016401290894,
      "learning_rate": 1.2564061499039079e-05,
      "loss": 0.521,
      "step": 280500
    },
    {
      "epoch": 4.500320307495196,
      "grad_norm": 6.1656880378723145,
      "learning_rate": 1.2497330770873372e-05,
      "loss": 0.5103,
      "step": 281000
    },
    {
      "epoch": 4.50832799487508,
      "grad_norm": 7.274654865264893,
      "learning_rate": 1.2430600042707666e-05,
      "loss": 0.5069,
      "step": 281500
    },
    {
      "epoch": 4.516335682254965,
      "grad_norm": 6.096523761749268,
      "learning_rate": 1.236386931454196e-05,
      "loss": 0.5151,
      "step": 282000
    },
    {
      "epoch": 4.52434336963485,
      "grad_norm": 2.2012948989868164,
      "learning_rate": 1.2297138586376256e-05,
      "loss": 0.5008,
      "step": 282500
    },
    {
      "epoch": 4.5323510570147345,
      "grad_norm": 3.280164957046509,
      "learning_rate": 1.223040785821055e-05,
      "loss": 0.5022,
      "step": 283000
    },
    {
      "epoch": 4.540358744394619,
      "grad_norm": 20.409669876098633,
      "learning_rate": 1.2163677130044844e-05,
      "loss": 0.5087,
      "step": 283500
    },
    {
      "epoch": 4.548366431774504,
      "grad_norm": 3.3011295795440674,
      "learning_rate": 1.2096946401879137e-05,
      "loss": 0.5076,
      "step": 284000
    },
    {
      "epoch": 4.5563741191543885,
      "grad_norm": 4.40270471572876,
      "learning_rate": 1.2030215673713433e-05,
      "loss": 0.4972,
      "step": 284500
    },
    {
      "epoch": 4.564381806534273,
      "grad_norm": 2.9016330242156982,
      "learning_rate": 1.1963484945547725e-05,
      "loss": 0.5287,
      "step": 285000
    },
    {
      "epoch": 4.572389493914158,
      "grad_norm": 4.172075271606445,
      "learning_rate": 1.189675421738202e-05,
      "loss": 0.4951,
      "step": 285500
    },
    {
      "epoch": 4.5803971812940425,
      "grad_norm": 6.362120628356934,
      "learning_rate": 1.1830023489216315e-05,
      "loss": 0.491,
      "step": 286000
    },
    {
      "epoch": 4.588404868673927,
      "grad_norm": 7.832621097564697,
      "learning_rate": 1.176329276105061e-05,
      "loss": 0.5017,
      "step": 286500
    },
    {
      "epoch": 4.596412556053812,
      "grad_norm": 3.891714096069336,
      "learning_rate": 1.1696562032884902e-05,
      "loss": 0.5129,
      "step": 287000
    },
    {
      "epoch": 4.6044202434336965,
      "grad_norm": 3.234403610229492,
      "learning_rate": 1.1629831304719198e-05,
      "loss": 0.5164,
      "step": 287500
    },
    {
      "epoch": 4.612427930813581,
      "grad_norm": 7.876539707183838,
      "learning_rate": 1.1563100576553492e-05,
      "loss": 0.4971,
      "step": 288000
    },
    {
      "epoch": 4.620435618193466,
      "grad_norm": 11.54860782623291,
      "learning_rate": 1.1496369848387786e-05,
      "loss": 0.5194,
      "step": 288500
    },
    {
      "epoch": 4.6284433055733505,
      "grad_norm": 4.4252448081970215,
      "learning_rate": 1.142963912022208e-05,
      "loss": 0.4944,
      "step": 289000
    },
    {
      "epoch": 4.636450992953235,
      "grad_norm": 6.529786586761475,
      "learning_rate": 1.1362908392056375e-05,
      "loss": 0.5108,
      "step": 289500
    },
    {
      "epoch": 4.64445868033312,
      "grad_norm": 5.203530311584473,
      "learning_rate": 1.1296177663890669e-05,
      "loss": 0.5009,
      "step": 290000
    },
    {
      "epoch": 4.6524663677130045,
      "grad_norm": 9.2627534866333,
      "learning_rate": 1.1229446935724963e-05,
      "loss": 0.508,
      "step": 290500
    },
    {
      "epoch": 4.660474055092889,
      "grad_norm": 4.854918003082275,
      "learning_rate": 1.1162716207559257e-05,
      "loss": 0.4899,
      "step": 291000
    },
    {
      "epoch": 4.668481742472774,
      "grad_norm": 3.552065134048462,
      "learning_rate": 1.1095985479393552e-05,
      "loss": 0.5064,
      "step": 291500
    },
    {
      "epoch": 4.6764894298526585,
      "grad_norm": 2.438626527786255,
      "learning_rate": 1.1029254751227846e-05,
      "loss": 0.5072,
      "step": 292000
    },
    {
      "epoch": 4.684497117232543,
      "grad_norm": 7.479953765869141,
      "learning_rate": 1.096252402306214e-05,
      "loss": 0.52,
      "step": 292500
    },
    {
      "epoch": 4.692504804612428,
      "grad_norm": 7.670618534088135,
      "learning_rate": 1.0895793294896436e-05,
      "loss": 0.5084,
      "step": 293000
    },
    {
      "epoch": 4.7005124919923125,
      "grad_norm": 145.91036987304688,
      "learning_rate": 1.0829062566730728e-05,
      "loss": 0.5138,
      "step": 293500
    },
    {
      "epoch": 4.708520179372197,
      "grad_norm": 3.721892833709717,
      "learning_rate": 1.0762331838565023e-05,
      "loss": 0.506,
      "step": 294000
    },
    {
      "epoch": 4.716527866752082,
      "grad_norm": 12.177992820739746,
      "learning_rate": 1.0695601110399317e-05,
      "loss": 0.5187,
      "step": 294500
    },
    {
      "epoch": 4.7245355541319665,
      "grad_norm": 3.512115001678467,
      "learning_rate": 1.0628870382233611e-05,
      "loss": 0.4954,
      "step": 295000
    },
    {
      "epoch": 4.732543241511851,
      "grad_norm": 4.103353977203369,
      "learning_rate": 1.0562139654067905e-05,
      "loss": 0.5208,
      "step": 295500
    },
    {
      "epoch": 4.740550928891736,
      "grad_norm": 3.5942816734313965,
      "learning_rate": 1.04954089259022e-05,
      "loss": 0.5162,
      "step": 296000
    },
    {
      "epoch": 4.7485586162716205,
      "grad_norm": 11.2730131149292,
      "learning_rate": 1.0428678197736495e-05,
      "loss": 0.5347,
      "step": 296500
    },
    {
      "epoch": 4.756566303651505,
      "grad_norm": 3.0580708980560303,
      "learning_rate": 1.0361947469570788e-05,
      "loss": 0.5246,
      "step": 297000
    },
    {
      "epoch": 4.76457399103139,
      "grad_norm": 4.894040107727051,
      "learning_rate": 1.0295216741405082e-05,
      "loss": 0.5356,
      "step": 297500
    },
    {
      "epoch": 4.7725816784112745,
      "grad_norm": 4.470303058624268,
      "learning_rate": 1.0228486013239378e-05,
      "loss": 0.4998,
      "step": 298000
    },
    {
      "epoch": 4.78058936579116,
      "grad_norm": 4.158844470977783,
      "learning_rate": 1.016175528507367e-05,
      "loss": 0.4937,
      "step": 298500
    },
    {
      "epoch": 4.788597053171044,
      "grad_norm": 2.6250667572021484,
      "learning_rate": 1.0095024556907966e-05,
      "loss": 0.5246,
      "step": 299000
    },
    {
      "epoch": 4.796604740550929,
      "grad_norm": 14.928330421447754,
      "learning_rate": 1.002829382874226e-05,
      "loss": 0.5085,
      "step": 299500
    },
    {
      "epoch": 4.804612427930813,
      "grad_norm": 8.037238121032715,
      "learning_rate": 9.961563100576553e-06,
      "loss": 0.5,
      "step": 300000
    },
    {
      "epoch": 4.812620115310699,
      "grad_norm": 3.0412943363189697,
      "learning_rate": 9.894832372410847e-06,
      "loss": 0.5019,
      "step": 300500
    },
    {
      "epoch": 4.820627802690583,
      "grad_norm": 1.9552834033966064,
      "learning_rate": 9.828101644245143e-06,
      "loss": 0.5136,
      "step": 301000
    },
    {
      "epoch": 4.828635490070468,
      "grad_norm": 3.6778342723846436,
      "learning_rate": 9.761370916079437e-06,
      "loss": 0.4959,
      "step": 301500
    },
    {
      "epoch": 4.836643177450353,
      "grad_norm": 8.766436576843262,
      "learning_rate": 9.69464018791373e-06,
      "loss": 0.5069,
      "step": 302000
    },
    {
      "epoch": 4.844650864830237,
      "grad_norm": 5.2205891609191895,
      "learning_rate": 9.627909459748024e-06,
      "loss": 0.4909,
      "step": 302500
    },
    {
      "epoch": 4.852658552210122,
      "grad_norm": 2.4997761249542236,
      "learning_rate": 9.56117873158232e-06,
      "loss": 0.5134,
      "step": 303000
    },
    {
      "epoch": 4.860666239590007,
      "grad_norm": 4.627324104309082,
      "learning_rate": 9.494448003416614e-06,
      "loss": 0.4884,
      "step": 303500
    },
    {
      "epoch": 4.868673926969891,
      "grad_norm": 4.733732223510742,
      "learning_rate": 9.427717275250908e-06,
      "loss": 0.4853,
      "step": 304000
    },
    {
      "epoch": 4.876681614349776,
      "grad_norm": 6.7082390785217285,
      "learning_rate": 9.360986547085203e-06,
      "loss": 0.4945,
      "step": 304500
    },
    {
      "epoch": 4.884689301729661,
      "grad_norm": 4.812807559967041,
      "learning_rate": 9.294255818919497e-06,
      "loss": 0.4858,
      "step": 305000
    },
    {
      "epoch": 4.892696989109545,
      "grad_norm": 6.763813018798828,
      "learning_rate": 9.227525090753791e-06,
      "loss": 0.5036,
      "step": 305500
    },
    {
      "epoch": 4.90070467648943,
      "grad_norm": 3.2796010971069336,
      "learning_rate": 9.160794362588085e-06,
      "loss": 0.4857,
      "step": 306000
    },
    {
      "epoch": 4.908712363869315,
      "grad_norm": 6.4682135581970215,
      "learning_rate": 9.09406363442238e-06,
      "loss": 0.5022,
      "step": 306500
    },
    {
      "epoch": 4.916720051249199,
      "grad_norm": 4.228434085845947,
      "learning_rate": 9.027332906256673e-06,
      "loss": 0.4854,
      "step": 307000
    },
    {
      "epoch": 4.924727738629084,
      "grad_norm": 4.0644378662109375,
      "learning_rate": 8.960602178090968e-06,
      "loss": 0.5171,
      "step": 307500
    },
    {
      "epoch": 4.932735426008969,
      "grad_norm": 3.108691930770874,
      "learning_rate": 8.893871449925262e-06,
      "loss": 0.4814,
      "step": 308000
    },
    {
      "epoch": 4.940743113388853,
      "grad_norm": 3.828885555267334,
      "learning_rate": 8.827140721759556e-06,
      "loss": 0.4868,
      "step": 308500
    },
    {
      "epoch": 4.948750800768738,
      "grad_norm": 3.627183675765991,
      "learning_rate": 8.76040999359385e-06,
      "loss": 0.5101,
      "step": 309000
    },
    {
      "epoch": 4.956758488148623,
      "grad_norm": 7.307015419006348,
      "learning_rate": 8.693679265428145e-06,
      "loss": 0.4837,
      "step": 309500
    },
    {
      "epoch": 4.964766175528507,
      "grad_norm": 3.346383571624756,
      "learning_rate": 8.62694853726244e-06,
      "loss": 0.5127,
      "step": 310000
    },
    {
      "epoch": 4.972773862908392,
      "grad_norm": 2.1650428771972656,
      "learning_rate": 8.560217809096733e-06,
      "loss": 0.4977,
      "step": 310500
    },
    {
      "epoch": 4.980781550288277,
      "grad_norm": 2.1133127212524414,
      "learning_rate": 8.493487080931027e-06,
      "loss": 0.5016,
      "step": 311000
    },
    {
      "epoch": 4.988789237668161,
      "grad_norm": 2.6721668243408203,
      "learning_rate": 8.426756352765323e-06,
      "loss": 0.495,
      "step": 311500
    },
    {
      "epoch": 4.996796925048046,
      "grad_norm": 4.618086338043213,
      "learning_rate": 8.360025624599615e-06,
      "loss": 0.4964,
      "step": 312000
    },
    {
      "epoch": 5.004804612427931,
      "grad_norm": 8.765559196472168,
      "learning_rate": 8.29329489643391e-06,
      "loss": 0.4745,
      "step": 312500
    },
    {
      "epoch": 5.012812299807815,
      "grad_norm": 4.1259894371032715,
      "learning_rate": 8.226564168268204e-06,
      "loss": 0.4651,
      "step": 313000
    },
    {
      "epoch": 5.0208199871877,
      "grad_norm": 6.5845417976379395,
      "learning_rate": 8.159833440102498e-06,
      "loss": 0.4879,
      "step": 313500
    },
    {
      "epoch": 5.028827674567585,
      "grad_norm": 5.27044677734375,
      "learning_rate": 8.093102711936792e-06,
      "loss": 0.494,
      "step": 314000
    },
    {
      "epoch": 5.036835361947469,
      "grad_norm": 3.092660665512085,
      "learning_rate": 8.026371983771088e-06,
      "loss": 0.4856,
      "step": 314500
    },
    {
      "epoch": 5.044843049327354,
      "grad_norm": 3.2038395404815674,
      "learning_rate": 7.959641255605381e-06,
      "loss": 0.469,
      "step": 315000
    },
    {
      "epoch": 5.052850736707239,
      "grad_norm": 2.299567699432373,
      "learning_rate": 7.892910527439675e-06,
      "loss": 0.4842,
      "step": 315500
    },
    {
      "epoch": 5.060858424087123,
      "grad_norm": 2.303654432296753,
      "learning_rate": 7.82617979927397e-06,
      "loss": 0.4909,
      "step": 316000
    },
    {
      "epoch": 5.068866111467008,
      "grad_norm": 5.940292835235596,
      "learning_rate": 7.759449071108265e-06,
      "loss": 0.4763,
      "step": 316500
    },
    {
      "epoch": 5.076873798846893,
      "grad_norm": 9.555686950683594,
      "learning_rate": 7.692718342942559e-06,
      "loss": 0.4842,
      "step": 317000
    },
    {
      "epoch": 5.084881486226777,
      "grad_norm": 3.485595226287842,
      "learning_rate": 7.6259876147768525e-06,
      "loss": 0.476,
      "step": 317500
    },
    {
      "epoch": 5.092889173606663,
      "grad_norm": 5.489136219024658,
      "learning_rate": 7.559256886611147e-06,
      "loss": 0.476,
      "step": 318000
    },
    {
      "epoch": 5.1008968609865475,
      "grad_norm": 2.3655288219451904,
      "learning_rate": 7.492526158445441e-06,
      "loss": 0.4758,
      "step": 318500
    },
    {
      "epoch": 5.108904548366432,
      "grad_norm": 4.3599138259887695,
      "learning_rate": 7.425795430279736e-06,
      "loss": 0.4769,
      "step": 319000
    },
    {
      "epoch": 5.116912235746317,
      "grad_norm": 5.789588928222656,
      "learning_rate": 7.35906470211403e-06,
      "loss": 0.4758,
      "step": 319500
    },
    {
      "epoch": 5.1249199231262015,
      "grad_norm": 8.993159294128418,
      "learning_rate": 7.2923339739483245e-06,
      "loss": 0.4885,
      "step": 320000
    },
    {
      "epoch": 5.132927610506086,
      "grad_norm": 3.0439155101776123,
      "learning_rate": 7.225603245782618e-06,
      "loss": 0.4626,
      "step": 320500
    },
    {
      "epoch": 5.140935297885971,
      "grad_norm": 6.192640781402588,
      "learning_rate": 7.158872517616913e-06,
      "loss": 0.4939,
      "step": 321000
    },
    {
      "epoch": 5.1489429852658555,
      "grad_norm": 9.164207458496094,
      "learning_rate": 7.092141789451207e-06,
      "loss": 0.4661,
      "step": 321500
    },
    {
      "epoch": 5.15695067264574,
      "grad_norm": 1.9001259803771973,
      "learning_rate": 7.025411061285502e-06,
      "loss": 0.4913,
      "step": 322000
    },
    {
      "epoch": 5.164958360025625,
      "grad_norm": 6.814502239227295,
      "learning_rate": 6.958680333119795e-06,
      "loss": 0.4723,
      "step": 322500
    },
    {
      "epoch": 5.1729660474055095,
      "grad_norm": 6.305384635925293,
      "learning_rate": 6.89194960495409e-06,
      "loss": 0.4839,
      "step": 323000
    },
    {
      "epoch": 5.180973734785394,
      "grad_norm": 5.89647912979126,
      "learning_rate": 6.825218876788383e-06,
      "loss": 0.485,
      "step": 323500
    },
    {
      "epoch": 5.188981422165279,
      "grad_norm": 7.292586803436279,
      "learning_rate": 6.758488148622678e-06,
      "loss": 0.4818,
      "step": 324000
    },
    {
      "epoch": 5.1969891095451635,
      "grad_norm": 4.718400478363037,
      "learning_rate": 6.691757420456972e-06,
      "loss": 0.4807,
      "step": 324500
    },
    {
      "epoch": 5.204996796925048,
      "grad_norm": 8.413100242614746,
      "learning_rate": 6.625026692291267e-06,
      "loss": 0.4828,
      "step": 325000
    },
    {
      "epoch": 5.213004484304933,
      "grad_norm": 2.8939242362976074,
      "learning_rate": 6.5582959641255605e-06,
      "loss": 0.4929,
      "step": 325500
    },
    {
      "epoch": 5.2210121716848175,
      "grad_norm": 12.550703048706055,
      "learning_rate": 6.491565235959855e-06,
      "loss": 0.474,
      "step": 326000
    },
    {
      "epoch": 5.229019859064702,
      "grad_norm": 3.062390089035034,
      "learning_rate": 6.424834507794149e-06,
      "loss": 0.4923,
      "step": 326500
    },
    {
      "epoch": 5.237027546444587,
      "grad_norm": 18.374021530151367,
      "learning_rate": 6.358103779628444e-06,
      "loss": 0.4872,
      "step": 327000
    },
    {
      "epoch": 5.2450352338244715,
      "grad_norm": 7.918933391571045,
      "learning_rate": 6.291373051462737e-06,
      "loss": 0.4979,
      "step": 327500
    },
    {
      "epoch": 5.253042921204356,
      "grad_norm": 12.665647506713867,
      "learning_rate": 6.224642323297032e-06,
      "loss": 0.4813,
      "step": 328000
    },
    {
      "epoch": 5.261050608584241,
      "grad_norm": 21.97609519958496,
      "learning_rate": 6.157911595131326e-06,
      "loss": 0.4872,
      "step": 328500
    },
    {
      "epoch": 5.2690582959641254,
      "grad_norm": 8.657448768615723,
      "learning_rate": 6.091180866965621e-06,
      "loss": 0.4765,
      "step": 329000
    },
    {
      "epoch": 5.27706598334401,
      "grad_norm": 3.231367588043213,
      "learning_rate": 6.024450138799915e-06,
      "loss": 0.5089,
      "step": 329500
    },
    {
      "epoch": 5.285073670723895,
      "grad_norm": 3.2561051845550537,
      "learning_rate": 5.957719410634209e-06,
      "loss": 0.4941,
      "step": 330000
    },
    {
      "epoch": 5.293081358103779,
      "grad_norm": 3.5132923126220703,
      "learning_rate": 5.8909886824685035e-06,
      "loss": 0.4806,
      "step": 330500
    },
    {
      "epoch": 5.301089045483664,
      "grad_norm": 6.223061561584473,
      "learning_rate": 5.824257954302797e-06,
      "loss": 0.475,
      "step": 331000
    },
    {
      "epoch": 5.309096732863549,
      "grad_norm": 26.55878257751465,
      "learning_rate": 5.757527226137092e-06,
      "loss": 0.4877,
      "step": 331500
    },
    {
      "epoch": 5.317104420243433,
      "grad_norm": 2.6483941078186035,
      "learning_rate": 5.690796497971386e-06,
      "loss": 0.4824,
      "step": 332000
    },
    {
      "epoch": 5.325112107623318,
      "grad_norm": 5.1695027351379395,
      "learning_rate": 5.62406576980568e-06,
      "loss": 0.4866,
      "step": 332500
    },
    {
      "epoch": 5.333119795003203,
      "grad_norm": 4.2939677238464355,
      "learning_rate": 5.5573350416399746e-06,
      "loss": 0.4687,
      "step": 333000
    },
    {
      "epoch": 5.341127482383087,
      "grad_norm": 4.238346576690674,
      "learning_rate": 5.4906043134742684e-06,
      "loss": 0.4876,
      "step": 333500
    },
    {
      "epoch": 5.349135169762972,
      "grad_norm": 3.1566333770751953,
      "learning_rate": 5.423873585308563e-06,
      "loss": 0.4821,
      "step": 334000
    },
    {
      "epoch": 5.357142857142857,
      "grad_norm": 3.7363898754119873,
      "learning_rate": 5.357142857142857e-06,
      "loss": 0.485,
      "step": 334500
    },
    {
      "epoch": 5.365150544522741,
      "grad_norm": 12.400586128234863,
      "learning_rate": 5.290412128977151e-06,
      "loss": 0.4834,
      "step": 335000
    },
    {
      "epoch": 5.373158231902627,
      "grad_norm": 5.882442951202393,
      "learning_rate": 5.2236814008114465e-06,
      "loss": 0.4908,
      "step": 335500
    },
    {
      "epoch": 5.381165919282511,
      "grad_norm": 3.5278327465057373,
      "learning_rate": 5.15695067264574e-06,
      "loss": 0.4872,
      "step": 336000
    },
    {
      "epoch": 5.389173606662396,
      "grad_norm": 7.061009883880615,
      "learning_rate": 5.090219944480035e-06,
      "loss": 0.4968,
      "step": 336500
    },
    {
      "epoch": 5.397181294042281,
      "grad_norm": 2.266859769821167,
      "learning_rate": 5.023489216314329e-06,
      "loss": 0.4856,
      "step": 337000
    },
    {
      "epoch": 5.405188981422166,
      "grad_norm": 2.6333773136138916,
      "learning_rate": 4.956758488148623e-06,
      "loss": 0.477,
      "step": 337500
    },
    {
      "epoch": 5.41319666880205,
      "grad_norm": 17.05234718322754,
      "learning_rate": 4.8900277599829176e-06,
      "loss": 0.4915,
      "step": 338000
    },
    {
      "epoch": 5.421204356181935,
      "grad_norm": 5.005670547485352,
      "learning_rate": 4.8232970318172114e-06,
      "loss": 0.4647,
      "step": 338500
    },
    {
      "epoch": 5.42921204356182,
      "grad_norm": 6.949012756347656,
      "learning_rate": 4.756566303651506e-06,
      "loss": 0.4815,
      "step": 339000
    },
    {
      "epoch": 5.437219730941704,
      "grad_norm": 6.24033784866333,
      "learning_rate": 4.6898355754858e-06,
      "loss": 0.4803,
      "step": 339500
    },
    {
      "epoch": 5.445227418321589,
      "grad_norm": 6.081872940063477,
      "learning_rate": 4.623104847320094e-06,
      "loss": 0.4711,
      "step": 340000
    },
    {
      "epoch": 5.453235105701474,
      "grad_norm": 7.6153059005737305,
      "learning_rate": 4.556374119154389e-06,
      "loss": 0.4592,
      "step": 340500
    },
    {
      "epoch": 5.461242793081358,
      "grad_norm": 3.346057176589966,
      "learning_rate": 4.4896433909886825e-06,
      "loss": 0.4764,
      "step": 341000
    },
    {
      "epoch": 5.469250480461243,
      "grad_norm": 10.272503852844238,
      "learning_rate": 4.422912662822977e-06,
      "loss": 0.4707,
      "step": 341500
    },
    {
      "epoch": 5.477258167841128,
      "grad_norm": 6.078878879547119,
      "learning_rate": 4.356181934657271e-06,
      "loss": 0.4614,
      "step": 342000
    },
    {
      "epoch": 5.485265855221012,
      "grad_norm": 5.830956935882568,
      "learning_rate": 4.289451206491565e-06,
      "loss": 0.4695,
      "step": 342500
    },
    {
      "epoch": 5.493273542600897,
      "grad_norm": 6.5341081619262695,
      "learning_rate": 4.22272047832586e-06,
      "loss": 0.4879,
      "step": 343000
    },
    {
      "epoch": 5.501281229980782,
      "grad_norm": 5.779694080352783,
      "learning_rate": 4.155989750160154e-06,
      "loss": 0.4615,
      "step": 343500
    },
    {
      "epoch": 5.509288917360666,
      "grad_norm": 7.372576713562012,
      "learning_rate": 4.089259021994448e-06,
      "loss": 0.4628,
      "step": 344000
    },
    {
      "epoch": 5.517296604740551,
      "grad_norm": 2.799283266067505,
      "learning_rate": 4.022528293828742e-06,
      "loss": 0.4847,
      "step": 344500
    },
    {
      "epoch": 5.525304292120436,
      "grad_norm": 7.743137836456299,
      "learning_rate": 3.955797565663037e-06,
      "loss": 0.4707,
      "step": 345000
    },
    {
      "epoch": 5.53331197950032,
      "grad_norm": 16.805219650268555,
      "learning_rate": 3.889066837497331e-06,
      "loss": 0.4804,
      "step": 345500
    },
    {
      "epoch": 5.541319666880205,
      "grad_norm": 5.464966773986816,
      "learning_rate": 3.822336109331625e-06,
      "loss": 0.4594,
      "step": 346000
    },
    {
      "epoch": 5.54932735426009,
      "grad_norm": 6.363125801086426,
      "learning_rate": 3.7556053811659194e-06,
      "loss": 0.481,
      "step": 346500
    },
    {
      "epoch": 5.557335041639974,
      "grad_norm": 1.7123464345932007,
      "learning_rate": 3.6888746530002133e-06,
      "loss": 0.4647,
      "step": 347000
    },
    {
      "epoch": 5.565342729019859,
      "grad_norm": 4.194815635681152,
      "learning_rate": 3.6221439248345076e-06,
      "loss": 0.484,
      "step": 347500
    },
    {
      "epoch": 5.573350416399744,
      "grad_norm": 8.106250762939453,
      "learning_rate": 3.555413196668802e-06,
      "loss": 0.4689,
      "step": 348000
    },
    {
      "epoch": 5.581358103779628,
      "grad_norm": 5.541666507720947,
      "learning_rate": 3.488682468503097e-06,
      "loss": 0.4838,
      "step": 348500
    },
    {
      "epoch": 5.589365791159513,
      "grad_norm": 4.804227352142334,
      "learning_rate": 3.421951740337391e-06,
      "loss": 0.4787,
      "step": 349000
    },
    {
      "epoch": 5.597373478539398,
      "grad_norm": 9.19482135772705,
      "learning_rate": 3.355221012171685e-06,
      "loss": 0.4809,
      "step": 349500
    },
    {
      "epoch": 5.605381165919282,
      "grad_norm": 13.677319526672363,
      "learning_rate": 3.2884902840059795e-06,
      "loss": 0.4796,
      "step": 350000
    },
    {
      "epoch": 5.613388853299167,
      "grad_norm": 5.262714385986328,
      "learning_rate": 3.2217595558402738e-06,
      "loss": 0.4725,
      "step": 350500
    },
    {
      "epoch": 5.621396540679052,
      "grad_norm": 4.824803829193115,
      "learning_rate": 3.155028827674568e-06,
      "loss": 0.461,
      "step": 351000
    },
    {
      "epoch": 5.629404228058936,
      "grad_norm": 4.22475528717041,
      "learning_rate": 3.088298099508862e-06,
      "loss": 0.4873,
      "step": 351500
    },
    {
      "epoch": 5.637411915438821,
      "grad_norm": 16.183687210083008,
      "learning_rate": 3.0215673713431563e-06,
      "loss": 0.4811,
      "step": 352000
    },
    {
      "epoch": 5.645419602818706,
      "grad_norm": 50.56525802612305,
      "learning_rate": 2.9548366431774506e-06,
      "loss": 0.4707,
      "step": 352500
    },
    {
      "epoch": 5.653427290198591,
      "grad_norm": 6.292402744293213,
      "learning_rate": 2.888105915011745e-06,
      "loss": 0.4789,
      "step": 353000
    },
    {
      "epoch": 5.661434977578475,
      "grad_norm": 6.319599151611328,
      "learning_rate": 2.821375186846039e-06,
      "loss": 0.473,
      "step": 353500
    },
    {
      "epoch": 5.6694426649583605,
      "grad_norm": 10.424015998840332,
      "learning_rate": 2.7546444586803334e-06,
      "loss": 0.4886,
      "step": 354000
    },
    {
      "epoch": 5.677450352338244,
      "grad_norm": 2.838763475418091,
      "learning_rate": 2.6879137305146273e-06,
      "loss": 0.4942,
      "step": 354500
    },
    {
      "epoch": 5.68545803971813,
      "grad_norm": 6.149264812469482,
      "learning_rate": 2.6211830023489216e-06,
      "loss": 0.4716,
      "step": 355000
    },
    {
      "epoch": 5.6934657270980145,
      "grad_norm": 5.771134853363037,
      "learning_rate": 2.554452274183216e-06,
      "loss": 0.4864,
      "step": 355500
    },
    {
      "epoch": 5.701473414477899,
      "grad_norm": 5.413075923919678,
      "learning_rate": 2.4877215460175102e-06,
      "loss": 0.4853,
      "step": 356000
    },
    {
      "epoch": 5.709481101857784,
      "grad_norm": 6.618795871734619,
      "learning_rate": 2.4209908178518045e-06,
      "loss": 0.4904,
      "step": 356500
    },
    {
      "epoch": 5.7174887892376685,
      "grad_norm": 4.308932781219482,
      "learning_rate": 2.354260089686099e-06,
      "loss": 0.4802,
      "step": 357000
    },
    {
      "epoch": 5.725496476617553,
      "grad_norm": 4.6922688484191895,
      "learning_rate": 2.2875293615203927e-06,
      "loss": 0.4766,
      "step": 357500
    },
    {
      "epoch": 5.733504163997438,
      "grad_norm": 24.19145393371582,
      "learning_rate": 2.2207986333546874e-06,
      "loss": 0.4717,
      "step": 358000
    },
    {
      "epoch": 5.7415118513773225,
      "grad_norm": 5.315155029296875,
      "learning_rate": 2.1540679051889817e-06,
      "loss": 0.4844,
      "step": 358500
    },
    {
      "epoch": 5.749519538757207,
      "grad_norm": 8.700489044189453,
      "learning_rate": 2.087337177023276e-06,
      "loss": 0.4593,
      "step": 359000
    },
    {
      "epoch": 5.757527226137092,
      "grad_norm": 8.896708488464355,
      "learning_rate": 2.02060644885757e-06,
      "loss": 0.467,
      "step": 359500
    },
    {
      "epoch": 5.7655349135169764,
      "grad_norm": 3.5301380157470703,
      "learning_rate": 1.953875720691864e-06,
      "loss": 0.4581,
      "step": 360000
    },
    {
      "epoch": 5.773542600896861,
      "grad_norm": 5.9156012535095215,
      "learning_rate": 1.8871449925261585e-06,
      "loss": 0.489,
      "step": 360500
    },
    {
      "epoch": 5.781550288276746,
      "grad_norm": 6.3766679763793945,
      "learning_rate": 1.8204142643604528e-06,
      "loss": 0.4834,
      "step": 361000
    },
    {
      "epoch": 5.78955797565663,
      "grad_norm": 10.973319053649902,
      "learning_rate": 1.753683536194747e-06,
      "loss": 0.475,
      "step": 361500
    },
    {
      "epoch": 5.797565663036515,
      "grad_norm": 12.462100982666016,
      "learning_rate": 1.6869528080290412e-06,
      "loss": 0.4852,
      "step": 362000
    },
    {
      "epoch": 5.8055733504164,
      "grad_norm": 6.3160319328308105,
      "learning_rate": 1.6202220798633355e-06,
      "loss": 0.4711,
      "step": 362500
    },
    {
      "epoch": 5.813581037796284,
      "grad_norm": 3.182410717010498,
      "learning_rate": 1.5534913516976298e-06,
      "loss": 0.4811,
      "step": 363000
    },
    {
      "epoch": 5.821588725176169,
      "grad_norm": 4.807241439819336,
      "learning_rate": 1.486760623531924e-06,
      "loss": 0.4726,
      "step": 363500
    },
    {
      "epoch": 5.829596412556054,
      "grad_norm": 18.34586524963379,
      "learning_rate": 1.4200298953662184e-06,
      "loss": 0.4883,
      "step": 364000
    },
    {
      "epoch": 5.837604099935938,
      "grad_norm": 7.108320713043213,
      "learning_rate": 1.3532991672005125e-06,
      "loss": 0.4748,
      "step": 364500
    },
    {
      "epoch": 5.845611787315823,
      "grad_norm": 3.7425782680511475,
      "learning_rate": 1.2865684390348068e-06,
      "loss": 0.4697,
      "step": 365000
    },
    {
      "epoch": 5.853619474695708,
      "grad_norm": 5.137964725494385,
      "learning_rate": 1.219837710869101e-06,
      "loss": 0.471,
      "step": 365500
    },
    {
      "epoch": 5.861627162075592,
      "grad_norm": 27.71477508544922,
      "learning_rate": 1.1531069827033954e-06,
      "loss": 0.4742,
      "step": 366000
    },
    {
      "epoch": 5.869634849455477,
      "grad_norm": 5.923786640167236,
      "learning_rate": 1.0863762545376897e-06,
      "loss": 0.4885,
      "step": 366500
    },
    {
      "epoch": 5.877642536835362,
      "grad_norm": 8.95864486694336,
      "learning_rate": 1.0196455263719838e-06,
      "loss": 0.4821,
      "step": 367000
    },
    {
      "epoch": 5.885650224215246,
      "grad_norm": 14.131387710571289,
      "learning_rate": 9.529147982062781e-07,
      "loss": 0.4662,
      "step": 367500
    },
    {
      "epoch": 5.893657911595131,
      "grad_norm": 6.35711145401001,
      "learning_rate": 8.861840700405722e-07,
      "loss": 0.4589,
      "step": 368000
    },
    {
      "epoch": 5.901665598975016,
      "grad_norm": 10.677464485168457,
      "learning_rate": 8.194533418748665e-07,
      "loss": 0.4768,
      "step": 368500
    },
    {
      "epoch": 5.9096732863549,
      "grad_norm": 3.2761917114257812,
      "learning_rate": 7.527226137091608e-07,
      "loss": 0.4695,
      "step": 369000
    },
    {
      "epoch": 5.917680973734785,
      "grad_norm": 2.721064329147339,
      "learning_rate": 6.859918855434551e-07,
      "loss": 0.4737,
      "step": 369500
    },
    {
      "epoch": 5.92568866111467,
      "grad_norm": 4.378329753875732,
      "learning_rate": 6.192611573777493e-07,
      "loss": 0.4817,
      "step": 370000
    },
    {
      "epoch": 5.933696348494554,
      "grad_norm": 2.491569995880127,
      "learning_rate": 5.525304292120435e-07,
      "loss": 0.4673,
      "step": 370500
    },
    {
      "epoch": 5.941704035874439,
      "grad_norm": 5.123173236846924,
      "learning_rate": 4.857997010463378e-07,
      "loss": 0.4583,
      "step": 371000
    },
    {
      "epoch": 5.949711723254325,
      "grad_norm": 5.359287261962891,
      "learning_rate": 4.1906897288063213e-07,
      "loss": 0.4648,
      "step": 371500
    },
    {
      "epoch": 5.957719410634208,
      "grad_norm": 3.161585569381714,
      "learning_rate": 3.5233824471492633e-07,
      "loss": 0.4603,
      "step": 372000
    },
    {
      "epoch": 5.965727098014094,
      "grad_norm": 6.793612480163574,
      "learning_rate": 2.8560751654922063e-07,
      "loss": 0.4769,
      "step": 372500
    },
    {
      "epoch": 5.973734785393978,
      "grad_norm": 8.137609481811523,
      "learning_rate": 2.1887678838351487e-07,
      "loss": 0.4651,
      "step": 373000
    },
    {
      "epoch": 5.981742472773863,
      "grad_norm": 4.73988676071167,
      "learning_rate": 1.521460602178091e-07,
      "loss": 0.483,
      "step": 373500
    },
    {
      "epoch": 5.989750160153748,
      "grad_norm": 7.941307067871094,
      "learning_rate": 8.541533205210335e-08,
      "loss": 0.4651,
      "step": 374000
    },
    {
      "epoch": 5.997757847533633,
      "grad_norm": 14.444925308227539,
      "learning_rate": 1.868460388639761e-08,
      "loss": 0.4623,
      "step": 374500
    }
  ],
  "logging_steps": 500,
  "max_steps": 374640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.942925349717176e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
